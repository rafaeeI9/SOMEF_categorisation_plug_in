,Repo,Header,Content
0,LibGEOS.jl-README.md,LibGEOS.jl,"[![Build Status](https://travis-ci.org/JuliaGeo/LibGEOS.jl.svg?branch=master)](https://travis-ci.org/JuliaGeo/LibGEOS.jl)
[![Build Status](https://ci.appveyor.com/api/projects/status/github/JuliaGeo/LibGEOS.jl?svg=true&branch=master)](https://ci.appveyor.com/project/JuliaGeo/LibGEOS-jl/branch/master)
[![Coverage Status](https://coveralls.io/repos/github/JuliaGeo/LibGEOS.jl/badge.svg)](https://coveralls.io/github/JuliaGeo/LibGEOS.jl)

LibGEOS is a LGPL-licensed package for manipulation and analysis of planar geometric objects, based on the libraries [GEOS](https://trac.osgeo.org/geos/) (the engine of PostGIS) and JTS (from which GEOS is ported).

Among other things, it allows you to parse [Well-known Text (WKT)](https://en.wikipedia.org/wiki/Well-known_text)

```julia
p1 = readgeom(""POLYGON((0 0,1 0,1 1,0 0))"")
p2 = readgeom(""POLYGON((0 0,1 0,1 1,0 1,0 0))"")
p3 = readgeom(""POLYGON((2 0,3 0,3 1,2 1,2 0))"")
```
![Example 1](examples/example1.png)

Add a buffer around them
```julia
g1 = buffer(p1, 0.5)
g2 = buffer(p2, 0.5)
g3 = buffer(p3, 0.5)
```
![Example 2](examples/example2.png)

and take the union of different geometries
```julia
polygon = LibGEOS.union(g1, g3)
```
![Example 3](examples/example3.png)

GEOS functionality is extensive, so coverage is incomplete, but the basic functionality for working with geospatial data is already available. I'm learning as I go along, so documentation is lacking, but if you're interested, you can have a look at the examples in the `examples/` folder, or the tests in `test/test_geo_interface.jl` and `test/test_geos_operations.jl`.

"
1,LibGEOS.jl-README.md,Installation,"1. At the Julia prompt, run 
  ```julia
  julia> Pkg.add(""LibGEOS"")
  ```
  This will install both the Julia package and GEOS shared libraries together. To just reinstall the GEOS shared libraries, run `Pkg.build(""LibGEOS"")`.

2. Test that `LibGEOS` works by runnning
  ```julia
  julia> Pkg.test(""LibGEOS"")
  ```
"
0,Flow-Guided-Feature-Aggregation-README.md, Flow-Guided Feature Aggregation for Video Object Detection,"This repository is implemented by [Yuqing Zhu](https://github.com/jeremy43), [Shuhao Fu](https://github.com/howardmumu), and [Xizhou Zhu](https://github.com/einsiedler0408), when they are interns at MSRA.

"
1,Flow-Guided-Feature-Aggregation-README.md, Introduction,"**Flow-Guided Feature Aggregation (FGFA)** is initially described in an [ICCV 2017 paper](https://arxiv.org/abs/1703.10025). It provides an accurate and end-to-end learning framework for video object detection. The proposed FGFA method, together with our previous work of [Deep Feature Flow](https://github.com/msracver/Deep-Feature-Flow), powered the winning entry of [ImageNet VID 2017](http://image-net.org/challenges/LSVRC/2017/results). It is worth noting that:

* FGFA improves the per-frame features by aggregating nearby frame features along the motion paths. It significantly improves the object detection accuracy in videos, especially for fast moving objects.
* FGFA is end-to-end trainable for the task of video object detection, which is vital for improving the recognition accuracy.
* We proposed to evaluate the detection accuracy for slow, medium and fast moving objects respectively, for better understanding and analysis of video object detection. The [motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is included in this repository.

***Click image to watch our demo video***

[![Demo Video on YouTube](https://media.giphy.com/media/7D9tmDgzB10HK/giphy.gif)](https://www.youtube.com/watch?v=R2h3DbTPvVg)

***Example object instances with slow, medium and fast motions***

![Instance Motion](instance_motion.png)

"
2,Flow-Guided-Feature-Aggregation-README.md, Disclaimer,"This is an official implementation for [Flow-Guided Feature Aggregation for Video Recognition](https://arxiv.org/abs/1703.10025) (FGFA) based on MXNet. It is worth noticing that:

  * The original implementation is based on our internal Caffe version on Windows. There are slight differences in the final accuracy and running time due to the plenty details in platform switch.
  * One-phase training is performed on the mixture of ImageNet DET+VID, instead of two-phase training as in the original paper (on ImageNet DET first, followed by ImageNet VID).
  * The code is tested on official [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0) with the extra operators for Flow-guided Feature Aggregation.
  * We trained our model based on the ImageNet pre-trained [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) model and [Flying Chairs](https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html) pre-trained [FlowNet](https://lmb.informatik.uni-freiburg.de/resources/binaries/dispflownet/dispflownet-release-1.2.tar.gz) model using a [model converter](https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter). The converted [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%).
  * This repository used code from [MXNet rcnn example](https://github.com/apache/incubator-mxnet/tree/master/example/rcnn) and [mx-rfcn](https://github.com/giorking/mx-rfcn).



"
3,Flow-Guided-Feature-Aggregation-README.md, License,"© Microsoft, 2017. Licensed under the [MIT](LICENSE) License.

"
4,Flow-Guided-Feature-Aggregation-README.md, Citing Flow-Guided Feature Aggregation,"If you find Flow-Guided Feature Aggregation useful in your research, please consider citing:
```
@inproceedings{zhu17fgfa,
    Author = {Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei},
    Title = {Flow-Guided Feature Aggregation for Video Object Detection},
    Conference = {ICCV},
    Year = {2017}
}

@inproceedings{dai16rfcn,
    Author = {Jifeng Dai, Yi Li, Kaiming He, Jian Sun},
    Title = {{R-FCN}: Object Detection via Region-based Fully Convolutional Networks},
    Conference = {NIPS},
    Year = {2016}
}
```

"
5,Flow-Guided-Feature-Aggregation-README.md, Main Results,"|                                 | <sub>training data</sub>     | <sub>testing data</sub> | <sub>mAP(%)</sub> | <sub>mAP(%)</br>(slow)</sub>  | <sub>mAP(%)</br>(medium)</sub> | <sub>mAP(%)</br>(fast)</sub> |
|---------------------------------|-------------------|--------------|---------|---------|--------|--------|
| <sub>Single-frame baseline</br>(R-FCN, ResNet-v1-101)</sub>   | <sub>ImageNet DET train</br> + VID train</sub> | <sub>ImageNet VID validation</sub> | 74.1 | 83.6 | 71.6 | 51.2 |
| <sub>FGFA</br>(R-FCN, ResNet-v1-101, FlowNet)</sub>           | <sub>ImageNet DET train</br> + VID train</sub> | <sub>ImageNet VID validation</sub> | 77.1 | 85.9 | 75.7 | 56.1 |
| <sub>FGFA + SeqNMS</br>(R-FCN, ResNet-v1-101, FlowNet)</sub>  | <sub>ImageNet DET train</br> + VID train</sub> | <sub>ImageNet VID validation</sub> | 78.9 | 86.8 | 77.9 | 57.9 |



*Detection accuracy of slow (motion IoU > 0.9), medium (0.7 ≤ motion IoU ≤ 0.9), and fast (motion IoU < 0.7) moving object instances.*

**[Motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is available!**


"
6,Flow-Guided-Feature-Aggregation-README.md, Requirements: Software,"1. MXNet from [the offical repository](https://github.com/apache/incubator-mxnet). We tested our code on [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0). Due to the rapid development of MXNet, it is recommended to checkout this version if you encounter any issues. We may maintain this repository periodically if MXNet adds important feature in future release.

2. Python packages might missing: cython, opencv-python >= 3.2.0, easydict. If `pip` is set up on your system, those packages should be able to be fetched and installed by running
	```
	pip install Cython
	pip install opencv-python==3.2.0.6
	pip install easydict==1.6
	```
3. For Windows users, Visual Studio 2015 is needed to compile cython module.


"
7,Flow-Guided-Feature-Aggregation-README.md, Requirements: Hardware,"Any NVIDIA GPUs with at least 8GB memory should be OK.

"
8,Flow-Guided-Feature-Aggregation-README.md, Installation,"1. Clone the Flow-Guided Feature Aggregation repository, and we call the directory that you cloned as ${FGFA_ROOT}.

~~~
git clone https://github.com/msracver/Flow-Guided-Feature-Aggregation.git
~~~
2. For Windows users, run ``cmd .\init.bat``. For Linux user, run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.

3. Install MXNet:

	3.1 Clone MXNet and checkout to [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0) by
	```
	git clone --recursive https://github.com/apache/incubator-mxnet.git
	cd incubator-mxnet
	git checkout v0.10.0
	git submodule update
	```
	3.2 Copy operators in `$(FGFA_ROOT)/fgfa_rfcn/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by
	```
	cp -r $(FGFA_ROOT)/fgfa_rfcn/operator_cxx/* $(MXNET_ROOT)/src/operator/contrib/
	```
	3.3 Compile MXNet
	```
	cd ${MXNET_ROOT}
	make -j4
	```
	3.4 Install the MXNet Python binding by

	***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***
	```
	cd python
	sudo python setup.py install
	```
	3.5 For advanced users, you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`, and modify `MXNET_VERSION` in `./experiments/fgfa_rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.


"
9,Flow-Guided-Feature-Aggregation-README.md, Demo,"1. To run the demo with our trained model (on ImageNet DET + VID train), please download the model manually from [OneDrive](https://1drv.ms/u/s!AqfHNsil2nOiiwDiKev7DB6L9ay7), and put it under folder `model/`.

	Make sure it looks like this:
	```
	./model/rfcn_fgfa_flownet_vid-0000.params
	```
2. Run
	```
	python ./fgfa_rfcn/demo.py
	```

"
10,Flow-Guided-Feature-Aggregation-README.md, Preparation for Training & Testing,"1. Please download ILSVRC2015 DET and ILSVRC2015 VID dataset, and make sure it looks like this:

	```
	./data/ILSVRC2015/
	./data/ILSVRC2015/Annotations/DET
	./data/ILSVRC2015/Annotations/VID
	./data/ILSVRC2015/Data/DET
	./data/ILSVRC2015/Data/VID
	./data/ILSVRC2015/ImageSets
	```

2. Please download ImageNet pre-trained ResNet-v1-101 model and Flying-Chairs pre-trained FlowNet model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMOBdCBiNaKbcjPrA), and put it under folder `./model`. Make sure it looks like this:
	```
	./model/pretrained_model/resnet_v1_101-0000.params
	./model/pretrained_model/flownet-0000.params
	```

"
11,Flow-Guided-Feature-Aggregation-README.md, Usage,"1. All of our experiment settings (GPU #, dataset, etc.) are kept in yaml config files at folder `./experiments/fgfa_rfcn/cfgs`.

2. Two config files have been provided so far, namely, frame baseline (R-FCN) and the proposed FGFA  for ImageNet VID. We use 4 GPUs to train models on ImageNet VID.

3. To perform experiments, run the python script with the corresponding config file as input. For example, to train and test FGFA with R-FCN, use the following command
    ```
    python experiments/fgfa_rfcn/fgfa_rfcn_end2end_train_test.py --cfg experiments/fgfa_rfcn/cfgs/resnet_v1_101_flownet_imagenet_vid_rfcn_end2end_ohem.yaml
    ```
	A cache folder would be created automatically to save the model and the log under `output/fgfa_rfcn/imagenet_vid/`.

4. Please find more details in config files and in our code.

"
12,Flow-Guided-Feature-Aggregation-README.md, Misc.,"Code has been tested under:

- Windows Server 2012 R2 with 4 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz
- Windows Server 2012 R2 with 2 Pascal Titan X GPUs and Intel Xeon CPU E5-2670 v2 @ 2.50GHz

"
13,Flow-Guided-Feature-Aggregation-README.md, FAQ,"Q: I encounter `segment fault` at the beginning.

A: A compatibility issue has been identified between MXNet and opencv-python 3.0+. We suggest that you always `import cv2` first before `import mxnet` in the entry script. 

<br/><br/>
Q: I find the training speed becomes slower when training for a long time.

A: It has been identified that MXNet on Windows has this problem. So we recommend to run this program on Linux. You could also stop it and resume the training process to regain the training speed if you encounter this problem.

<br/><br/>
Q: Can you share your caffe implementation?

A: Due to several reasons (code is based on a old, internal Caffe, port to public Caffe needs extra work, time limit, etc.). We do not plan to release our Caffe code. Since a warping layer is easy to implement, anyone who wish to do it is welcome to make a pull request.
"
0,empymod-README.md,"   
",".. image:: https://readthedocs.org/projects/empymod/badge/?version=latest
   :target: http://empymod.readthedocs.io/en/latest
   :alt: Documentation Status
.. image:: https://travis-ci.org/empymod/empymod.svg?branch=master
   :target: https://travis-ci.org/empymod/empymod
   :alt: Travis-CI
.. image:: https://coveralls.io/repos/github/empymod/empymod/badge.svg?branch=master
   :target: https://coveralls.io/github/empymod/empymod?branch=master
   :alt: Coveralls
.. image:: https://img.shields.io/codacy/grade/b28ed3989ed248fe95e34288e43667b9/master.svg
   :target: https://www.codacy.com/app/prisae/empymod
   :alt: Codacy
.. image:: https://img.shields.io/badge/benchmark-asv-blue.svg?style=flat
   :target: https://empymod.github.io/empymod-asv
   :alt: Airspeed Velocity
.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.593094.svg
   :target: https://doi.org/10.5281/zenodo.593094
   :alt: Zenodo DOI

.. sphinx-inclusion-marker

The electromagnetic modeller **empymod** can model electric or magnetic
responses due to a three-dimensional electric or magnetic source in a
layered-earth model with vertical transverse isotropic (VTI) resistivity, VTI
electric permittivity, and VTI magnetic permeability, from very low frequencies
(DC) to very high frequencies (GPR). The calculation is carried out in the
wavenumber-frequency domain, and various Hankel- and Fourier-transform methods
are included to transform the responses into the space-frequency and space-time
domains.


"
1,empymod-README.md,"More information
","For more information regarding installation, usage, contributing, roadmap, bug
reports, and much more, see

- **Website**: https://empymod.github.io,
- **Documentation**: https://empymod.readthedocs.io,
- **Source Code**: https://github.com/empymod,
- **Examples**: https://github.com/empymod/empymod-examples.


"
2,empymod-README.md,"Features
","- Calculates the complete (diffusion and wave phenomena) 3D electromagnetic
  field in a layered-earth model including vertical transverse isotropic (VTI)
  resistivity, VTI electric permittivity, and VTI magnetic permeability, for
  electric and magnetic sources as well as electric and magnetic receivers.

- Modelling routines:

  - ``bipole``: arbitrary oriented, finite length bipoles with given source
    strength; space-frequency and space-time domains.
  - ``dipole``: infinitesimal small dipoles oriented along the principal axes,
    normalized field; space-frequency and space-time domains.
  - ``wavenumber``: as ``dipole``, but returns the wavenumber-frequency domain
    response.
  - ``gpr``: calculates the ground-penetrating radar response for given central
    frequency, using a Ricker wavelet (experimental).
  - ``analytical``: interface to the analytical, space-frequency and space-time
    domain solutions.

- Hankel transforms (wavenumber-frequency to space-frequency transform):

  - Digital Linear Filters DLF (using included filters or providing own ones)
  - Quadrature with extrapolation QWE
  - Adaptive quadrature QUAD

- Fourier transforms (space-frequency to space-time transform):
  - Digital Linear Filters DLF (using included filters or providing own ones)
  - Quadrature with extrapolation QWE
  - Logarithmic Fast Fourier Transform FFTLog
  - Fast Fourier Transform FFT

- Analytical, space-frequency and space-time domain solutions:

  - Complete full-space (electric and magnetic sources and receivers);
    space-frequency domain
  - Diffusive half-space (electric sources and receivers); space-frequency and
    space-time domains:

    - Direct wave (= diffusive full-space solution)
    - Reflected wave
    - Airwave (semi-analytical in the case of step responses)

- Add-ons (``empymod.scripts``):

  The add-ons for empymod provide some very specific, additional
  functionalities:

  - ``tmtemod``: Return up- and down-going TM/TE-mode contributions for
    x-directed electric sources and receivers, which are located in the same
    layer.
  - ``fdesign``: Design digital linear filters for the Hankel and Fourier
    transforms.
  - ``printinfo``: Can be used to show date, time, and package version
    information at the end of a notebook or a script.


"
3,empymod-README.md,"Installation
","You can install empymod either via ``conda``:

.. code-block:: console

   conda install -c prisae empymod

or via ``pip``:

.. code-block:: console

   pip install empymod

Required are Python version 3.5 or higher and the modules `NumPy` and `SciPy`.
Consult the installation notes in the `manual
<https://empymod.readthedocs.io/en/stable/manual.html#installation>`_ for more
information regarding installation and requirements.


"
4,empymod-README.md,"Citation
","If you publish results for which you used empymod, please give credit by citing
`Werthmüller (2017)  <http://doi.org/10.1190/geo2016-0626.1>`_:

    Werthmüller, D., 2017, An open-source full 3D electromagnetic modeler for
    1D VTI media in Python: empymod: Geophysics, 82(6), WB9--WB19; DOI:
    `10.1190/geo2016-0626.1 <http://doi.org/10.1190/geo2016-0626.1>`_.

All releases have a Zenodo-DOI, provided on the
`release-page <https://github.com/empymod/empymod/releases>`_.
Also consider citing
`Hunziker et al. (2015) <https://doi.org/10.1190/geo2013-0411.1>`_ and
`Key (2012) <https://doi.org/10.1190/geo2011-0237.1>`_, without which
empymod would not exist.


"
5,empymod-README.md,"License information
","Copyright 2016-2019 Dieter Werthmüller

Licensed under the Apache License, Version 2.0. See the ``LICENSE``- and
``NOTICE``-files or the documentation for more information.
"
0,vue-devtools-README.md, vue-devtools,"<p align=""center""><img width=""720px"" src=""https://raw.githubusercontent.com/vuejs/vue-devtools/dev/media/screenshot-shadow.png"" alt=""screenshot""></p>

"
1,vue-devtools-README.md, Installation,"- [Get the Chrome Extension](https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd) / ([beta channel](https://chrome.google.com/webstore/detail/vuejs-devtools/ljjemllljcmogpfapbkkighbhhppjdbg))

- [Get the Firefox Addon](https://addons.mozilla.org/en-US/firefox/addon/vue-js-devtools/) / ([beta channel](https://github.com/vuejs/vue-devtools/releases))

- [Get standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)

"
2,vue-devtools-README.md, Important Usage Notes,"1. If the page uses a production/minified build of Vue.js, devtools inspection is disabled by default so the Vue pane won't show up.

2. To make it work for pages opened via `file://` protocol, you need to check ""Allow access to file URLs"" for this extension in Chrome's extension management panel.

"
3,vue-devtools-README.md, Open component in editor,"To enable this feature, follow [this guide](./docs/open-in-editor.md).

"
4,vue-devtools-README.md, Manual Installation,"This is only necessary when you want to build the extension yourself from source to get not-yet-released features.

**Make sure you are using Node 6+ and NPM 3+**

1. Clone this repo
2. `npm install` (Or `yarn install` if you are using yarn as the package manager)
3. `npm run build`
4. Open Chrome extension page
5. Check ""developer mode""
6. Click ""load unpacked extension"", and choose `shells/chrome`.

"
5,vue-devtools-README.md, Development,"1. Clone this repo
2. `npm install`
3. `npm run dev`
4. A plain shell with a test app will be available at `localhost:8100`.

"
6,vue-devtools-README.md, Quick Start in chrome,"```
// Before you create app
Vue.config.devtools = process.env.NODE_ENV === 'development'

// After you create app
window.__VUE_DEVTOOLS_GLOBAL_HOOK__.Vue = app.constructor;

// then had to add in ./store.js as well.
Vue.config.devtools = process.env.NODE_ENV === 'development'

```

"
7,vue-devtools-README.md, Testing as Firefox addon," 1. Install `web-ext`

	~~~~
	$ npm install --global web-ext
	~~~~

	Or, for Yarn:

	~~~~
	$ yarn global add web-ext
	~~~~

	Also, make sure `PATH` is set up. Something like this in `~/.bash_profile`:

	~~~~
	$ PATH=$PATH:$(yarn global bin)
	~~~~

 2. Build and run in Firefox

	~~~~
	$ npm run build
	$ npm run run:firefox
	~~~~

	When using Yarn, just replace `npm` with `yarn`.


"
8,vue-devtools-README.md, Common problems and how to fix,"1. Fixing ""Download the Vue Devtools for a better development experience"" console message when working locally over `file://` protocol:
  1.1 - Google Chrome: Right click on vue-devtools icon and click ""Manage Extensions"" then search for vue-devtools on the extensions list. Check the ""Allow access to file URLs"" box.

2. How to use the devtools in IE/Edge/Safari or any other browser? [Get the standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)


"
9,vue-devtools-README.md, License,"[MIT](http://opensource.org/licenses/MIT)
"
1,pyro-ppl-pyro-README.md, Installing a stable Pyro release,"**Install using pip:**

Pyro supports Python 3.4+.

```sh
pip install pyro-ppl
```

**Install from source:**
```sh
git clone git@github.com:pyro-ppl/pyro.git
cd pyro
git checkout master  #: master is pinned to the latest release
pip install .
```

**Install with extra packages:**

To install the dependencies required to run the probabilistic models included in the `examples`/`tutorials` directories, please use the following command:
```sh
pip install pyro-ppl[extras] 
```
Make sure that the models come from the same release version of the [Pyro source code](https://github.com/pyro-ppl/pyro/releases) as you have installed.

"
2,pyro-ppl-pyro-README.md, Installing Pyro dev branch,"For recent features you can install Pyro from source.

**Install using pip:**

```sh
pip install git+https://github.com/pyro-ppl/pyro.git
```

or, with the `extras` dependency to run the probabilistic models included in the `examples`/`tutorials` directories:
```sh
pip install git+https://github.com/pyro-ppl/pyro.git#:egg=project[extras]
```

**Install from source:**

```sh
git clone https://github.com/pyro-ppl/pyro
cd pyro
pip install .  #: pip install .[extras] for running models in examples/tutorials
```

"
3,pyro-ppl-pyro-README.md, Running Pyro from a Docker Container,"Refer to the instructions [here](docker/README.md).

"
4,pyro-ppl-pyro-README.md, Citation,"If you use Pyro, please consider citing:
```
@article{bingham2018pyro,
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and
            Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and
            Horsfall, Paul and Goodman, Noah D.},
  title = {{Pyro: Deep Universal Probabilistic Programming}},
  journal = {arXiv preprint arXiv:1810.09538},
  year = {2018}
}
```
"
0,LapSRN-README.md, Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017),"[Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), 
[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/), 
[Narendra Ahuja](http://vision.ai.illinois.edu/ahuja.html), 
and [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)

IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017

"
1,LapSRN-README.md, Table of Contents,"1. [Introduction](#introduction)
1. [Citation](#citation)
1. [Requirements and Dependencies](#requirements-and-dependencies)
1. [Installation](#installation)
1. [Test Pre-trained Models](#test-pre-trained-models)
1. [Training LapSRN](#training-lapsrn)
1. [Training MS-LapSRN](#training-ms-lapsrn)
1. [Third-Party Implementation](#third-party-implementation)

"
2,LapSRN-README.md, Introduction,"The Laplacian Pyramid Super-Resolution Network (LapSRN) is a progressive super-resolution model that super-resolves an low-resolution images in a coarse-to-fine Laplacian pyramid framework.
Our method is fast and achieves state-of-the-art performance on five benchmark datasets for 4x and 8x SR.
For more details and evaluation results, please check out our [project webpage](http://vllab.ucmerced.edu/wlai24/LapSRN/) and [paper](http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN.pdf).

![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)



"
3,LapSRN-README.md, Citation,"If you find the code and datasets useful in your research, please cite:
    
    @inproceedings{LapSRN,
        author    = {Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan}, 
        title     = {Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution}, 
        booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},
        year      = {2017}
    }
    

"
4,LapSRN-README.md, Requirements and Dependencies,"- MATLAB (we test with MATLAB R2017a on Ubuntu 16.04 and Windows 7)
- Cuda & Cudnn (we test with Cuda 8.0 and Cudnn 5.1)

"
5,LapSRN-README.md, Installation,"Download repository:

    $ git clone https://github.com/phoenix104104/LapSRN.git

Run install.m in MATLAB to compile MatConvNet:

    "
6,LapSRN-README.md, Start MATLAB,"    $ matlab
    >> install
   
If you install MatConvNet in your own path, you need to change the corresponding path in `install.m`, `train_LapSRN.m` and `test_LapSRN.m`.

"
7,LapSRN-README.md, Test Pre-trained Models,"To test LapSRN / MS-LapSRN on a single-image:

    >> demo_LapSRN
    >> demo_MSLapSRN

This script will load the pretrained LapSRN / MS-LapSRN model and apply SR on emma.jpg.

To test LapSRN / MS-LapSRN on benchmark datasets, first download the testing datasets:

    $ cd datasets
    $ wget http://vllab1.ucmerced.edu/~wlai24/LapSRN/results/SR_testing_datasets.zip
    $ unzip SR_testing_datasets.zip
    $ cd ..

Then choose the evaluated dataset and upsampling scale in `evaluate_LapSRN_dataset.m` and `evaluate_MSLapSRN_dataset.m`, and run:

    >> evaluate_LapSRN_dataset
    >> evaluate_MSLapSRN_dataset

which can reproduce the results in our paper.


"
8,LapSRN-README.md, Training LapSRN,"To train LapSRN from scratch, first download the training datasets:

    $ cd datasets
    $ wget http://vllab1.ucmerced.edu/~wlai24/LapSRN/results/SR_training_datasets.zip
    $ unzip SR_train_datasets.zip
    $ cd ..

or use the provided bash script to download all datasets and unzip at once:

    $ cd datasets
    $ ./download_SR_datasets.sh
    $ cd ..

Then, setup training options in `init_LapSRN_opts.m`, and run `train_LapSRN(scale, depth, gpuID)`. For example, to train LapSRN with depth = 10 for 4x SR using GPU ID = 1:

    >> train_LapSRN(4, 10, 1)
    
Note that we only test our code on single-GPU mode. MatConvNet supports training with multiple GPUs but you may need to modify our script and options (e.g., `opts.gpu`).

To test your trained LapSRN model, use `test_LapSRN(model_name, epoch, dataset, test_scale, gpu)`. For example, test LapSRN with depth = 10, scale = 4, epoch = 10 on Set5:

    >> test_LapSRN('LapSRN_x4_depth10_L1_train_T91_BSDS200_pw128_lr1e-05_step50_drop0.5_min1e-06_bs64', 10, 'Set5', 4, 1)

which will report the PSNR and SSIM.


"
9,LapSRN-README.md, Training MS-LapSRN,"Setup training options in `init_MSLapSRN_opts.m`, and run `train_MSLapSRN(scales, depth, recursive, gpuID)`, where `scales` should be a vector, e.g., [2, 4, 8]. For example, to train MS-LapSRN with D = 5, R = 2 for 2x, 4x and 8x SR:

    >> train_MSLapSRN([2, 4, 8], 5, 2, 1)
    
To test your trained MS-LapSRN model, use `test_MS-LapSRN(model_name, model_scale, epoch, dataset, test_scale, gpu)`, where `model_scale` is used to define the number of pyramid levels. `test_scale` could be different from `model_scale`. For example, test MS-LapSRN-D5R2 with two pyramid levels (`model_scale = 4`), epoch = 10, on Set5 for 3x SR:

    >> test_MSLapSRN('MSLapSRN_x248_SS_D5_R2_fn64_L1_train_T91_BSDS200_pw128_lr5e-06_step100_drop0.5_min1e-06_bs64', 4, 10, 'Set5', 3, 1)

which will report the PSNR and SSIM.

"
10,LapSRN-README.md, Third-Party Implementation,"- [Pytorch](https://github.com/twtygqyy/pytorch-LapSRN)
- [TensorFlow](https://github.com/zjuela/LapSRN-tensorflow)
"
0,bootstrap-README.md, Table of contents,"- [Quick start](#quick-start)
- [Status](#status)
- [What's included](#whats-included)
- [Bugs and feature requests](#bugs-and-feature-requests)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [Community](#community)
- [Versioning](#versioning)
- [Creators](#creators)
- [Thanks](#thanks)
- [Copyright and license](#copyright-and-license)


"
1,bootstrap-README.md, Quick start,"Several quick start options are available:

- [Download the latest release.](https://github.com/twbs/bootstrap/archive/v4.3.1.zip)
- Clone the repo: `git clone https://github.com/twbs/bootstrap.git`
- Install with [npm](https://www.npmjs.com/): `npm install bootstrap`
- Install with [yarn](https://yarnpkg.com/): `yarn add bootstrap@4.3.1`
- Install with [Composer](https://getcomposer.org/): `composer require twbs/bootstrap:4.3.1`
- Install with [NuGet](https://www.nuget.org/): CSS: `Install-Package bootstrap` Sass: `Install-Package bootstrap.sass`

Read the [Getting started page](https://getbootstrap.com/docs/4.3/getting-started/introduction/) for information on the framework contents, templates and examples, and more.


"
2,bootstrap-README.md, Status,"[![Slack](https://bootstrap-slack.herokuapp.com/badge.svg)](https://bootstrap-slack.herokuapp.com/)
[![Build Status](https://img.shields.io/travis/twbs/bootstrap/master.svg)](https://travis-ci.org/twbs/bootstrap)
[![npm version](https://img.shields.io/npm/v/bootstrap.svg)](https://www.npmjs.com/package/bootstrap)
[![Gem version](https://img.shields.io/gem/v/bootstrap.svg)](https://rubygems.org/gems/bootstrap)
[![Meteor Atmosphere](https://img.shields.io/badge/meteor-twbs%3Abootstrap-blue.svg)](https://atmospherejs.com/twbs/bootstrap)
[![Packagist Prerelease](https://img.shields.io/packagist/vpre/twbs/bootstrap.svg)](https://packagist.org/packages/twbs/bootstrap)
[![NuGet](https://img.shields.io/nuget/vpre/bootstrap.svg)](https://www.nuget.org/packages/bootstrap/absoluteLatest)
[![peerDependencies Status](https://img.shields.io/david/peer/twbs/bootstrap.svg)](https://david-dm.org/twbs/bootstrap?type=peer)
[![devDependency Status](https://img.shields.io/david/dev/twbs/bootstrap.svg)](https://david-dm.org/twbs/bootstrap?type=dev)
[![Coverage Status](https://img.shields.io/coveralls/github/twbs/bootstrap/master.svg)](https://coveralls.io/github/twbs/bootstrap?branch=master)
[![CSS gzip size](https://img.badgesize.io/twbs/bootstrap/master/dist/css/bootstrap.min.css?compression=gzip&label=CSS+gzip+size)](https://github.com/twbs/bootstrap/tree/master/dist/css/bootstrap.min.css)
[![JS gzip size](https://img.badgesize.io/twbs/bootstrap/master/dist/js/bootstrap.min.js?compression=gzip&label=JS+gzip+size)](https://github.com/twbs/bootstrap/tree/master/dist/js/bootstrap.min.js)
[![BrowserStack Status](https://www.browserstack.com/automate/badge.svg?badge_key=SkxZcStBeExEdVJqQ2hWYnlWckpkNmNEY213SFp6WHFETWk2bGFuY3pCbz0tLXhqbHJsVlZhQnRBdEpod3NLSDMzaHc9PQ==--3d0b75245708616eb93113221beece33e680b229)](https://www.browserstack.com/automate/public-build/SkxZcStBeExEdVJqQ2hWYnlWckpkNmNEY213SFp6WHFETWk2bGFuY3pCbz0tLXhqbHJsVlZhQnRBdEpod3NLSDMzaHc9PQ==--3d0b75245708616eb93113221beece33e680b229)
[![Backers on Open Collective](https://img.shields.io/opencollective/backers/bootstrap.svg)](#backers)
[![Sponsors on Open Collective](https://img.shields.io/opencollective/sponsors/bootstrap.svg)](#sponsors)


"
3,bootstrap-README.md, What's included,"Within the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:

```text
bootstrap/
└── dist/
    ├── css/
    │   ├── bootstrap-grid.css
    │   ├── bootstrap-grid.css.map
    │   ├── bootstrap-grid.min.css
    │   ├── bootstrap-grid.min.css.map
    │   ├── bootstrap-reboot.css
    │   ├── bootstrap-reboot.css.map
    │   ├── bootstrap-reboot.min.css
    │   ├── bootstrap-reboot.min.css.map
    │   ├── bootstrap.css
    │   ├── bootstrap.css.map
    │   ├── bootstrap.min.css
    │   └── bootstrap.min.css.map
    └── js/
        ├── bootstrap.bundle.js
        ├── bootstrap.bundle.js.map
        ├── bootstrap.bundle.min.js
        ├── bootstrap.bundle.min.js.map
        ├── bootstrap.esm.js
        ├── bootstrap.esm.js.map
        ├── bootstrap.esm.min.js
        ├── bootstrap.esm.min.js.map
        ├── bootstrap.js
        ├── bootstrap.js.map
        ├── bootstrap.min.js
        └── bootstrap.min.js.map
```

We provide compiled CSS and JS (`bootstrap.*`), as well as compiled and minified CSS and JS (`bootstrap.min.*`). [source maps](https://developers.google.com/web/tools/chrome-devtools/javascript/source-maps) (`bootstrap.*.map`) are available for use with certain browsers' developer tools. Bundled JS files (`bootstrap.bundle.js` and minified `bootstrap.bundle.min.js`) include [Popper](https://popper.js.org/).


"
4,bootstrap-README.md, Bugs and feature requests,"Have a bug or a feature request? Please first read the [issue guidelines](https://github.com/twbs/bootstrap/blob/master/.github/CONTRIBUTING.md#using-the-issue-tracker) and search for existing and closed issues. If your problem or idea is not addressed yet, [please open a new issue](https://github.com/twbs/bootstrap/issues/new).


"
5,bootstrap-README.md, Documentation,"Bootstrap's documentation, included in this repo in the root directory, is built with [Hugo](https://gohugo.io/) and publicly hosted on GitHub Pages at <https://getbootstrap.com/>. The docs may also be run locally.

Documentation search is powered by [Algolia's DocSearch](https://community.algolia.com/docsearch/). Working on our search? Be sure to set `debug: true` in `site/static/docs/4.3/assets/js/src/search.js` file.

"
6,bootstrap-README.md, Running documentation locally,"1. Run `npm install` to install the Node.js dependencies, including Hugo (the site builder).
2. Run `npm run test` (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.
3. From the root `/bootstrap` directory, run `npm run docs-serve` in the command line.
4. Open `http://localhost:9001/` in your browser, and voilà.

Learn more about using Hugo by reading its [documentation](https://gohugo.io/documentation/).

"
7,bootstrap-README.md, Documentation for previous releases,"You can find all our previous releases docs on <https://getbootstrap.com/docs/versions/>.

[Previous releases](https://github.com/twbs/bootstrap/releases) and their documentation are also available for download.


"
8,bootstrap-README.md, Contributing,"Please read through our [contributing guidelines](https://github.com/twbs/bootstrap/blob/master/.github/CONTRIBUTING.md). Included are directions for opening issues, coding standards, and notes on development.

Moreover, if your pull request contains JavaScript patches or features, you must include [relevant unit tests](https://github.com/twbs/bootstrap/tree/master/js/tests). All HTML and CSS should conform to the [Code Guide](https://github.com/mdo/code-guide), maintained by [Mark Otto](https://github.com/mdo).

Editor preferences are available in the [editor config](https://github.com/twbs/bootstrap/blob/master/.editorconfig) for easy use in common text editors. Read more and download plugins at <https://editorconfig.org/>.


"
9,bootstrap-README.md, Community,"Get updates on Bootstrap's development and chat with the project maintainers and community members.

- Follow [@getbootstrap on Twitter](https://twitter.com/getbootstrap).
- Read and subscribe to [The Official Bootstrap Blog](https://blog.getbootstrap.com/).
- Join [the official Slack room](https://bootstrap-slack.herokuapp.com/).
- Chat with fellow Bootstrappers in IRC. On the `irc.freenode.net` server, in the `##bootstrap` channel.
- Implementation help may be found at Stack Overflow (tagged [`bootstrap-4`](https://stackoverflow.com/questions/tagged/bootstrap-4)).
- Developers should use the keyword `bootstrap` on packages which modify or add to the functionality of Bootstrap when distributing through [npm](https://www.npmjs.com/browse/keyword/bootstrap) or similar delivery mechanisms for maximum discoverability.


"
10,bootstrap-README.md, Versioning,"For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under [the Semantic Versioning guidelines](https://semver.org/). Sometimes we screw up, but we adhere to those rules whenever possible.

See [the Releases section of our GitHub project](https://github.com/twbs/bootstrap/releases) for changelogs for each release version of Bootstrap. Release announcement posts on [the official Bootstrap blog](https://blog.getbootstrap.com/) contain summaries of the most noteworthy changes made in each release.


"
11,bootstrap-README.md, Creators,"**Mark Otto**

- <https://twitter.com/mdo>
- <https://github.com/mdo>

**Jacob Thornton**

- <https://twitter.com/fat>
- <https://github.com/fat>


"
12,bootstrap-README.md, Thanks,"<a href=""https://www.browserstack.com/"">
  <img src=""https://live.browserstack.com/images/opensource/browserstack-logo.svg"" alt=""BrowserStack Logo"" width=""192"" height=""42"">
</a>

Thanks to [BrowserStack](https://www.browserstack.com/) for providing the infrastructure that allows us to test in real browsers!


"
13,bootstrap-README.md, Backers,"Thank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/bootstrap#backer)]

[![Bakers](https://opencollective.com/bootstrap/backers.svg?width=890)](https://opencollective.com/bootstrap#backers)


"
14,bootstrap-README.md, Sponsors,"Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [[Become a sponsor](https://opencollective.com/bootstrap#sponsor)]

[![](https://opencollective.com/bootstrap/sponsor/0/avatar.svg)](https://opencollective.com/bootstrap/sponsor/0/website)
[![](https://opencollective.com/bootstrap/sponsor/1/avatar.svg)](https://opencollective.com/bootstrap/sponsor/1/website)
[![](https://opencollective.com/bootstrap/sponsor/2/avatar.svg)](https://opencollective.com/bootstrap/sponsor/2/website)
[![](https://opencollective.com/bootstrap/sponsor/3/avatar.svg)](https://opencollective.com/bootstrap/sponsor/3/website)
[![](https://opencollective.com/bootstrap/sponsor/4/avatar.svg)](https://opencollective.com/bootstrap/sponsor/4/website)
[![](https://opencollective.com/bootstrap/sponsor/5/avatar.svg)](https://opencollective.com/bootstrap/sponsor/5/website)
[![](https://opencollective.com/bootstrap/sponsor/6/avatar.svg)](https://opencollective.com/bootstrap/sponsor/6/website)
[![](https://opencollective.com/bootstrap/sponsor/7/avatar.svg)](https://opencollective.com/bootstrap/sponsor/7/website)
[![](https://opencollective.com/bootstrap/sponsor/8/avatar.svg)](https://opencollective.com/bootstrap/sponsor/8/website)
[![](https://opencollective.com/bootstrap/sponsor/9/avatar.svg)](https://opencollective.com/bootstrap/sponsor/9/website)


"
15,bootstrap-README.md, Copyright and license,"Code and documentation copyright 2011-2019 the [Bootstrap Authors](https://github.com/twbs/bootstrap/graphs/contributors) and [Twitter, Inc.](https://twitter.com) Code released under the [MIT License](https://github.com/twbs/bootstrap/blob/master/LICENSE). Docs released under [Creative Commons](https://github.com/twbs/bootstrap/blob/master/docs/LICENSE).
"
0,facebookresearch-wav2letter-README.md, wav2letter++,"[![CircleCI](https://circleci.com/gh/facebookresearch/wav2letter.svg?style=svg)](https://circleci.com/gh/facebookresearch/wav2letter)

wav2letter++ is a fast open source speech processing toolkit from the Speech Team at Facebook AI Research.
It is written entirely in C++ and uses the [ArrayFire](https://github.com/arrayfire/arrayfire) tensor library and the [flashlight](https://github.com/facebookresearch/flashlight) machine learning library for maximum efficiency.
Our approach is detailed in this [arXiv paper](https://arxiv.org/abs/1812.07625).

The goal of this software is to facilitate research in end-to-end models for speech recognition.

The previous version of wav2letter (written in Lua) can be found in the ""wav2letter-lua"" branch under the repository.

"
1,facebookresearch-wav2letter-README.md, Building wav2letter++,"See [Building Instructions](docs/installation.md) for details.

"
2,facebookresearch-wav2letter-README.md, Full documentation,"- [Data Preparation](docs/data_prep.md)
- [Training](docs/train.md)
- [Testing / Decoding](docs/decoder.md)

To get started with wav2letter++, checkout the [tutorials](tutorials) section.

We also provide complete recipes for WSJ, Timit and Librispeech and they can be found in [recipes](recipes) folder.

"
3,facebookresearch-wav2letter-README.md, Citation,"If you use the code in your paper, then please cite it as:

```
@article{pratap2018w2l,
  author          = {Vineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, Ronan Collobert},
  title           = {wav2letter++: The Fastest Open-source Speech Recognition System},
  journal         = {CoRR},
  volume          = {abs/1812.07625},
  year            = {2018},
  url             = {https://arxiv.org/abs/1812.07625},
}
```

"
4,facebookresearch-wav2letter-README.md, Join the wav2letter community,"* Facebook page: https://www.facebook.com/groups/717232008481207/
* Google group: https://groups.google.com/forum/#!forum/wav2letter-users
* Contact: vineelkpratap@fb.com, awni@fb.com, qiantong@fb.com, jcai@fb.com, jacobkahn@fb.com, gab@fb.com, vitaliy888@fb.com, locronan@fb.com

See the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.

"
5,facebookresearch-wav2letter-README.md, License,"wav2letter++ is BSD-licensed, as found in the [LICENSE](LICENSE) file.
"
0,apsg-README.md, APSG - python package for structural geologists,"[![GitHub version](https://badge.fury.io/gh/ondrolexa%2Fapsg.svg)](https://badge.fury.io/gh/ondrolexa%2Fapsg)
[![Build Status](https://travis-ci.org/ondrolexa/apsg.svg?branch=master)](https://travis-ci.org/ondrolexa/apsg)
[![Documentation Status](https://readthedocs.org/projects/apsg/badge/?version=stable)](https://apsg.readthedocs.io/en/stable/?badge=stable)
[![DOI](https://zenodo.org/badge/24879346.svg)](https://zenodo.org/badge/latestdoi/24879346)

APSG defines several new python classes to easily manage, analyze and
visualize orientational structural geology data.

"
2,apsg-README.md, PyPI,"To install APSG, just execute
```
pip install apsg
```
Alternatively, you download the package manually from the Python Package Index [https://pypi.org/project/apsg](https://pypi.org/project/apsg), unzip it, navigate into the package, and use the command:
```
python setup.py install
```
"
3,apsg-README.md, Upgrading via pip,"To upgrade an existing version of APSG from PyPI, execute
```
pip install apsg --upgrade --no-deps
```
Please note that the dependencies (Matplotlib, NumPy and SciPy) will also be upgraded if you omit the `--no-deps` flag; use the `--no-deps` (""no dependencies"") flag if you don't want this.

"
4,apsg-README.md, Installing APSG from the source distribution,"In rare cases, users reported problems on certain systems with the default pip installation command, which installs APSG from the binary distribution (""wheels"") on PyPI. If you should encounter similar problems, you could try to install APSG from the source distribution instead via
```
pip install --no-binary :all: apsg
```
Also, I would appreciate it if you could report any issues that occur when using `pip install apsg` in hope that we can fix these in future releases.

"
5,apsg-README.md, Conda,"The APSG package is also available through `conda-forge`.

"
6,apsg-README.md, Current release info,"| Name | Downloads | Version | Platforms |
| --- | --- | --- | --- |
| [![Conda Recipe](https://img.shields.io/badge/recipe-apsg-green.svg)](https://anaconda.org/conda-forge/apsg) | [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/apsg.svg)](https://anaconda.org/conda-forge/apsg) | [![Conda Version](https://img.shields.io/conda/vn/conda-forge/apsg.svg)](https://anaconda.org/conda-forge/apsg) | [![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/apsg.svg)](https://anaconda.org/conda-forge/apsg) |

"
7,apsg-README.md, Installing apsg,"Installing `apsg` from the `conda-forge` channel can be achieved by adding `conda-forge` to your channels with:

```
conda config --add channels conda-forge
```

Once the `conda-forge` channel has been enabled, `apsg` can be installed with:

```
conda install apsg
```

It is possible to list all of the versions of `apsg` available on your platform with:

```
conda search apsg --channel conda-forge
```

"
8,apsg-README.md, Master version,"The APSG version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing
```
pip install git+git://github.com/ondrolexa/apsg.git
```
Or, you can fork the GitHub repository from [https://github.com/ondrolexa/apsg](https://github.com/ondrolexa/apsg) and install APSG from your local drive via
```
python setup.py install
```

"
9,apsg-README.md, Getting started,"You can see APSG in action in accompanied Jupyter notebook [http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb)

And for fun check how simply you can animate stereonets
[http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb)

"
10,apsg-README.md, Documentation,"Explore the full features of APSG. You can find detailed documentation [here](https://apsg.readthedocs.org).

"
11,apsg-README.md, Contributing,"Most discussion happens on [Github](https://github.com/ondrolexa/apsg). Feel free to open [an issue](https://github.com/ondrolexa/apsg/issues/new) or comment on any open issue or pull request. Check ``CONTRIBUTING.md`` for more details.

"
12,apsg-README.md, Donate,"APSG is an open-source project, available for you for free. It took a lot of time and resources to build this software. If you find this software useful and want to support its future development please consider donating me.

[![Donate via PayPal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=QTYZWVUNDUAH8&item_name=APSG+development+donation&currency_code=EUR&source=url)

"
13,apsg-README.md, License,"APSG is free software: you can redistribute it and/or modify it under the terms of the MIT License. A copy of this license is provided in ``LICENSE`` file.
"
1,DCPDN-README.md, Densely Connected Pyramid Dehazing Network (CVPR'2018),"[He Zhang](https://sites.google.com/site/hezhangsprinter), [Vishal M. Patel](http://www.rci.rutgers.edu/~vmp93/)

[[Paper Link](https://arxiv.org/abs/1803.08396)] (CVPR'18)

We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the
corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.

	@inproceedings{dehaze_zhang_2018,		
	  title={Densely Connected Pyramid Dehazing Network},
	  author={Zhang, He and Patel, Vishal M},
	  booktitle={CVPR},
	  year={2018}
	} 

<p align=""center"">
<img src=""demo_image/over_input1.png"" width=""250px"" height=""200px""/>         <img src=""demo_image/over_our.png"" width=""250px"" height=""200px""/>



"
2,DCPDN-README.md, Prerequisites:,"1. Linux
2. Python 2 or 3
3. CPU or NVIDIA GPU + CUDA CuDNN (CUDA 8.0)
 
"
3,DCPDN-README.md, Installation:,"1. Install PyTorch and dependencies from http://pytorch.org (Ubuntu+Python2.7)
   (conda install pytorch torchvision -c pytorch)
Install pytorch 0.3.1 https://pytorch.org/previous-versions/
2. Install Torch vision from the source.
   
   	git clone https://github.com/pytorch/vision
	
   	cd vision
	
	python setup.py install

3. Install python package: 
   numpy, scipy, PIL, pdb
   
"
4,DCPDN-README.md, Demo using pre-trained model,"	python demo.py --dataroot ./facades/nat_new4 --valDataroot ./facades/nat_new4 --netG ./demo_model/netG_epoch_8.pth   
Pre-trained dehazing model can be downloaded at (put it in the folder 'demo_model'): https://drive.google.com/drive/folders/1BmNP5ZUWEFeGGEL1NsZSRbYPyjBQ7-nn?usp=sharing

Testing images (nature)  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1q5bRQGgS8SFEGqMwrLlku4Ad-0Tn3va7?usp=sharing

Testing images (syn (Test A in the paper))  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1hbwYCzoI3R3o2Gj_kfT6GHG7RmYEOA-P?usp=sharing


"
5,DCPDN-README.md, Training (Fine-tuning),"	python train.py --dataroot ./facades/train512 --valDataroot ./facades/test512 --exp ./checkpoints_new --netG ./demo_model/netG_epoch_8.pth
More training details (especially how to repreduce the results using stage-wise training strategy) can be found in the paper. 

"
6,DCPDN-README.md, Testing,"	python demo.py --dataroot ./your_dataroot --valDataroot ./your_dataroot --netG ./pre_trained/netG_epoch_9.pth   

"
7,DCPDN-README.md, Reproduce,"To reproduce the quantitative results shown in the paper, please save both generated and target using python demo.py  into the .png format and then test using offline tool such as the PNSR and SSIM measurement in Python or Matlab.   In addition, please use netG.train() for testing since the batch for training is 1. 


"
8,DCPDN-README.md, Dataset,"Training images (syn)  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1Qv7SIZBVAtb9G1d6iVKu_8rVSsXJdv26?usp=sharing

All the syn samples (both training and testing) are strored in Hdf5 file.
You can also generate your sample using 'create_train.py'
(Please download the NYU-depth @ http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat)

Following are the sample python codes how to read the Hdf5 file:
    
    file_name=self.root+'/'+str(index)+'.h5'
    f=h5py.File(file_name,'r')

    haze_image=f['haze'][:]
    gt_trans_map=f['trans'][:]
    gt_ato_map=f['ato'][:]
    GT=f['gt'][:]

Testing images (nature)  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1q5bRQGgS8SFEGqMwrLlku4Ad-0Tn3va7?usp=sharing

Testing images (syn (Test A in the paper))  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1hbwYCzoI3R3o2Gj_kfT6GHG7RmYEOA-P?usp=sharing

"
9,DCPDN-README.md, How to creat your own testing samples,"Since the proposed methods using hdf5 file to load the training samples, the generate_testsample.py help you to creat the testing or training sample yourself. 


"
10,DCPDN-README.md, Extension,"The proposed transmission net has demonstrated it effectiveness in multiple appplcaitions such as segmentation. crowd counting, face reconstruction from sparse sample and image synthesis. 

It has also been asked by other researchers and used for participating the [NTIRE-2018](http://www.vision.ee.ethz.ch/en/ntire18/) dehazing challenge and the proposed netowrk has demonstrated effectivenss from the performance in leaderboard. 

"
11,DCPDN-README.md, Acknowledgments,"Great thanks for the insight discussion with [Vishwanath Sindagi](http://www.vishwanathsindagi.com/) and initial discussion with [Dr. Kevin S. Zhou](https://sites.google.com/site/skevinzhou/home)

This work is under MIT license.
"
0,gempy-README.md," <p align=""left""><img src=""docs/logos/gempy1.png"" width=""300""></p>","> Open-source, implicit 3D structural geological modeling in Python for uncertainty analysis.


[![PyPI](https://img.shields.io/badge/python-3-blue.svg)](https://www.python.org/downloads/)
[![PyPI](https://img.shields.io/badge/pypi-1.0-blue.svg)](https://pypi.org/project/gempy/)
[![license: LGPL v3](https://img.shields.io/badge/license-LGPL%20v3-blue.svg)]()
[![Documentation Status](https://readthedocs.org/projects/gempy/badge/?version=latest)](http://gempy.readthedocs.io/?badge=latest)
[![Travis Build](https://travis-ci.org/cgre-aachen/gempy.svg?branch=master)]()
[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/cgre-aachen/gempy/master)
[![DOI](https://zenodo.org/badge/96211155.svg)](https://zenodo.org/badge/latestdoi/96211155)
[![DOCKER](https://img.shields.io/docker/cloud/automated/leguark/gempy.svg)](https://cloud.docker.com/repository/docker/leguark/gempy)

<p align=""center""><img src=""docs/source/images/model_examples.png"" width=""800""></p>

"
1,gempy-README.md, What is it,"*GemPy* is a Python-based, open-source library for **implicitly generating 3D structural geological models**. It is capable of
constructing complex 3D geological models of folded structures, fault networks and unconformities. It was designed from the 
ground up to support easy embedding in probabilistic frameworks for the uncertainty analysis of subsurface structures.

Check out the documentation either in [gempy.org](https://www.gempy.org/) (better option), or [read the docs](http://gempy.readthedocs.io/).

"
2,gempy-README.md, Table of Contents,"* [Features](#feat)
    * [Sandbox](#sandbox)
    * [Remote Geomod](#remotegeo)
* [Getting Started](#getstart)
    * [Dependencies](#depend)
    * [Installation](#installation)
* [Documentation](#doc)
* [References](#ref)

<a name=""feat""></a>
"
4,gempy-README.md, GemPy v2.0 beta release,"It has been a long journey since the release of GemPy v1.0. What started as a small library to carry out research
on uncertainty analysis for structural geology has grown to be used in multiple projects around the world. Carried
by the community enthusiasm, we commenced a way-longer-than-planned rewritten of the code in order to
not only be able to fulfill the needs of many of you but also to set the foundations of a package driven by the
community. For this end, all the logic has been splat into multiple modules, classes and containers limiting
duplicities and exposing a large mutation api at different levels of abstraction. Hope the work has been worth it.

So long,

Miguel

"
5,gempy-README.md, What is new,"- Full redesign of the back-end: much more modular, explicit and avoiding object duplicities to insane levels
- Topography
- Onlap, Erosion relations
- Choose your favourite type of fault: infinite faults, finite faults, faults offsetting faults, faults ending on series
- Masked marching cubes: this fix the ugly surfaces following the voxels faces
- All series are fully stored after interpolation
- Save your model
- Compile once, modify as much as you want
- Full integration with qgrid
- Real time computations via vtk or python-qgrid
- Adaptive regular grids for geophysics
- Refactored some legacy names:
    + formations renamed to surfaces
    + interfaces renamed to surfaces_points
- Minor changes:
    + New colormap and easy way to change the surfaces colors (even integration with widgets!)
    + The order of the formations will be given by the interpolation itself if the input was wrong
    + The split between reference and rest surface_points happens in theano. This makes much easier the modification
    of reference points


"
6,gempy-README.md, Features,"The core algorithm of *GemPy* is based on a universal cokriging interpolation method devised by
Lajaunie et al. (1997) and extended by Calcagno et al. (2008). Its implicit nature allows the user to automatically
generate complex 3D structural geological models through the interpolation of input data:

- *Surface contact points*: 3D coordinates of points marking the boundaries between different features (e.g. layer interfaces, fault planes, unconformities).
- *Orientation measurements*: Orientation of the poles perpendicular to the dipping of surfaces at any point in the 3D space.

*GemPy* also allows for the definition of topological elements such as combining multiple stratigraphic sequences and 
complex fault networks to be considered in the modeling process.

<p align=""center""><img src=""docs/source/images/modeling_principle.png"" width=""600""></p>

*GemPy* itself offers direct visualization of 2D model sections via matplotlib
and in full, interactive 3D using the Visualization Toolkit (VTK). The VTK support also allow to the real time maniulation
of the 3-D model, allowing for the exact modification of data. Models can also easily be exportes in VTK file format
for further visualization and processing in other software such as ParaView.

<p align=""center""><img src=""docs/source/images/vtkFault.png"" width=""600""></p>


*GemPy* was designed from the beginning to support stochastic geological modeling for uncertainty analysis (e.g. Monte Carlo simulations, Bayesian inference). This was achieved by writing *GemPy*'s core architecture
using the numerical computation library [Theano](http://deeplearning.net/software/theano/) to couple it with the probabilistic programming framework [PyMC3](https://pymc-devs.github.io/pymc3/notebooks/getting_started.html).
This enables the use of advanced sampling methods (e.g. Hamiltonian Monte Carlo) and is of particular relevance when considering
uncertainties in the model input data and making use of additional secondary information in a Bayesian inference framework.

We can, for example, include uncertainties with respect to the z-position of layer boundaries
in the model space. Simple Monte Carlo simulation via PyMC will then result in different model realizations:

<p align=""center""><img src=""docs/source/images/gempy_zunc.png"" height=""300""> <img src=""docs/source/images/model_wobble.gif"" height=""300""></p>

Theano allows the automated computation of gradients opening the door to the use of advanced gradient-based sampling methods
coupling *GeMpy* and [PyMC3](https://pymc-devs.github.io/pymc3/notebooks/getting_started.html) for advanced stochastic modeling.
Also, the use of Theano allows making use of GPUs through cuda (see the Theano documentation for more information.

Making use of vtk interactivity and Qgrid (https://github.com/quantopian/qgrid) *GemPy* provides a functional interface to interact with input data and models.

<p align=""center""><a href=""https://youtu.be/aA4MaHpLWVE?t=67""><img src=""https://img.youtube.com/vi/aA4MaHpLWVE/0.jpg"" width=""600""></a></p>




For a more detailed elaboration of the theory behind *GemPy*, take a look at the upcoming scientific publication
*""GemPy 1.0: open-source stochastic geological modeling and inversion""* by de la Varga et al. (2018).

Besides the main functionality GemPy is powering currently some further projects:

<a name=""sandbox""></a>
"
7,gempy-README.md, Sandbox,"New developments in the field of augmented reality, i.e. the superimposition of real and digital objects, offer interesting and diverse possibilities that have hardly been exploited to date.
The aim of the project is therefore the development and realization of an augmented reality sandbox for interaction with geoscientific data and models.
In this project, methods are to be developed to project geoscientific data (such as the outcrop of a geological layer surface or geophysical measurement data) onto real surfaces.

The AR Sandbox is based on a container filled with sand, the surface of which can be shaped as required. The topography of the sand surface is continuously scanned by a 3D sensor and a camera.
In the computer the scanned surface is now blended with a digital geological 3D model (or other data) in real time and an image is calculated, which is projected onto the sand surface by means
of a beamer. This results in an interactive model with which the user can interact in an intuitive way and which visualizes and comprehend complex three-dimensional facts in an accessible way.

In addition to applications in teaching and research, this development offers great potential as an interactive exhibit with high outreach for the geosciences thanks to its intuitive operation.
The finished sandbox can be used in numerous lectures and public events , but is mainly used as an interface to GemPy software and for rapid prototyping of implicit geological models.

<p align=""center""><a href=""https://youtu.be/oE3Atw-YvSA""><img src=""https://img.youtube.com/vi/oE3Atw-YvSA/0.jpg"" width=""600""></p>

<a name=""remotegeo""></a>
"
8,gempy-README.md, Remote Geomod: From GoogleEarth to 3-D Geology,"We support this effort here with a full 3-D geomodeling exercise
on the basis of the excellent possibilities offered by open global data sets, implemented in
GoogleEarth, and dedicated geoscientific open-source software and motivate the use of 3-D
geomodeling to address specific geological questions. Initial steps include the selection of
relevant geological surfaces in GoogleEarth and the analysis of determined orientation values
for a selected region This information is subsequently used
to construct a full 3-D geological model with a state-of-the-art interpolation algorithm. Fi-
nally, the generated model is intersected with a digital elevation model to obtain a geological
map, which can then be reimported into GoogleEarth.

<p align=""center""><img src=""docs/source/images/ge.png"" width=""900""></p>

<a name=""getstart""></a>
"
9,gempy-README.md, Getting Started,"<a name=""depend""></a>
"
10,gempy-README.md, Dependencies,"*GemPy* requires Python 3 and makes use of numerous open-source libraries:

* pandas>=0.21.0
* cython
* Theano
* matplotlib
* numpy
* pytest
* nbsphinx
* seaborn
* networkx
* ipywidgets

Optional:

* git+git://github.com/Leguark/scikit-image@master
* steno3d
* vtk
* gdal
* qgrid
* pymc
* pymc3

* `vtk>=7` for interactive 3-D visualization 
* `pymc` or `pymc3`
* `steno3d` 

Overall we recommend the use of a dedicated Python distribution, such as 
[Anaconda](https://www.continuum.io/what-is-anaconda), for hassle-free package installation. 
We are currently working on providing GemPy also via Anaconda Cloud, for easier installation of
its dependencies.

"
11,gempy-README.md, Conflictive packages.,"Installing Theano (specially in windows) and vtk sometimes is problematic. Here we give a few advices that
usually works for us:
* Theano: install the following packages before installing theano: `conda install mingw libpython m2w64-toolchain`. Then install Theano via `conda install theano`. 
If the installation fails at some point try to re-install anaconda for a single user (no administrator priveleges) and with the Path Environment set.
To use Theano with `numpy version 1.16.0` or following, it has to be updated to `Theano 1.0.4` using `pip install theano --upgrade`.
Note that this is not yet available in the conda package manager.

* scikit_image (Spring 2019): To use scikit_image with `numpy version 1.16.0` or following, it has to be updated to `scikit_image 1.14.2` using `pip install scikit_image --upgrade`.
Note that this is not yet available in the conda package manager.

* vtk: Right now (Fall 2018), does not have compatibility with python 3.7. The simplest solution to install it is to
use `conda install python=3.6` to downgrade the python version and then using `pip install vtk`.

<a name=""installation""></a>
"
12,gempy-README.md, Installation,"We provide the latest release version of *GemPy* via the **Conda** and **PyPi** package services. We highly
recommend using either PyPi as it will take care of automatically installing all dependencies.

"
13,gempy-README.md, PyPi ,"`$ pip install gempy`


"
14,gempy-README.md, New in GemPy 2.0: Docker image,"Finally e also provide precompiled Docker images hosted on Docker Hub with all necessary dependencies to get 
GemPy up and running (**except vtk**).

ocker is an operating-system-level-visualization software,
meaning that we can package a tiny operating system with pre-installed
software into a Docker image. This Docker image can then be shared
with and run by others, enabling them to use intricate dependencies
with just a few commands. For this to work the user needs to have a
working [Docker](https://www.docker.com/) installation.

"
15,gempy-README.md, Pull Docker image from DockerHub,"The easiest way to get remote-geomod running is by running the pre-compiled Docker image (containing everything you
need) directly from the cloud service Docker Hub to get a locally running Docker container. Make sure to set your 
Docker daemon to Linux containers in Docker's context menu.

    $ docker run -it -p 8899:8899 leguark/gempy
    
This will automatically pull the Docker image from Docker Hub and run it, opening a command line shell inside of the
running Docker container. There you have access to the file system inside of the container. Note that this pre-compiled
Docker image already contains the GemPy repository. 

Once you are in the docker console if you want to open the tutorials you will need to run:

    $ jupyter notebook --ip 0.0.0.0 --port 8899 --no-browser --allow-root
     
Notice that we are running the notebook on the port  8899 to try to avoid conflicts with jupyter servers running in
your system. If everything worked fine, the address to the jupyter notebook will be display on the console. It
has to look something like this (Just be aware of the  brackets):

    To access the notebook, open this file in a browser:
            file:///root/.local/share/jupyter/runtime/nbserver-286-open.html
    Or copy and paste one of these URLs:
        http://(ce2cdcc55bb0 or 127.0.0.1):8899/?token=97d52c1dc321c42083d8c1b4d


"
16,gempy-README.md, Manual,"Otherwise you can clone the current repository by downloading is manually or by using Git by calling

`$ git clone https://github.com/cgre-aachen/gempy.git`

and then manually install it using the provided Python install file by calling

`$ python gempy/setup.py install`

in the cloned or downloaded repository folder. Make sure you have installed all necessary dependencies listed above before using *GemPy*.

"
17,gempy-README.md, Windows installation guide (Jun 2019),"1) Install CUDA if you do not have it already.

2) Install Anaconda3 2019.03 with Python 3.7 (this is the last release).

3) Install Theano and associated packages from the Anaconda prompt as administrator, and finally install GemPy 2.0:

- conda update --all
- conda install libpython
- conda install m2w64-toolchain
- conda install git
- conda install pygpu
- pip install theano==1.0.4
- pip install gempy==2.0b0.dev2

Note that:

a) some other packages required by Theano are already included in Anaconda: numpy, scipy, mkl-service, nose, and sphinx.

b) pydot-ng (suggested on Theano web site) yields a lot of errors. I dropped this. It is needed to handle large picture for gif/images and probably it is not needed by GemPy.

c) Trying to install all the packages in one go but it does not work, as well as doing the same in Anaconda Navigator, or installing an older Anaconda release with Python 3.5 (Anaconda3 4.2.0) as indicated in some tutorial on Theano.


<a name=""doc""></a>
"
18,gempy-README.md, Documentation,"Extensive documentation for *GemPy* is hosted at [gempy.readthedocs.io](http://gempy.readthedocs.io/),
explaining its capabilities, [the theory behind it](http://gempy.readthedocs.io/Kriging.html) and 
providing detailed [tutorials](http://gempy.readthedocs.io/tutorial.html) on how to use it.

<a name=""ref""></a>
"
19,gempy-README.md, References,"* de la Varga, M., Schaaf, A., and Wellmann, F.: GemPy 1.0: open-source stochastic geological modeling and inversion, Geosci. Model Dev., 12, 1-32, https://doi.org/10.5194/gmd-12-1-2019, 2019
* Calcagno, P., Chilès, J. P., Courrioux, G., & Guillen, A. (2008). Geological modelling from field data and geological knowledge: Part I. Modelling method coupling 3D potential-field interpolation and geological rules. Physics of the Earth and Planetary Interiors, 171(1-4), 147-157.
* Lajaunie, C., Courrioux, G., & Manuel, L. (1997). Foliation fields and 3D cartography in geology: principles of a method based on potential interpolation. Mathematical Geology, 29(4), 571-584.
"
0,sentinelsat-README.md, connect to the API,"  api = SentinelAPI('user', 'password', 'https://scihub.copernicus.eu/dhus')

  "
1,sentinelsat-README.md, download single scene by known product id,"  api.download(<product_id>)

  "
2,sentinelsat-README.md," search by polygon, time, and Hub query keywords","  footprint = geojson_to_wkt(read_geojson('map.geojson'))
  products = api.query(footprint,
                       date = ('20151219', date(2015, 12, 29)),
                       platformname = 'Sentinel-2',
                       cloudcoverpercentage = (0, 30))

  "
3,sentinelsat-README.md, download all results from the search,"  api.download_all(products)

  "
4,sentinelsat-README.md, GeoJSON FeatureCollection containing footprints and metadata of the scenes,"  api.to_geojson(products)

  "
5,sentinelsat-README.md, GeoPandas GeoDataFrame with the metadata of the scenes and the footprints as geometries,"  api.to_geodataframe(products)

  "
6,sentinelsat-README.md," Get basic information about the product: its title, file size, MD5 sum, date, footprint and",  
7,sentinelsat-README.md, its download url,"  api.get_product_odata(<product_id>)

  "
8,sentinelsat-README.md, Get the product's full metadata available on the server,"  api.get_product_odata(<product_id>, full=True)

Valid search query keywords can be found at the `Copernicus Open Access Hub documentation
<https://scihub.copernicus.eu/userguide/3FullTextSearch>`_.

Command Line Interface
----------------------

A basic search query consists of a search area geometry as well as the username and
password to access the Copernicus Open Access Hub.

.. code-block:: bash

  sentinelsat -u <user> -p <password> -g <geojson>

Search areas are provided as GeoJSON files, which can be created with
`QGIS <http://qgis.org/en/site/>`_ or `geojson.io <http://geojson.io>`_.
If you do not specify a start and end date only products published in the last
24 hours will be queried.

Example
^^^^^^^

Search and download all Sentinel-1 scenes of type SLC, in descending
orbit, for the year 2015.

.. code-block:: bash

  sentinelsat -u <user> -p <password> -g <search_polygon.geojson> -s 20150101 -e 20151231 -d \
  --producttype SLC -q ""orbitdirection=Descending"" \
  --url ""https://scihub.copernicus.eu/dhus""

Username, password and DHuS URL can also be set via environment variables for convenience.

.. code-block:: bash
 
  "
9,sentinelsat-README.md, same result as query above,"  export DHUS_USER=""<user>""
  export DHUS_PASSWORD=""<password>""
  export DHUS_URL=""https://scihub.copernicus.eu/dhus""

  sentinelsat -g <search_polygon.geojson> -s 20150101 -e 20151231 -d \
  --producttype SLC -q ""orbitdirection=Descending""

Options
^^^^^^^

+----+---------------+------+--------------------------------------------------------------------------------------------+
| -u | -\-user       | TEXT | Username [required] (or environment variable DHUS_USER)                                    |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -p | -\-password   | TEXT | Password [required] (or environment variable DHUS_PASSWORD)                                |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-url        | TEXT | Define another API URL. Default URL is 'https://scihub.copernicus.eu/apihub/'.             |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -s | -\-start      | TEXT | Start date of the query in the format YYYYMMDD.                                            |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -e | -\-end        | TEXT | End date of the query in the format YYYYMMDD.                                              |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -g | -\-geometry   | PATH | Search area geometry as GeoJSON file.                                                      |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-uuid       | TEXT | Select a specific product UUID instead of a query. Multiple UUIDs can separated by commas. |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-name       | TEXT | Select specific product(s) by filename. Supports wildcards.                                |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-sentinel   | INT  | Limit search to a Sentinel satellite (constellation).                                      |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-instrument | TEXT | Limit search to a specific instrument on a Sentinel satellite.                             |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-producttype| TEXT | Limit search to a Sentinel product type.                                                   |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -c | -\-cloud      | INT  | Maximum cloud cover in percent. (requires --sentinel to be 2 or 3)                         |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -o | -\-order-by   | TEXT | Comma-separated list of keywords to order the result by. Prefix '-' for descending order.  |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -l | -\-limit      | INT  |  Maximum number of results to return. Defaults to no limit.                                |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -d | -\-download   |      | Download all results of the query.                                                         |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-path       | PATH | Set the path where the files will be saved.                                                |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -q | -\-query      | TEXT | Extra search keywords you want to use in the query. Separate keywords with comma.          |
|    |               |      | Example: 'producttype=GRD,polarisationmode=HH'.                                            |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -f | -\-footprints |      | Create geojson file search_footprints.geojson with footprints of the query result.         |
+----+---------------+------+--------------------------------------------------------------------------------------------+
|    | -\-version    |      | Show version number and exit.                                                              |
+----+---------------+------+--------------------------------------------------------------------------------------------+
| -h | -\-help       |      | Show help message and exit.                                                                |
+----+---------------+------+--------------------------------------------------------------------------------------------+

Tests
=====

To run the tests on ``sentinelsat``:

.. code-block:: bash

    git clone https://github.com/sentinelsat/sentinelsat.git
    cd sentinelsat
    pip install -e .[dev]
    pytest -v

By default, prerecorded responses to Copernicus Open Access Hub queries are used to not be affected by its downtime.
To allow the tests to run actual queries against the Copernicus Open Access Hub set the environment variables

.. code-block:: bash

    export DHUS_USER=<username>
    export DHUS_PASSWORD=<password>

and add ``--disable-vcr`` to ``pytest`` arguments.
To update the recordings use ``--vcr-record`` with ``once``, ``new_episodes`` or ``all``. See `vcrpy docs <https://vcrpy.readthedocs.io/en/latest/usage.html#record-modes>`_ for details.


Documentation
=============

To build the documentation:

.. code-block:: bash

    git clone https://github.com/sentinelsat/sentinelsat.git
    cd sentinelsat
    pip install -e .[dev]
    cd docs
    make html

The full documentation is also published at http://sentinelsat.readthedocs.io/.


Changelog
=========

See `CHANGELOG <CHANGELOG.rst>`_. You can also use GitHub's compare view to see the `changes in the master branch since last release <https://github.com/sentinelsat/sentinelsat/compare/v0.13...master>`_.

Contributors
============

We invite anyone to participate by contributing code, reporting bugs, fixing bugs, writing documentation and tutorials and discussing the future of this project. Please check `CONTRIBUTE.rst <CONTRIBUTE.rst>`_.

For a list of maintainers and contributors please see `AUTHORS.rst <AUTHORS.rst>`_ and the `contributor graph <https://github.com/sentinelsat/sentinelsat/graphs/contributors>`_.

License
=======

GPLv3+
"
0,DeepMVS-README.md, DeepMVS: Learning Multi-View Stereopsis,"[![License](https://img.shields.io/badge/License-BSD%202--Clause-orange.svg)](https://opensource.org/licenses/BSD-2-Clause)

**DeepMVS** is a Deep Convolutional Neural Network which learns to estimate pixel-wise disparity maps from a sequence of an arbitrary number of unordered images with the camera poses already known or estimated. 

![teaser1](img/teaser1.gif)
![teaser2](img/teaser2.gif)

If you use our codes or datasets in your work, please cite:
```
@inproceedings{DeepMVS,
  author       = ""Huang, Po-Han and Matzen, Kevin and Kopf, Johannes and Ahuja, Narendra and Huang, Jia-Bin"",
  title        = ""DeepMVS: Learning Multi-View Stereopsis"",
  booktitle    = ""IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"",
  year         = ""2018""
}
```

For the paper and other details of DeepMVS or the MYS-Synth Dataset, please see our [project webpage](https://phuang17.github.io/DeepMVS/index.html).


"
2,DeepMVS-README.md, Requirements,"- **python 2.7**
- **numpy 1.13.1**
- **pytorch 0.3.0** and **torchvision**: Follow the instructions from [their website](http://pytorch.org/).
- **opencv 3.1.0**: Run ``conda install -c menpo opencv`` or ``pip install opencv-python``.
- **imageio 2.2.0** (with freeimage plugin): Run ``conda install -c conda-forge imageio`` or ``pip install imageio``. To install freeimage plugin, run the following Python script once:
    ```python 
    import imageio
    imageio.plugins.freeimage.download()
    ```
- **h5py 2.7.0**: Run ``conda install h5py`` or ``pip install h5py``.
- **lz4 0.23.1**: Run ``pip install lz4``.
- **cuda 8.0.61** and **16GB GPU RAM** (required for gpu support): The training codes use up to 14GB of the GPU RAM with the default configuration. We train our model with an NVIDIA Tesla P100 GPU. To reduce GPU RAM usage, feel free to try smaller ``--patch_width``, ``--patch_height``, ``--num_depths``, and ``--max_num_neighbors``. However, the resulting model may not show the efficacy as appeared in our paper.

"
3,DeepMVS-README.md, Instructions,"1. Download the training datasets.
    ```bash
    python python/download_training_datasets.py #: This may take up to 1-2 days to complete.
    ```
    **Update: The training datasets have been updated on May 18, 2018 because of some errors in camera poses. Please remove the files and download them again if you have downloaded the old version.**
2. Train the network.
    ```bash
    python python/train.py #: This may take up to 4-6 days to complete, depending on which GPU is used.
    ```

"
5,DeepMVS-README.md, Requirements,"- **python 2.7**
- **numpy 1.13.1**
- **pytorch 0.3.0** and **torchvision**: Follow the instructions from [their website](http://pytorch.org/).
- **opencv 3.1.0**: Run ``conda install -c menpo opencv`` or ``pip install opencv-python``.
- **imageio 2.2.0**: Run ``conda install -c conda-forge imageio`` or ``pip install imageio``.
- **pyquaternion 0.9.0**: Run ``pip install pyquaternion``.
- **pydensecrf**: Run ``pip install pydensecrf``.
- **cuda 8.0.61** and **6GB GPU RAM** (required for gpu support): The testing codes use up to 4GB of the GPU RAM with the default configuration. 
- **COLMAP 3.2**: Follow the instructions from [their website](https://colmap.github.io/). 
 
"
6,DeepMVS-README.md, Instructions,"1. Download the trained model.
    ```bash
    python python/download_trained_model.py
    ```

2. Run the sparse reconstruction and the ``image_undistorter`` using [COLMAP](https://colmap.github.io/). The ``image_undistorter`` will generate a ``images`` folder which contains undistorted images and a ``sparse`` folder which contains three ``.bin`` files.

3. Run the testing script with the paths to the undistorted images and the sparse construction model.
    ```bash
    python python/test.py --load_bin --image_path path/to/images --sparse_path path/to/sparse --output_path path/to/output/directory
    ```
    By default, the script resizes the images to be 540px in height to reduce the running time. If you would like to run the model with other resolutions, please pass the arguments `--image_width XXX` and `--image_height XXX`.
    If your COLMAP outputs ``.txt`` files instead of ``.bin`` files for the sparse reconstruction, simply remove the `--load_bin` flag.

4. To evaluate the predicted results, run
    ```bash
    python python/eval.py --load_bin --image_path path/to/images --sparse_path path/to/sparse --output_path path/to/output/directory --gt_path path/to/gt/directory --image_width 810 --image_height 540 --size_mismatch crop_pad
    ```
    In ``gt_path``, the ground truth disparity maps should be stored in npy format with filenames being ``<image_name>.depth.npy``. If the ground truths are depth maps instead of disparity maps, please add ``--gt_type depth`` flag.

"
7,DeepMVS-README.md, License,DeepMVS is licensed under the [BSD 2-Clause License](LICENSE.txt)
0,Shapely-README.md,"Shapely
",".. image:: https://travis-ci.org/Toblerity/Shapely.svg?branch=master
   :target: https://travis-ci.org/Toblerity/Shapely

.. image:: https://coveralls.io/repos/github/Toblerity/Shapely/badge.svg?branch=master
   :target: https://coveralls.io/github/Toblerity/Shapely?branch=master

Manipulation and analysis of geometric objects in the Cartesian plane.

.. image:: https://c2.staticflickr.com/6/5560/31301790086_b3472ea4e9_c.jpg
   :width: 800
   :height: 378

Shapely is a BSD-licensed Python package for manipulation and analysis of
planar geometric objects. It is based on the widely deployed `GEOS
<http://trac.osgeo.org/geos/>`__ (the engine of `PostGIS
<http://postgis.org>`__) and `JTS
<https://locationtech.github.io/jts/>`__ (from which GEOS is ported)
libraries. Shapely is not concerned with data formats or coordinate systems,
but can be readily integrated with packages that are. For more details, see:

* `Shapely GitHub repository <https://github.com/Toblerity/Shapely>`__
* `Shapely documentation and manual <https://shapely.readthedocs.io/en/latest/>`__

"
1,Shapely-README.md,"Requirements
","Shapely 1.6 requires

* Python 2.7, >=3.4
* GEOS >=3.3 

"
2,Shapely-README.md,"Installing Shapely 1.6
","Shapely may be installed from a source distribution or one of several kinds
of built distribution.

"
3,Shapely-README.md,"Built distributions
","Windows users have two good installation options: the wheels at
http://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely and the 
Anaconda platform's `conda-forge <https://conda-forge.github.io/>`__
channel.

OS X and Linux users can get Shapely wheels with GEOS included from the 
Python Package Index with a recent version of pip (8+):

.. code-block:: console

    $ pip install shapely

A few extra speedups that require Numpy can be had by running

.. code-block:: console

    $ pip install shapely[vectorized]

Shapely is available via system package management tools like apt, yum, and
Homebrew, and is also provided by popular Python distributions like Canopy and
Anaconda.

"
4,Shapely-README.md,"Source distributions
","If you want to build Shapely from source for compatibility with
other modules that depend on GEOS (such as cartopy or osgeo.ogr) or want to
use a different version of GEOS than the one included in the project wheels
you should first install the GEOS library, Cython, and Numpy on your system
(using apt, yum, brew, or other means) and then direct pip to ignore the binary
wheels.

.. code-block:: console

    $ pip install shapely --no-binary shapely

If you've installed GEOS to a standard location, the geos-config program
will be used to get compiler and linker options. If geos-config is not on
your executable, it can be specified with a GEOS_CONFIG environment
variable, e.g.:

.. code-block:: console

    $ GEOS_CONFIG=/path/to/geos-config pip install shapely

"
5,Shapely-README.md,"Usage
","Here is the canonical example of building an approximately circular patch by
buffering a point.

.. code-block:: pycon

    >>> from shapely.geometry import Point
    >>> patch = Point(0.0, 0.0).buffer(10.0)
    >>> patch
    <shapely.geometry.polygon.Polygon object at 0x...>
    >>> patch.area
    313.65484905459385

See the manual for comprehensive usage snippets and the dissolve.py and
intersect.py examples.

"
6,Shapely-README.md,"Integration
","Shapely does not read or write data files, but it can serialize and deserialize
using several well known formats and protocols. The shapely.wkb and shapely.wkt
modules provide dumpers and loaders inspired by Python's pickle module.

.. code-block:: pycon

    >>> from shapely.wkt import dumps, loads
    >>> dumps(loads('POINT (0 0)'))
    'POINT (0.0000000000000000 0.0000000000000000)'

Shapely can also integrate with other Python GIS packages using GeoJSON-like
dicts.

.. code-block:: pycon

    >>> import json
    >>> from shapely.geometry import mapping, shape
    >>> s = shape(json.loads('{""type"": ""Point"", ""coordinates"": [0.0, 0.0]}'))
    >>> s
    <shapely.geometry.point.Point object at 0x...>
    >>> print(json.dumps(mapping(s)))
    {""type"": ""Point"", ""coordinates"": [0.0, 0.0]}

"
7,Shapely-README.md,"Development and Testing
","Dependencies for developing Shapely are listed in requirements-dev.txt. Cython
and Numpy are not required for production installations, only for development.
Use of a virtual environment is strongly recommended.

.. code-block:: console

    $ virtualenv .
    $ source bin/activate
    (env)$ pip install -r requirements-dev.txt
    (env)$ pip install -e .

We use py.test to run Shapely's suite of unittests and doctests.

.. code-block:: console

    (env)$ python -m pytest

"
8,Shapely-README.md,"Support
","Questions about using Shapely may be asked on the `GIS StackExchange 
<http://gis.stackexchange.com/questions/tagged/shapely>`__ using the ""shapely""
tag.

Bugs may be reported at https://github.com/Toblerity/Shapely/issues.
"
0,nextflow-io-nextflow-README.md,Quick overview,"Nextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows.
It supports deploying workflows on a variety of execution platforms including local, HPC schedulers, AWS Batch,
Google Genomics Pipelines, and Kubernetes. Additionally, it provides support for manage your workflow dependencies
through built-in support for Conda, Docker, Singularity, and Modules.

## Contents
- [Rationale](#rationale)
- [Quick start](#quick-start)
- [Documentation](#documentation)
- [Tool Management](#tool-management)
  - [Conda environments](#conda-environments)
  - [Docker and Singularity](#containers)
  - [Environment Modules](#environment-modules)
- [HPC Schedulers](#hpc-schedulers)
  - [SGE](#hpc-schedulers)
  - [Univa Grid Engine](#hpc-schedulers)
  - [LSF](#hpc-schedulers)
  - [SLURM](#hpc-schedulers)
  - [PBS/Torque](#hpc-schedulers)
  - [HTCondor (experimental)](#hpc-schedulers)
- [Cloud Support](#cloud-support)
  - [AWS Batch](#cloud-support)
  - [AWS EC2](#cloud-support)
  - [Google Cloud](#cloud-support)
  - [Google Genomics Pipelines](#cloud-support)
  - [Kubernetes](#cloud-support)
- [Community](#community)
- [Build from source](#build-from-source)
- [Contributing](#contributing)
- [License](#license)
- [Citations](#citations)
- [Credits](#credits)


"
1,nextflow-io-nextflow-README.md,"Rationale
","With the rise of big data, techniques to analyse and run experiments on large datasets are increasingly necessary.

Parallelization and distributed computing are the best ways to tackle this problem, but the tools commonly available to the bioinformatics community often lack good support for these techniques, or provide a model that fits badly with the specific requirements in the bioinformatics domain and, most of the time, require the knowledge of complex tools or low-level APIs.

Nextflow framework is based on the dataflow programming model, which greatly simplifies writing parallel and distributed pipelines without adding unnecessary complexity and letting you concentrate on the flow of data, i.e. the functional logic of the application/algorithm.

It doesn't aim to be another pipeline scripting language yet, but it is built around the idea that the Linux platform is the *lingua franca* of data science, since it provides many simple command line and scripting tools, which by themselves are powerful, but when chained together facilitate complex data manipulations.

In practice, this means that a Nextflow script is defined by composing many different processes. Each process can execute a given bioinformatics tool or scripting language, to which is added the ability to coordinate and synchronize the processes execution by simply specifying their inputs and outputs.



"
3,nextflow-io-nextflow-README.md,"Download the package
","Nextflow does not require any installation procedure, just download the distribution package by copying and pasting
this command in your terminal:

```
curl -fsSL https://get.nextflow.io | bash
```

It creates the ``nextflow`` executable file in the current directory. You may want to move it to a folder accessible from your ``$PATH``.

"
4,nextflow-io-nextflow-README.md,"Download from Conda
","Nextflow can also be installed from Bioconda

```
conda install -c bioconda nextflow 
```

"
5,nextflow-io-nextflow-README.md,"Documentation
","Nextflow documentation is available at this link http://docs.nextflow.io


"
6,nextflow-io-nextflow-README.md,"HPC Schedulers
","*Nextflow* supports common HPC schedulers, abstracting the submission of jobs from the user. 

Currently the following clusters are supported:

  + [SGE](https://www.nextflow.io/docs/latest/executor.html#sge)
  + [Univa Grid Engine](https://www.nextflow.io/docs/latest/executor.html#sge)
  + [LSF](https://www.nextflow.io/docs/latest/executor.html#lsf)
  + [SLURM](https://www.nextflow.io/docs/latest/executor.html#slurm)
  + [PBS/Torque](https://www.nextflow.io/docs/latest/executor.html#pbs-torque)
  + [HTCondor (experimental)](https://www.nextflow.io/docs/latest/executor.html#htcondor)

For example to submit the execution to a SGE cluster create a file named `nextflow.config`, in the directory
where the pipeline is going to be launched, with the following content:

```nextflow
process {
  executor='sge'
  queue='<your execution queue>'
}
```

In doing that, processes will be executed by Nextflow as SGE jobs using the `qsub` command. Your 
pipeline will behave like any other SGE job script, with the benefit that *Nextflow* will 
automatically and transparently manage the processes synchronisation, file(s) staging/un-staging, etc.  


"
7,nextflow-io-nextflow-README.md,Cloud support,"*Nextflow* also supports running workflows across various clouds and cloud technologies. *Nextflow* can create AWS EC2 or Google GCE clusters and deploy your workflow. Managed solutions from both Amazon and Google are also supported through AWS Batch and Google Genomics Pipelines. Additionally, *Nextflow* can run workflows on either on-prem or managed cloud Kubernetes clusters. 

Currently supported cloud platforms:
  + [AWS Batch](https://www.nextflow.io/docs/latest/awscloud.html#aws-batch)
  + [AWS EC2](https://www.nextflow.io/docs/latest/awscloud.html)
  + [Google GCE](https://www.nextflow.io/docs/latest/google.html)
  + [Google Genomics Pipelines](https://www.nextflow.io/docs/latest/google.html#google-pipelines)
  + [Kubernetes](https://www.nextflow.io/docs/latest/kubernetes.html)



"
9,nextflow-io-nextflow-README.md,"Containers
","*Nextflow* has first class support for containerization. It supports both [Docker](https://www.nextflow.io/docs/latest/docker.html) and [Singularity](https://www.nextflow.io/docs/latest/singularity.html) container engines. Additionally, *Nextflow* can easily switch between container engines enabling workflow portability. 

```nextflow
process samtools {
  container 'biocontainers/samtools:1.3.1'

  """"""
  samtools --version 
  """"""

}
```

"
10,nextflow-io-nextflow-README.md,"Conda environments
","[Conda environments](https://www.nextflow.io/docs/latest/conda.html) provide another option for managing software packages in your workflow. 


"
11,nextflow-io-nextflow-README.md,"Environment Modules
","[Environment modules](https://www.nextflow.io/docs/latest/process.html#module) commonly found in HPC environments can also be used to manage the tools used in a *Nextflow* workflow. 


"
12,nextflow-io-nextflow-README.md,"Community
","You can post questions, or report problems by using the Nextflow [discussion forum](https://groups.google.com/forum/#!forum/nextflow)
or the [Nextflow channel on Gitter](https://gitter.im/nextflow-io/nextflow).

*Nextflow* also hosts a yearly workshop showcasing researcher's workflows and advancements in the langauge. Talks from the past workshops are available on the [Nextflow YouTube Channel](https://www.youtube.com/channel/UCB-5LCKLdTKVn2F4V4KlPbQ)

The [nf-core](https://nf-co.re/) project is a community effort aggregating high quality *Nextflow* workflows which can be used by the community. 


"
14,nextflow-io-nextflow-README.md,"Required dependencies
","* Compiler Java 8
* Runtime Java 8 or later

"
15,nextflow-io-nextflow-README.md,"Build from source
","*Nextflow* is written in [Groovy](http://groovy-lang.org) (a scripting language for the JVM). A pre-compiled,
ready-to-run, package is available at the [Github releases page](https://github.com/nextflow-io/nextflow/releases),
thus it is not necessary to compile it in order to use it.

If you are interested in modifying the source code, or contributing to the project, it worth knowing that
the build process is based on the [Gradle](http://www.gradle.org/) build automation system.

You can compile *Nextflow* by typing the following command in the project home directory on your computer:

```bash
make compile
```

The very first time you run it, it will automatically download all the libraries required by the build process.
It may take some minutes to complete.

When complete, execute the program by using the `launch.sh` script in the project directory.

The self-contained runnable Nextflow packages can be created by using the following command:

```bash
make pack
```

In order to install the compiled packages use the following command:

```bash
make install
```

Then you will be able to run nextflow using the `nextflow` launcher script in the project root folder.

"
16,nextflow-io-nextflow-README.md,"Known compilation problems
","Nextflow required JDK 8 to be compiled. The Java compiler used by the build process can be choose by setting the
`JAVA_HOME` environment variable accordingly.


If the compilation stops reporting the error: `java.lang.VerifyError: Bad <init> method call from inside of a branch`,
this is due to a bug affecting the following Java JDK:

- 1.8.0 update 11
- 1.8.0 update 20

Upgrade to a newer JDK to avoid to this issue. Alternatively a possible workaround is to define the following variable
in your environment:

```bash
_JAVA_OPTIONS='-Xverify:none'
```

Read more at these links:

- https://bugs.openjdk.java.net/browse/JDK-8051012
- https://jira.codehaus.org/browse/GROOVY-6951


"
17,nextflow-io-nextflow-README.md,"IntelliJ IDEA
","Nextflow development with [IntelliJ IDEA](https://www.jetbrains.com/idea/) requires the latest version of the IDE (2019.1.2 or later).

If you have it installed in your computer, follow the steps below in order to use it with Nextflow:

1. Clone the Nextflow repository to a directory in your computer.
2. Open IntelliJ IDEA and choose ""Import project"" in the ""File"" menu bar.
3. Select the Nextflow project root directory in your computer and click ""OK"".
4. Then, choose the ""Gradle"" item in the ""external module"" list and click on ""Next"" button.
5. Confirm the default import options and click on ""Finish"" to finalize the project configuration.
6. When the import process complete, select the ""Project structure"" command in the ""File"" menu bar.
7. In the showed dialog click on the ""Project"" item in the list of the left, and make sure that
   the ""Project SDK"" choice on the right contains Java 8.
8. Set the code formatting options with setting provided [here](https://github.com/nextflow-io/nextflow/blob/master/CONTRIBUTING.md#ide-settings).



"
18,nextflow-io-nextflow-README.md,"Contributing
","Project contribution are more than welcome. See the [CONTRIBUTING](CONTRIBUTING.md) file for details.


"
19,nextflow-io-nextflow-README.md,"Build servers
","  * [Travis-CI](https://travis-ci.org/nextflow-io/nextflow)
  * [Groovy Joint build](http://ci.groovy-lang.org/project.html?projectId=JointBuilds_Nextflow&guest=1)

"
20,nextflow-io-nextflow-README.md,"License
","The *Nextflow* framework is released under the Apache 2.0 license.

"
21,nextflow-io-nextflow-README.md,"Citations
","If you use Nextflow in your research, please cite:

P. Di Tommaso, et al. Nextflow enables reproducible computational workflows. Nature Biotechnology 35, 316–319 (2017) doi:[10.1038/nbt.3820](http://www.nature.com/nbt/journal/v35/n4/full/nbt.3820.html)

"
22,nextflow-io-nextflow-README.md,"Credits
","Nextflow is built on two great pieces of open source software, namely <a href='http://groovy-lang.org' target='_blank'>Groovy</a>
and <a href='http://www.gpars.org/' target='_blank'>Gpars</a>.

YourKit is kindly supporting this open source project with its full-featured Java Profiler.
Read more http://www.yourkit.com
"
0,scikit-image-scikit-image-README.md, scikit-image: Image processing in Python,"[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fscikit-image.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/scikit-image)
[![Stackoverflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](https://stackoverflow.com/questions/tagged/scikit-image)
[![project chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://skimage.zulipchat.com)
[![codecov.io](https://codecov.io/github/scikit-image/scikit-image/coverage.svg?branch=master)](https://codecov.io/github/scikit-image/scikit-image?branch=master)

- **Website (including documentation):** [https://scikit-image.org/](https://scikit-image.org)
- **Mailing list:** [https://mail.python.org/mailman/listinfo/scikit-image](https://mail.python.org/mailman/listinfo/scikit-image)
- **Source:** [https://github.com/scikit-image/scikit-image](https://github.com/scikit-image/scikit-image)
- **Benchmarks:** [https://pandas.pydata.org/speed/scikit-image/](https://pandas.pydata.org/speed/scikit-image/)

"
1,scikit-image-scikit-image-README.md, Installation from binaries,"- **Debian/Ubuntu:** ``sudo apt-get install python-skimage``
- **OSX:** ``pip install scikit-image``
- **Anaconda:** ``conda install -c conda-forge scikit-image``
- **Windows:** Download [Windows binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-image)

Also see [installing ``scikit-image``](INSTALL.rst).

"
2,scikit-image-scikit-image-README.md, Installation from source,"Install dependencies using:

```
pip install -r requirements.txt
```

Then, install scikit-image using:

```
$ pip install .
```

If you plan to develop the package, you may run it directly from source:

```
$ pip install -e .  #: Do this once to add package to Python path
```

Every time you modify Cython files, also run:

```
$ python setup.py build_ext -i  #: Build binary extensions
```

"
3,scikit-image-scikit-image-README.md, License (Modified BSD),"Copyright (C) 2011, the scikit-image team
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

 1. Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer.
 2. Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in
    the documentation and/or other materials provided with the
    distribution.
 3. Neither the name of skimage nor the names of its contributors may be
    used to endorse or promote products derived from this software without
    specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.

"
4,scikit-image-scikit-image-README.md, Citation,"If you find this project useful, please cite:

> Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias,
> François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle
> Gouillart, Tony Yu, and the scikit-image contributors.
> *scikit-image: Image processing in Python*. PeerJ 2:e453 (2014)
> https://doi.org/10.7717/peerj.453
"
3,CU-Net-README.md, Overview,"The follwoing figure gives an illustration of naive dense U-Net, stacked U-Nets and coupled U-Nets (CU-Net). The naive dense U-Net and stacked U-Nets have shortcut connections only inside each U-Net. In contrast, the coupled U-Nets also have connections for semantic blocks across U-Nets. The CU-Net is a hybrid of naive dense U-Net and stacked U-Net, integrating the merits of both dense connectivity, intermediate supervisions and multi-stage top-down and bottom-up refinement. The resulted CU-Net could save ~70% parameters of the previous stacked U-Nets but with comparable accuracy.
<p align=""center""><img src=""figures/framework-comparison.jpg"" alt="""" width=""600""></p>

If we couple each U-Net pair in multiple U-Nets, the coupling connections would have quadratic growth with respect to the U-Net number. To make the model more parameter efficient, we propose the order-K coupling to trim off the long-distance coupling connections.
<p align=""center""><img src=""figures/order-k.jpg"" alt="""" width=""400""></p>
For simplicity, each dot represents one U-Net. The red and blue lines are the shortcut connections of inside semantic blocks and outside inputs. Order-0 connectivity (Top) strings U-Nets together only by their inputs and outputs, i.e. stacked U-Nets. Order-1 connectivity (Middle) has shortcut connections for adjacent U-Nets. Similarly, order-2 connectivity (Bottom) has shortcut connections for 3 nearby U-Nets.

"
4,CU-Net-README.md, Prerequisites,"This package has the following requirements:

* `Python 2.7`
* `Pytorch v0.4.0` or `Pytorch v0.1.12`

Note that the script name with string `prev-version` requires `Pytorch v0.1.12`.

"
5,CU-Net-README.md, Training,"```
python cu-net.py --gpu_id 0 --exp_id cu-net-2 --layer_num 2 --order 1 --loss_num 2 --is_train true --bs 24
```

"
6,CU-Net-README.md, Validation,"```
python cu-net.py --gpu_id 0 --exp_id cu-net-2 --layer_num 2 --order 1 --loss_num 2 --resume_prefix your_pretrained_model.pth.tar --is_train false --bs 24
```

"
7,CU-Net-README.md, Model Options,"```
layer_num     #: number of coupled U-Nets
order         #: the order of coupling
loss_num      #: number of losses. Losses are uniformly distributed along the CU-Net. Each U-Net at most has one loss. (loss_num <= layer_num)
```

"
8,CU-Net-README.md, Project Page,"For more details, please refer to our **[project page](https://sites.google.com/site/xipengcshomepage/research/eccv18)**.

"
9,CU-Net-README.md, Citation,"If you find this code useful in your research, please consider citing:

```
@inproceedings{tang2018quantized,
  title={Quantized densely connected U-Nets for efficient landmark localization},
  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Wu, Lingfei and Zhang, Shaoting and Metaxas, Dimitris},
  booktitle={ECCV},
  year={2018}
}
@inproceedings{tang2018cu,
  title={CU-Net: Coupled U-Nets},
  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Zhu, Yizhe and Metaxas, Dimitris},
  booktitle={BMVC},
  year={2018}
}
```

"
0,pyGeoPressure-README.md, pyGeoPressure -->,"<img src=""docs/img/pygeopressure-logo.png"" alt=""Logo"" height=""240"">

[![PyPI version](https://badge.fury.io/py/pyGeoPressure.svg)](https://badge.fury.io/py/pyGeoPressure)
[![GitHub release](https://img.shields.io/github/tag/whimian/pyGeoPressure.svg?label=Release)](https://github.com/whimian/pyGeoPressure/releases)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/whimian/pyGeoPressure/blob/master/LICENSE)
[![Documentation Status](https://readthedocs.org/projects/pygeopressure/badge/?version=latest)](http://pygeopressure.readthedocs.io/en/latest/?badge=latest)
[![Build Status](https://travis-ci.org/whimian/pyGeoPressure.svg?branch=master)](https://travis-ci.org/whimian/pyGeoPressure)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/2f79d873803d4ef1a3c306603fcfd767)](https://www.codacy.com/app/whimian/pyGeoPressure?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=whimian/pyGeoPressure&amp;utm_campaign=Badge_Grade)
[![codecov](https://codecov.io/gh/whimian/pyGeoPressure/branch/master/graph/badge.svg)](https://codecov.io/gh/whimian/pyGeoPressure)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1452001.svg)](https://doi.org/10.5281/zenodo.1452001)


A Python package for pore pressure prediction using well log data and seismic velocity data.


[![DOI](http://joss.theoj.org/papers/10.21105/joss.00992/status.svg)](https://doi.org/10.21105/joss.00992)

Cite pyGeoPressure as:
> Yu, (2018). PyGeoPressure: Geopressure Prediction in Python. Journal of Open Source Software, 3(30), 992, https://doi.org/10.21105/joss.00992

BibTex:

```bibtex
@article{yu2018pygeopressure,
  title = {{PyGeoPressure}: {Geopressure} {Prediction} in {Python}},
  author = {Yu, Hao},
  journal = {Journal of Open Source Software},
  volume = {3},
  pages = {922}
  number = {30},
  year = {2018},
  doi = {10.21105/joss.00992},
}
```

"
1,pyGeoPressure-README.md, Features,"1. Overburden (or Lithostatic) Pressure Calculation
2. Eaton's method and Parameter Optimization
3. Bowers' method and Parameter Optimization
4. Multivariate method and Parameter Optimization

"
3,pyGeoPressure-README.md, Installation,"`pyGeoPressure` is on `PyPI`:

```bash
pip install pygeopressure
```

"
5,pyGeoPressure-README.md, Pore Pressure Prediction using well log data,"```python
import pygeopressure as ppp

survey = ppp.Survey(""CUG"")

well = survey.wells['CUG1']

a, b = ppp.optimize_nct(well.get_log(""Velocity""),
                        well.params['horizon'][""T16""],
                        well.params['horizon'][""T20""])
n = ppp.optimize_eaton(well, ""Velocity"", ""Overburden_Pressure"", a, b)

pres_eaton_log = well.eaton(np.array(well.get_log(""Velocity"").data), n)

fig, ax = plt.subplots()
ax.invert_yaxis()

pres_eaton_log.plot(ax, color='blue')
well.get_log(""Overburden_Pressure"").plot(ax, 'g')
ax.plot(well.hydrostatic, well.depth, 'g', linestyle='--')
well.plot_horizons(ax)
```

<img src=""docs/img/readme_example.svg"" alt=""Logo"" height=""600"">

"
6,pyGeoPressure-README.md, Documentation,"Read the documentaion for detailed explanations, tutorials and references:
https://pygeopressure.readthedocs.io/en/latest/

"
8,pyGeoPressure-README.md, Report Bugs,"If you find a bug, please report it at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) by opening a new issue with `bug` label.

"
9,pyGeoPressure-README.md, Suggest Enhancements,"If you have new ideas or need new features, you can request them by opening a new issue at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) with `enhancement` label. We will see if we can work on it together.

"
10,pyGeoPressure-README.md, Submit Pull Requests,"If you would like to help fix known bugs, please submit a PR.
(See [The beginner's guide to contributing to a GitHub project](https://akrabat.com/the-beginners-guide-to-contributing-to-a-github-project/), if you are new to Github).

Before creating a pull request, please try to make sure the tests pass and use numpy-style docstrings. (Please see the documentation on setting up the development environment https://pygeopressure.readthedocs.io/en/latest/install.html)

"
11,pyGeoPressure-README.md, Support,"If you have any questions, please open an issue at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) with `question` label. Tell us about your question, we will provide assistance. And maybe we could add it to the documentation.

"
12,pyGeoPressure-README.md, License,"The project is licensed under the MIT license, see the file [LICENSE](<https://github.com/whimian/pyGeoPressure/blob/master/LICENSE>) for details.
"
0,Fiona-README.md," Open a file for reading. We'll call this the ""source.""","    with fiona.open('tests/data/coutwildrnp.shp') as src:

        "
1,Fiona-README.md," The file we'll write to, the ""destination"", must be initialized",        
2,Fiona-README.md," with a coordinate system, a format driver name, and",        
3,Fiona-README.md, a record schema.  We can get initial values from the open,        
4,Fiona-README.md, collection's ``meta`` property and then modify them as,        
5,Fiona-README.md, desired.,"        meta = src.meta
        meta['schema']['geometry'] = 'Point'

        "
6,Fiona-README.md," Open an output file, using the same format driver and",        
7,Fiona-README.md, coordinate reference system as the source. The ``meta``,        
8,Fiona-README.md, mapping fills in the keyword parameters of fiona.open().,"        with fiona.open('test_write.shp', 'w', **meta) as dst:

            "
9,Fiona-README.md, Process only the records intersecting a box.,"            for f in src.filter(bbox=(-107.0, 37.0, -105.0, 39.0)):

                "
10,Fiona-README.md, Get a point on the boundary of the record's,                
11,Fiona-README.md, geometry.,"                f['geometry'] = {
                    'type': 'Point',
                    'coordinates': f['geometry']['coordinates'][0][0]}

                "
12,Fiona-README.md, Write the record out.,"                dst.write(f)

    "
13,Fiona-README.md, The destination's contents are flushed to disk and the file is,    
14,Fiona-README.md, closed when its ``with`` block ends. This effectively,    
15,Fiona-README.md, executes ``dst.flush(); dst.close()``.,"Reading Multilayer data
-----------------------

Collections can also be made from single layers within multilayer files or
directories of data. The target layer is specified by name or by its integer
index within the file or directory. The ``fiona.listlayers()`` function
provides an index ordered list of layer names.

.. code-block:: python

    for layername in fiona.listlayers('tests/data'):
        with fiona.open('tests/data', layer=layername) as src:
            print(layername, len(src))

    "
16,Fiona-README.md, Output:,    
17,Fiona-README.md," (u'coutwildrnp', 67)","Layer can also be specified by index. In this case, ``layer=0`` and
``layer='test_uk'`` specify the same layer in the data file or directory.

.. code-block:: python

    for i, layername in enumerate(fiona.listlayers('tests/data')):
        with fiona.open('tests/data', layer=i) as src:
            print(i, layername, len(src))

    "
18,Fiona-README.md, Output:,    
19,Fiona-README.md," (0, u'coutwildrnp', 67)","Writing Multilayer data
-----------------------

Multilayer data can be written as well. Layers must be specified by name when
writing.

.. code-block:: python

    with open('tests/data/cowildrnp.shp') as src:
        meta = src.meta
        f = next(src)

    with fiona.open('/tmp/foo', 'w', layer='bar', **meta) as dst:
        dst.write(f)

    print(fiona.listlayers('/tmp/foo'))

    with fiona.open('/tmp/foo', layer='bar') as src:
        print(len(src))
        f = next(src)
        print(f['geometry']['type'])
        print(f['properties'])

        "
20,Fiona-README.md, Output:,        
21,Fiona-README.md, [u'bar'],        
22,Fiona-README.md, 1,        
23,Fiona-README.md, Polygon,        
24,Fiona-README.md," OrderedDict([(u'PERIMETER', 1.22107), (u'FEATURE2', None), (u'NAME', u'Mount Naomi Wilderness'), (u'FEATURE1', u'Wilderness'), (u'URL', u'http://www.wilderness.net/index.cfm?fuse=NWPS&sec=wildView&wname=Mount%20Naomi'), (u'AGBUR', u'FS'), (u'AREA', 0.0179264), (u'STATE_FIPS', u'49'), (u'WILDRNP020', 332), (u'STATE', u'UT')])","A view of the /tmp/foo directory will confirm the creation of the new files.

.. code-block:: console

    $ ls /tmp/foo
    bar.cpg bar.dbf bar.prj bar.shp bar.shx

Collections from archives and virtual file systems
--------------------------------------------------

Zip and Tar archives can be treated as virtual filesystems and Collections can
be made from paths and layers within them. In other words, Fiona lets you read
and write zipped Shapefiles.

.. code-block:: python

    for i, layername in enumerate(
            fiona.listlayers('zip://tests/data/coutwildrnp.zip'):
        with fiona.open('zip://tests/data/coutwildrnp.zip', layer=i) as src:
            print(i, layername, len(src))

    "
25,Fiona-README.md, Output:,    
26,Fiona-README.md," (0, u'coutwildrnp', 67)","Fiona can also read from more exotic file systems. For instance, a
zipped shape file in S3 can be accessed like so:

.. code-block:: python

   with fiona.open('zip+s3://mapbox/rasterio/coutwildrnp.zip') as src:
       print(len(src))

   "
27,Fiona-README.md, Output:,   
28,Fiona-README.md, 67,"Fiona CLI
=========

Fiona's command line interface, named ""fio"", is documented at `docs/cli.rst
<https://github.com/Toblerity/Fiona/blob/master/docs/cli.rst>`__. Its ``fio
info`` pretty prints information about a data file.

.. code-block:: console

    $ fio info --indent 2 tests/data/coutwildrnp.shp
    {
      ""count"": 67,
      ""crs"": ""EPSG:4326"",
      ""driver"": ""ESRI Shapefile"",
      ""bounds"": [
        -113.56424713134766,
        37.0689811706543,
        -104.97087097167969,
        41.99627685546875
      ],
      ""schema"": {
        ""geometry"": ""Polygon"",
        ""properties"": {
          ""PERIMETER"": ""float:24.15"",
          ""FEATURE2"": ""str:80"",
          ""NAME"": ""str:80"",
          ""FEATURE1"": ""str:80"",
          ""URL"": ""str:101"",
          ""AGBUR"": ""str:80"",
          ""AREA"": ""float:24.15"",
          ""STATE_FIPS"": ""str:80"",
          ""WILDRNP020"": ""int:10"",
          ""STATE"": ""str:80""
        }
      }
    }

Installation
============

Fiona requires Python versions 2.7 or 3.4+ and GDAL version 1.11-2.4. GDAL version 3 is not yet supported.
To build from a source distribution you will need a C compiler and GDAL and Python
development headers and libraries (libgdal1-dev for Debian/Ubuntu, gdal-dev for
CentOS/Fedora).

To build from a repository copy, you will also need Cython to build C sources
from the project's .pyx files. See the project's requirements-dev.txt file for
guidance.

The `Kyngchaos GDAL frameworks
<http://www.kyngchaos.com/software/frameworks#gdal_complete>`__ will satisfy
the GDAL/OGR dependency for OS X, as will Homebrew's GDAL Formula (``brew install
gdal``).

Python Requirements
-------------------

Fiona depends on the modules ``enum34``, ``six``, ``cligj``,  ``munch``, ``argparse``, and
``ordereddict`` (the two latter modules are standard in Python 2.7+). Pip will
fetch these requirements for you, but users installing Fiona from a Windows
installer must get them separately.

Unix-like systems
-----------------

Assuming you're using a virtualenv (if not, skip to the 4th command) and
GDAL/OGR libraries, headers, and `gdal-config`_ program are installed to well
known locations on your system via your system's package manager (``brew
install gdal`` using Homebrew on OS X), installation is this simple.

.. code-block:: console

  $ mkdir fiona_env
  $ virtualenv fiona_env
  $ source fiona_env/bin/activate
  (fiona_env)$ pip install fiona

If gdal-config is not available or if GDAL/OGR headers and libs aren't
installed to a well known location, you must set include dirs, library dirs,
and libraries options via the setup.cfg file or setup command line as shown
below (using ``git``). You must also specify the version of the GDAL API on the
command line using the ``--gdalversion`` argument (see example below) or with
the ``GDAL_VERSION`` environment variable (e.g. ``export GDAL_VERSION=2.1``).

.. code-block:: console

  (fiona_env)$ git clone git://github.com/Toblerity/Fiona.git
  (fiona_env)$ cd Fiona
  (fiona_env)$ python setup.py build_ext -I/path/to/gdal/include -L/path/to/gdal/lib -lgdal install --gdalversion 2.1

Or specify that build options and GDAL API version should be provided by a
particular gdal-config program.

.. code-block:: console

  (fiona_env)$ GDAL_CONFIG=/path/to/gdal-config pip install fiona

Windows
-------

Binary installers are available at
http://www.lfd.uci.edu/~gohlke/pythonlibs/#fiona and coming eventually to PyPI.

You can download a binary distribution of GDAL from `here
<http://www.gisinternals.com/release.php>`_.  You will also need to download
the compiled libraries and headers (include files).

When building from source on Windows, it is important to know that setup.py
cannot rely on gdal-config, which is only present on UNIX systems, to discover
the locations of header files and libraries that Fiona needs to compile its
C extensions. On Windows, these paths need to be provided by the user.
You will need to find the include files and the library files for gdal and
use setup.py as follows. You must also specify the version of the GDAL API on the
command line using the ``--gdalversion`` argument (see example below) or with
the ``GDAL_VERSION`` environment variable (e.g. ``set GDAL_VERSION=2.1``).

.. code-block:: console

    $ python setup.py build_ext -I<path to gdal include files> -lgdal_i -L<path to gdal library> install --gdalversion 2.1

Note: The GDAL DLL (``gdal111.dll`` or similar) and gdal-data directory need to
be in your Windows PATH otherwise Fiona will fail to work.

The `Appveyor CI build <https://ci.appveyor.com/project/sgillies/fiona/history>`__
uses the GISInternals GDAL binaries to build Fiona. This produces a binary wheel
for successful builds, which includes GDAL and other dependencies, for users
wanting to try an unstable development version.
The `Appveyor configuration file <https://github.com/Toblerity/Fiona/blob/master/appveyor.yml>`__ may be a useful example for
users building from source on Windows.

Development and testing
=======================

Building from the source requires Cython. Tests require `pytest <http://pytest.org>`_. If the GDAL/OGR
libraries, headers, and `gdal-config`_ program are installed to well known
locations on your system (via your system's package manager), you can do this::

  (fiona_env)$ git clone git://github.com/Toblerity/Fiona.git
  (fiona_env)$ cd Fiona
  (fiona_env)$ pip install cython
  (fiona_env)$ pip install -e .[test]
  (fiona_env)$ py.test

Or you can use the ``pep-518-install`` script::

  (fiona_env)$ git clone git://github.com/Toblerity/Fiona.git
  (fiona_env)$ cd Fiona
  (fiona_env)$ ./pep-518-install

If you have a non-standard environment, you'll need to specify the include and
lib dirs and GDAL library on the command line::

  (fiona_env)$ python setup.py build_ext -I/path/to/gdal/include -L/path/to/gdal/lib -lgdal --gdalversion 2 develop
  (fiona_env)$ py.test

.. _OGR: http://www.gdal.org/ogr
.. _pyproj: http://pypi.python.org/pypi/pyproj/
.. _Rtree: http://pypi.python.org/pypi/Rtree/
.. _Shapely: http://pypi.python.org/pypi/Shapely/
.. _gdal-config: http://www.gdal.org/gdal-config.html
"
0,sg2im-README.md, sg2im,"This is the code for the paper

**<a href=""https://arxiv.org/abs/1804.01622"">Image Generation from Scene Graphs</a>**
<br>
<a href=""http://cs.stanford.edu/people/jcjohns/"">Justin Johnson</a>,
<a href=""http://web.stanford.edu/~agrim/"">Agrim Gupta</a>,
<a href=""http://vision.stanford.edu/feifeili/"">Li Fei-Fei</a>
<br>
Presented at [CVPR 2018](http://cvpr2018.thecvf.com/)

Please note that this is not an officially supported Google product.

A **scene graph** is a structured representation of a visual scene where nodes represent *objects* in the scene and edges represent *relationships* between objects. In this paper we present and end-to-end neural network model that inputs a scene graph and outputs an image.

Below we show some example scene graphs along with images generated from those scene graphs using our model. By modifying the input scene graph we can exercise fine-grained control over the objects in the generated image.

<div align='center'>
  <img src='images/sheep/sg000000.png' width='112px'>
  <img src='images/sheep/sg000001.png' width='112px'>
  <img src='images/sheep/sg000002.png' width='112px'>
  <img src='images/sheep/sg000003.png' width='112px'>
  <img src='images/sheep/sg000004.png' width='112px'>
  <img src='images/sheep/sg000005.png' width='112px'>
  <img src='images/sheep/sg000006.png' width='112px'>
  <br>
  <img src='images/sheep/img000000.png' height='112px'>
  <img src='images/sheep/img000001.png' height='112px'>
  <img src='images/sheep/img000002.png' height='112px'>
  <img src='images/sheep/img000003.png' height='112px'>
  <img src='images/sheep/img000004.png' height='112px'>
  <img src='images/sheep/img000005.png' height='112px'>
  <img src='images/sheep/img000006.png' height='112px'>
</div>

If you find this code useful in your research then please cite
```
@inproceedings{johnson2018image,
  title={Image Generation from Scene Graphs},
  author={Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
  booktitle={CVPR},
  year={2018}
}
```

"
1,sg2im-README.md, Model,"The input scene graph is processed with a *graph convolution network* which passes information along edges to compute embedding vectors for all objects. These vectors are used to predict bounding boxes and segmentation masks for all objects, which are combined to form a coarse *scene layout*. The layout is passed to a *cascaded refinement network* (Chen an Koltun, ICCV 2017) which generates an output image at increasing spatial scales. The model is trained adversarially against a pair of *discriminator networks* which ensure that output images look realistic.

<div align='center'>
  <img src='images/system.png' width='1000px'>
</div>

"
2,sg2im-README.md, Setup,"All code was developed and tested on Ubuntu 16.04 with Python 3.5 and PyTorch 0.4.

You can setup a virtual environment to run the code like this:

```bash
python3 -m venv env               #: Create a virtual environment
source env/bin/activate           #: Activate virtual environment
pip install -r requirements.txt   #: Install dependencies
echo $PWD > env/lib/python3.5/site-packages/sg2im.pth  #: Add current directory to python path
#: Work for a while ...
deactivate  #: Exit virtual environment
```

"
3,sg2im-README.md, Pretrained Models,"You can download pretrained models by running the script `bash scripts/download_models.sh`. This will download the following models, and will require about 355 MB of disk space:

- `sg2im-models/coco64.pt`: Trained to generate 64 x 64 images on the COCO-Stuff dataset. This model was used to generate the COCO images in Figure 5 from the paper.
- `sg2im-models/vg64.pt`: Trained to generate 64 x 64 images on the Visual Genome dataset. This model was used to generate the Visual Genome images in Figure 5 from the paper.
- `sg2im-models/vg128.pt`: Trained to generate 128 x 128 images on the Visual Genome dataset. This model was used to generate the images in Figure 6 from the paper.

Table 1 in the paper presents an ablation study where we disable various components of the full model. You can download the additional models used in this ablation study by running the script `bash scripts/download_ablated_models.sh`. This will download 12 additional models, requiring and additional 1.25 GB of disk space.

"
4,sg2im-README.md, Running Models,"You can use the script `scripts/run_model.py` to easily run any of the pretrained models on new scene graphs using a simple human-readable JSON format. For example you can replicate the sheep images above like this:

```bash
python scripts/run_model.py \
  --checkpoint sg2im-models/vg128.pt \
  --scene_graphs scene_graphs/figure_6_sheep.json \
  --output_dir outputs
```

The generated images will be saved to the directory specified by the `--output_dir` flag. You can control whether the model runs on CPU or GPU using py passing the flag `--device cpu` or `--device gpu`.

We provide JSON files and pretrained models allowing you to recreate all images from Figures 5 and 6 from the paper.

"
5,sg2im-README.md, (Optional): GraphViz,"This script can also draw images for the scene graphs themselves using [GraphViz](http://www.graphviz.org/); to enable this option just add the flag `--draw_scene_graphs 1` and the scene graph images will also be saved in the output directory. For this option to work you must install GraphViz; on Ubuntu 16.04 you can simply run `sudo apt-get install graphviz`.

"
6,sg2im-README.md, Training new models,"Instructions for training new models can be [found here](TRAINING.md).
"
0,pyvista-README.md,"Highlights
",".. |binder| image:: https://mybinder.org/badge_logo.svg
   :target: https://mybinder.org/v2/gh/pyvista/pyvista-examples/master
   :alt: Launch on Binder

Head over to the `Quick Examples`_ page in the docs to explore our gallery of
examples showcasing what PyVista can do! Want to test-drive PyVista?
All of the examples from the gallery are live on MyBinder for you to test
drive without installing anything locally: |binder|

.. _Quick Examples: http://docs.pyvista.org/examples/index.html


"
1,pyvista-README.md,"Overview of Features
","* Embeddable rendering in Jupyter Notebooks
* Filtering/plotting tools built for interactivity in Jupyter notebooks (see `IPython Tools`_)
* Direct access to mesh analysis and transformation routines (see Filters_)
* Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)
* Import meshes from many common formats (use ``pyvista.read()``)
* Export meshes as VTK, STL, OBJ, or PLY file types


.. _IPython Tools: http://docs.pyvista.org/tools/ipy_tools.html
.. _Filters: http://docs.pyvista.org/tools/filters.html
.. _Plotting: http://docs.pyvista.org/tools/plotting.html


"
2,pyvista-README.md,"Documentation
","Refer to the `documentation <http://docs.pyvista.org/>`_ for detailed
installation and usage details.

For general questions about the project, its applications, or about software
usage, please create an issue in the `pyvista/pyvista-support`_ repository
where the community can collectively address your questions. You are also
welcome to join us on join us on Slack_ or send one of the developers an email.
The project support team can be reached at `info@pyvista.org`_.

.. _pyvista/pyvista-support: https://github.com/pyvista/pyvista-support
.. _Slack: http://slack.pyvista.org
.. _info@pyvista.org: mailto:info@pyvista.org


"
3,pyvista-README.md,"Installation
","PyVista can be installed from `PyPI <http://pypi.python.org/pypi/pyvista>`_
using ``pip`` on Python >= 3.5::

    pip install pyvista

You can also visit `PyPi <http://pypi.python.org/pypi/pyvista>`_,
`Anaconda <https://anaconda.org/conda-forge/pyvista>`_, or
`GitHub <https://github.com/pyvista/pyvista>`_ to download the source.

See the `Installation <http://docs.pyvista.org/getting-started/installation.html#install-ref.>`_
for more details if the installation through pip doesn't work out.

"
4,pyvista-README.md,"Connections
","PyVista is a powerful tool that researchers can harness to create compelling,
integrated visualizations of large datasets in an intuitive, Pythonic manner.
Here are a few open-source projects that leverage PyVista:

* pyansys_: Pythonic interface to ANSYS result, full, and archive files
* PVGeo_: Python package of VTK-based algorithms to analyze geoscientific data and models. PyVista is used to make the inputs and outputs of PVGeo's algorithms more accessible.
* omfvista_: 3D visualization for the Open Mining Format (omf). PyVista provides the foundation for this library's visualization.
* discretize_: Discretization tools for finite volume and inverse problems. ``discretize`` provides ``toVTK`` methods that return PyVista versions of their data types for `creating compelling visualizations`_.
* pymeshfix_: Python/Cython wrapper of Marco Attene's wonderful, award-winning MeshFix software.
* tetgen_: Python Interface to Hang Si's C++ TetGen Library


.. _pyansys: https://github.com/akaszynski/pyansys
.. _PVGeo: https://github.com/OpenGeoVis/PVGeo
.. _omfvista: https://github.com/OpenGeoVis/omfvista
.. _discretize: http://discretize.simpeg.xyz/en/master/
.. _creating compelling visualizations: http://discretize.simpeg.xyz/en/master/content/mixins.html#module-discretize.mixins.vtkModule
.. _pymeshfix: https://github.com/pyvista/pymeshfix
.. _MeshFix: https://github.com/MarcoAttene/MeshFix-V2.1
.. _tetgen: https://github.com/pyvista/tetgen


"
5,pyvista-README.md,"Authors
","Please take a look at the `contributors page`_ and the active `list of authors`_
to learn more about the developers of PyVista.

.. _contributors page: https://github.com/pyvista/pyvista/graphs/contributors/
.. _list of authors: http://docs.pyvista.org/authors


"
6,pyvista-README.md,"Contributing
","We absolutely welcome contributions and we hope that our `Contributing Guide`_
will facilitate your ability to make PyVista better. PyVista is mostly
maintained on a volunteer basis and thus we need to foster a community that can
support user questions and develop new features to make this software a useful
tool for all users while encouraging every member of the commutinity to share
their ideas. To learn more about contributing to PyVista, please see the
`Contributing Guide`_ and our `Code of Conduct`_.

.. _Contributing Guide: https://github.com/pyvista/pyvista/blob/master/CONTRIBUTING.md
.. _Code of Conduct: https://github.com/pyvista/pyvista/blob/master/CODE_OF_CONDUCT.md


"
7,pyvista-README.md,"Citing PyVista
","There is a `paper about PyVista <https://doi.org/10.21105/joss.01450>`_!

If you are using PyVista in your scientific research, please help our scientific
visibility by citing our work!


    Sullivan et al., (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450


BibTex:

.. code::

    @article{sullivan2019pyvista,
      doi = {10.21105/joss.01450},
      url = {https://doi.org/10.21105/joss.01450},
      year = {2019},
      month = {may},
      publisher = {The Open Journal},
      volume = {4},
      number = {37},
      pages = {1450},
      author = {C. Bane Sullivan and Alexander Kaszynski},
      title = {{PyVista}: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit ({VTK})},
      journal = {Journal of Open Source Software}
    }
"
0,ICNet-README.md, ICNet for Real-Time Semantic Segmentation on High-Resolution Images,"by Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia, details are in [project page](https://hszhao.github.io/projects/icnet).

"
1,ICNet-README.md, Introduction,"Based on [PSPNet](https://github.com/hszhao/PSPNet), this repository is build for evaluation in ICNet. For installation, please follow the description in PSPNet repository (support CUDA 7.0/7.5 + cuDNN v4).

"
2,ICNet-README.md, Usage,"1. Clone the repository recursively:

   ```shell
   git clone --recursive https://github.com/hszhao/ICNet.git
   ```

2. Build Caffe and matcaffe:

   ```shell
   cd $ICNET_ROOT/PSPNet
   cp Makefile.config.example Makefile.config
   vim Makefile.config
   make -j8 && make matcaffe
   cd ..
   ```

3. Evaluation mIoU:

   - Evaluation code is in folder 'evaluation'.
   - Download trained models and put them in folder 'evaluation/model':
     - icnet_cityscapes_train_30k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCRXpXMnVIbXdfaW8) 

       (31M, md5: c7038630c4b6c869afaaadd811bdb539; train on trainset for 30k)

     - icnet_cityscapes_trainval_90k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCTFVpZWJINi1Iblk) 

       (31M, md5: 4f4dd9eecd465dd8de7e4cf88ba5d5d5; train on trainvalset for 90k)
   - Modify the related paths in 'eval_all.m':
     - Mainly variables 'data_root' and 'eval_list', and your image list for evaluation should be similar to that in folder 'evaluation/samplelist' if you use this evaluation code structure. 

   ```shell
   cd evaluation
   vim eval_all.m
   ```

   - Run the evaluation scripts:

   ```
   ./run.sh
   ```

4. Evaluation time:

   - To get inference time as accurate as possible, it's suggested to make sure the GPU card with specified ID in script 'test_time.sh' is empty (without other processes executing)

   - Run the evaluation scripts:

   ```
   ./test_time.sh
   ```

5. Results: 

   - Prediction results will show in folder 'evaluation/mc_result' and the expected scores are:
     - ICNet train on trainset for 30K, evaluated on valset (mIoU/pAcc): 67.7/94.5
     - ICNet train on trainvalset for 90K, evaluated on testset (mIoU): 69.5
   - Log information of inference time will be in file 'time.log', approximately 33~36ms on TitanX.

6. Demo video:

   - Video processed by ICNet on cityscapes dataset:
     - Alpha blending with value as 0.5: [Video](https://youtu.be/qWl9idsCuLQ)

"
3,ICNet-README.md, Citation,"If ICNet is useful for your research, please consider citing:

    @article{zhao2017icnet,
      author = {Hengshuang Zhao and
                Xiaojuan Qi and
                Xiaoyong Shen and
                Jianping Shi and
                Jiaya Jia},
      title = {ICNet for Real-Time Semantic Segmentation on High-Resolution Images},
      journal={arXiv preprint arXiv:1704.08545},
      year = {2017}
    }
"
4,ICNet-README.md, Questions,"Please contact 'hszhao@cse.cuhk.edu.hk'
"
0,neural_renderer-README.md, Neural 3D Mesh Renderer (CVPR 2018),"This is code for the paper [Neural 3D Mesh Renderer](http://hiroharu-kato.com/projects_en/neural_renderer.html) by Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada.

![](http://hiroharu-kato.com/assets/img/neural_renderer/thumbnail_en.png)

For more details, please visit [project page](http://hiroharu-kato.com/projects_en/neural_renderer.html).

This repository only contains the core component and simple examples. Related repositories are:

* Neural Renderer (this repository)
    * [Single-image 3D mesh reconstruction](https://github.com/hiroharu-kato/mesh_reconstruction)
    * [2D-to-3D style transfer](https://github.com/hiroharu-kato/style_transfer_3d)
    * [3D DeepDream](https://github.com/hiroharu-kato/deep_dream_3d)

"
1,neural_renderer-README.md, For PyTorch users,"This code is written in Chainer. For PyTorch users, there are two options.

* [Angjoo Kanazawa & Shubham Tulsiani provides PyTorch wrapper of our renderer](https://github.com/akanazawa/cmr) used in their work ""Learning Category-Specific Mesh Reconstruction from Image Collections"" (ECCV 2018).
* [Nikos Kolotouros provides PyTorch re-implementation of our renderer](https://github.com/daniilidis-group/neural_renderer), which does not require installation of Chainer / CuPy.

I'm grateful to these researchers for writing and releasing their codes.

"
2,neural_renderer-README.md, Installation,"```
sudo python setup.py install
```

"
3,neural_renderer-README.md, Running examples,"```
python ./examples/example1.py
python ./examples/example2.py
python ./examples/example3.py
python ./examples/example4.py
```


"
4,neural_renderer-README.md, Example 1: Drawing an object from multiple viewpoints,"![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example1.gif)

"
5,neural_renderer-README.md, Example 2: Optimizing vertices,"Transforming the silhouette of a teapot into a rectangle. The loss function is the difference between the rendered image and the reference image.

Reference image, optimization, and the result.

![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_optimization.gif) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_result.gif)

"
6,neural_renderer-README.md, Example 3: Optimizing textures,"Matching the color of a teapot with a reference image.

Reference image, result.

![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example3_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example3_result.gif)

"
7,neural_renderer-README.md, Example 4: Finding camera parameters,"The derivative of images with respect to camera pose can be computed through this renderer. In this example the position of the camera is optimized by gradient descent.

From left to right: reference image, initial state, and optimization process.

![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_init.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_result.gif)

"
9,neural_renderer-README.md, CPU implementation?,"Currently, this code has no CPU implementation. Since CPU implementation would be probably too slow for practical usage, we do not plan to support CPU.

"
10,neural_renderer-README.md, Python3 support?,"Code in this repository is only for Python 2.x. [PyTorch port by Nikos Kolotourosr](https://github.com/daniilidis-group/neural_renderer), supports Python 3.x.

If you want to install neural renderer using Python 3, please add ./neural_renderer to $PYTHON_PATH temporarily as mentioned in [issue #6](https://github.com/hiroharu-kato/neural_renderer/issues/6). However, since we did not tested our code using Python 3, it might not work well.

"
11,neural_renderer-README.md, Citation,"```
@InProceedings{kato2018renderer
    title={Neural 3D Mesh Renderer},
    author={Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2018}
}
```
"
0,harismuneer-Ultimate-Facebook-Scraper-README.md, 🔥 Ultimate Facebook Scrapper ,"[![DOI](https://zenodo.org/badge/145763277.svg)](https://zenodo.org/badge/latestdoi/145763277)

[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen.svg?style=for-the-badge&logo=appveyor)](#)
[![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](#)
[![GitHub Forks](https://img.shields.io/github/forks/harismuneer/Ultimate-Facebook-Scraper.svg?style=social&label=Fork&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/fork)
[![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues)
[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&label=Contributions&colorA=red&colorB=black	)](#)


A bot which scrapes almost everything about a facebook user's profile including

* uploaded photos
* tagged photos
* videos
* friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc)
* and all public posts/statuses available on the user's timeline.

The best thing about this scraper is that the data is scraped in an organized format so that it can be used for educational/research purpose by researchers. Moreover, this scraper does not use Facebook's Graph API so there are no rate limiting issues as such. 

**This tool is being used by thousands of developers weekly and we are pretty amazed at this response! Thankyou guys!🎉**

For details regarding **citing/referencing** this tool for your research, check the 'Citation' section below.

"
1,harismuneer-Ultimate-Facebook-Scraper-README.md, Note,"At its core, this tool uses xpaths of **'divs'** to extract data from them. Since Facebook keeps on updating its site frequently and the 'divs' get changed. Consequently, we have to update the divs accordingly to correctly scrape the data. 

The developers of this tool have devoted a lot of time and effort in developing and most importantly maintaining this tool for quite a lot time now. **In order to keep this amazing tool alive, we need support from you geeks.**

The code is pretty intuitive and easy to understand, so you can update the relevant xpaths in the code when you feel that you have tried many profiles and the data isn't being scraped for any of them (that's a hint that Facebook has updated their site) and generate a pull request. That's quite an easy thing to do. Thanks!

"
2,harismuneer-Ultimate-Facebook-Scraper-README.md, Sample,"<p align=""middle"">
  <img src=""../master/images/main.png"" width=""700""/>
 </p>


"
3,harismuneer-Ultimate-Facebook-Scraper-README.md, Screenshot,"<p align=""middle"">
  <img src=""../master/images/screenshot.png"" width=""700""/>
 </p>


----------------------------------------------------------------------------------------------------------------------------------------
"
5,harismuneer-Ultimate-Facebook-Scraper-README.md, Installation,"You will need to install latest version of [Google Chrome](https://www.google.com/chrome/). Moreover, you need to install selenium module as well using

```
pip install selenium
```

Run the code using Python 3. Also, the code is multi-platform and is tested on both Windows and Linux.
The tool uses latest version of [Chrome Web Driver](http://chromedriver.chromium.org/downloads). I have placed the webdriver along with the code but if that version doesn't work then replace the chrome web driver with the latest one.

"
6,harismuneer-Ultimate-Facebook-Scraper-README.md, How to Run,"There's a file named ""input.txt"". You can add as many profiles as you want in the following format with each link on a new line:

```
https://www.facebook.com/andrew.ng.96
https://www.facebook.com/zuck
```

Make sure the link only contains the username or id number at the end and not any other stuff. Make sure its in the format mentioned above.

Note: There are two modes to download Friends Profile Pics and the user's Photos: Large Size and Small Size. You can change the following variables. By default they are set to Small Sized Pics because its really quick while Large Size Mode takes time depending on the number of pictures to download

```
#: whether to download the full image or its thumbnail (small size)
#: if small size is True then it will be very quick else if its False then it will open each photo to download it
#: and it will take much more time
friends_small_size = True
photos_small_size = True
```
----------------------------------------------------------------------------------------------------------------------------------------

"
7,harismuneer-Ultimate-Facebook-Scraper-README.md, Citation,"[![DOI](https://zenodo.org/badge/145763277.svg)](https://zenodo.org/badge/latestdoi/145763277)

If you use this tool for your research, then kindly cite it. Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE, APA etc.




----------------------------------------------------------------------------------------------------------------------------------------

"
8,harismuneer-Ultimate-Facebook-Scraper-README.md, Important Message,"This tool is for research purposes only. Hence, the developers of this tool won't be responsible for any misuse of data collected using this tool. 

----------------------------------------------------------------------------------------------------------------------------------------

"
9,harismuneer-Ultimate-Facebook-Scraper-README.md, Authors,"You can get in touch with us on our LinkedIn Profiles:

"
10,harismuneer-Ultimate-Facebook-Scraper-README.md, Haris Muneer,"[![LinkedIn Link](https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=linkedin&longCache=true&style=social&label=Connect
)](https://www.linkedin.com/in/harismuneer)

You can also follow my GitHub Profile to stay updated about my latest projects: [![GitHub Follow](https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=Github&longCache=true&style=social&label=Follow)](https://github.com/harismuneer)

"
11,harismuneer-Ultimate-Facebook-Scraper-README.md, Hassaan Elahi,"[![LinkedIn Link](https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=linkedin&longCache=true&style=social&label=Connect)](https://www.linkedin.com/in/hassaan-elahi/)

You can also follow my GitHub Profile to stay updated about my latest projects:[![GitHub Follow](https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=Github&longCache=true&style=social&label=Follow)](https://github.com/Hassaan-Elahi)


If you liked the repo then kindly support it by giving it a star ⭐!

"
12,harismuneer-Ultimate-Facebook-Scraper-README.md, Contributions Welcome,"[![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg)](#)

If you find any bug in the code or have any improvements in mind then feel free to generate a pull request.

"
13,harismuneer-Ultimate-Facebook-Scraper-README.md, Issues,"[![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues)

If you face any issue, you can create a new issue in the Issues Tab and I will be glad to help you out.

"
14,harismuneer-Ultimate-Facebook-Scraper-README.md, License,"[![MIT](https://img.shields.io/cocoapods/l/AFNetworking.svg?style=style&label=License&maxAge=2592000)](../master/LICENSE)

Copyright (c) 2018-present, harismuneer, Hassaan-Elahi                                                        
"
0,geonotebook-README.md, GeoNotebook [![CircleCI](https://circleci.com/gh/OpenGeoscience/geonotebook.svg?style=shield)](https://circleci.com/gh/OpenGeoscience/geonotebook) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/OpenGeoscience/geonotebook),"GeoNotebook is an application that provides client/server
environment with interactive visualization and analysis capabilities
using [Jupyter](http://jupyter.org), [GeoJS](http://www.github.com/OpenGeoscience/geojs) and other open source tools.
Jointly developed by  [Kitware](http://www.kitware.com) and
[NASA Ames](https://www.nasa.gov/centers/ames/home/index.html).

Documentation for GeoNotebook can be found at http://geonotebook.readthedocs.io.

"
1,geonotebook-README.md, Screenshots,"![screen shot](https://data.kitware.com/api/v1/file/5898b1788d777f07219fcafb/download?contentDisposition=inline)

Checkout some additional [screenshots](screenshots/)


"
3,geonotebook-README.md, System Prerequisites,"For default tile serving
  + GDAL >= 2.1.0
  + mapnik >= 3.1.0
  + python-mapnik >= 0.1

"
4,geonotebook-README.md, Clone the repo:,"```bash
git clone https://github.com/OpenGeoscience/geonotebook.git
cd geonotebook
```
"
5,geonotebook-README.md," Make a virtualenv, install jupyter[notebook], install geonotebook","```bash
mkvirtualenv -a . geonotebook

#: Numpy must be fully installed before rasterio
pip install -r prerequirements.txt

pip install -r requirements.txt

pip install .

#: Enable both the notebook and server extensions
jupyter serverextension enable --sys-prefix --py geonotebook
jupyter nbextension enable --sys-prefix --py geonotebook
```

*Note* The `serverextension` and `nbextension` commands accept flags that configure how
and where the extensions are installed.  See `jupyter serverextension --help` for more
information.

"
6,geonotebook-README.md, Installing geonotebook for development,"When developing geonotebook, it is often helpful to install packages as a reference to the
checked out repository rather than copying them to the system `site-packages`.  A ""development
install"" will allow you to make live changes to python or javascript without reinstalling the
package.
```bash
#: Install the geonotebook python package as ""editable""
pip install -e .

#: Install the notebook extension as a symlink
jupyter nbextension install --sys-prefix --symlink --py geonotebook

#: Enable the extension
jupyter serverextension enable --sys-prefix --py geonotebook
jupyter nbextension enable --sys-prefix --py geonotebook

#: Start the javascript builder
cd js
npm run watch
```

"
7,geonotebook-README.md, Run the notebook:,"```bash
cd notebooks/
jupyter notebook
```

"
8,geonotebook-README.md, Configure the notebook:,"Geonotebook relies on a configuration for several of its options. The system will merge configuration files in the following precedence:

+ /etc/geonotebook.ini
+ /usr/etc/geonotebook.ini
+ /usr/local/etc/geonotebook.ini
+ ```sys.prefix```/etc/geonotebook.ini 
  (e.g. /home/user/.virtual_environments/geonotebook/etc/geonotebook.inig)
+ ~/.geonotebook.ini
+ ```os.getcwd()```/.geonotebook.ini
+ any path specified in the ```GEONOTEBOOK_INI``` environment variable.

The [default configuration](config/geonotebook.ini) is installed in ```sys.prefix```/etc/geonotebook.ini


"
9,geonotebook-README.md, Run the tests,"```bash
#: From the source root
pip install -r requirements-dev.txt
tox

#: Optionally only run tests on python 2.7
#: tox -e py27
```

"
10,geonotebook-README.md, Docker Container,"System requirements for running the notebook can sometimes prove burdensome to install. To ease these issues we have included a [docker container](devops/docker) that will run the notebook inside a containerized process. 

"
11,geonotebook-README.md, Vagrant Machine,"Additionally there is a `Vagrantfile` for standing up an instance of Geonotebook within a virtual machine, further instructions can be found [here](Vagrant.md).

"
12,geonotebook-README.md, Tile Server,"By default geonotebook provides its own tile server based on [Mapnik](https://github.com/mapnik) and [GDAL](http://www.gdal.org/) as a Jupyter Notebook server extension. Assuming system pre-requisites are available this should not need to be configured. Alternately geonotebook may be configured to use a pre-existing [Geoserver](http://geoserver.org/) for serving tiles. A built in geoserver implementation is available as a virtual machine in devops/geoserver/.  

"
13,geonotebook-README.md, Use geoserver for tile serving,"First provision the geoserver

```
cd devops/geoserver/
vagrant up
```

Second change the ```vis_server``` configuration to ```geoserver``` in the ```[default]``` section of your configuration. Then include a ```[geoserver]``` section with the pertinent configuration.  E.g.:

```
[default]
vis_server=geoserver

...

[geoserver]
username = admin
password = geoserver
url = http://127.0.0.1:8080/geoserver
```
"
0,gitbucket-gitbucket-README.md, 4.31.2 - 7 Apr 2019,"- Bug and security fix

"
1,gitbucket-gitbucket-README.md, 4.31.1 - 17 Mar 2019,"- Bug fix

"
2,gitbucket-gitbucket-README.md, 4.31.0 - 17 Mar 2019,"- Docker support in CI plugin
- Verify GPG key signed commit
- OAuth2 Token (sent as a parameter) authentication support and new APIs in Web API
- OGP (Open Graph protocol) support
- Username completion with avatars

See the [change log](CHANGELOG.md) for all of the updates.
"
0,gprMax-README.md,"What is gprMax?
","`gprMax <http://www.gprmax.com>`_ is open source software that simulates electromagnetic wave propagation. It solves Maxwell's equations in 3D using the Finite-Difference Time-Domain (FDTD) method. gprMax was designed for modelling Ground Penetrating Radar (GPR) but can also be used to model electromagnetic wave propagation for many other applications.

gprMax is currently released under the `GNU General Public License v3 or higher <http://www.gnu.org/copyleft/gpl.html>`_.

gprMax is principally written in `Python <https://www.python.org>`_ 3 with performance-critical parts written in `Cython <http://cython.org>`_. It includes a CPU-based solver parallelised using `OpenMP <http://www.openmp.org>`_, and a GPU-based solver written using the `NVIDIA CUDA <https://developer.nvidia.com/cuda-zone>`_ programming model.

"
1,gprMax-README.md,"Using gprMax? Cite us
","If you use gprMax and publish your work we would be grateful if you could cite our work using:

* Warren, C., Giannopoulos, A., & Giannakis I. (2016). gprMax: Open source software to simulate electromagnetic wave propagation for Ground Penetrating Radar, `Computer Physics Communications` (http://dx.doi.org/10.1016/j.cpc.2016.08.020)

For further information on referencing gprMax visit the `Publications section of our website <http://www.gprmax.com/publications.shtml>`_.


"
2,gprMax-README.md,"Package overview
",".. code-block:: bash

    gprMax/
        conda_env.yml
        CONTRIBUTORS
        docs/
        gprMax/
        gsoc/
        LICENSE
        README.rst
        setup.cfg
        setup.py
        tests/
        tools/
        user_libs/
        user_models/


* ``conda_env.yml`` is a configuration file for Anaconda (Miniconda) that sets up a Python environment with all the required Python packages for gprMax.
* ``CONTRIBUTORS`` contains a list of names of people who have contributed to the gprMax codebase.
* ``docs`` contains source files for the User Guide. The User Guide is written using `reStructuredText <http://docutils.sourceforge.net/rst.html>`_ markup, and is built using `Sphinx <http://sphinx-doc.org>`_ and `Read the Docs <https://readthedocs.org>`_.
* ``gprMax`` is the main package. Within this package the main module is ``gprMax.py``
* ``gsoc`` contains information for `Google Summer of Code <https://summerofcode.withgoogle.com>`_ program - project ideas and proposal guidance.
* ``LICENSE`` contains information on the `GNU General Public License v3 or higher <http://www.gnu.org/copyleft/gpl.html>`_.
* ``README.rst`` contains getting started information on installation, usage, and new features/changes.
* ``setup.cfg`` is used to set preference for code formatting/styling using flake8.
* ``setup.py`` is used to compile the Cython extension modules.
* ``tests`` is a sub-package which contains test modules and input files.
* ``tools`` is a sub-package which contains scripts to assist with viewing and post-processing output from models.
* ``user_libs`` is a sub-package where useful modules contributed by users are stored.
* ``user_models`` is a sub-package where useful input files contributed by users are stored.

"
3,gprMax-README.md,"Installation
","The following steps provide guidance on how to install gprMax:

1. Install Python, required Python packages, and get the gprMax source code from GitHub
2. Install a C compiler which supports OpenMP
3. Build and install gprMax

You can `watch screencasts <http://docs.gprmax.com/en/latest/screencasts.html>`_ that demonstrate the installation and update processes.

"
4,gprMax-README.md,"1. Install Python, required Python packages, and get gprMax source
","We recommend using Miniconda to install Python and the required Python packages for gprMax in a self-contained Python environment. Miniconda is a mini version of Anaconda which is a completely free Python distribution (including for commercial use and redistribution). It includes more than 300 of the most popular Python packages for science, math, engineering, and data analysis.

* `Download and install Miniconda <http://conda.pydata.org/miniconda.html>`_. Choose the Python 3.x version for your platform. We recommend choosing the installation options to: install Miniconda only for your user account; add Miniconda to your PATH environment variable; and to register Miniconda Python as your default Python. See the `Quick Install page <http://conda.pydata.org/docs/install/quick.html>`_ for help installing Miniconda.
* Open a Terminal (Linux/macOS) or Command Prompt (Windows) and run the following commands:

.. code-block:: bash

    $ conda update conda
    $ conda install git
    $ git clone https://github.com/gprMax/gprMax.git
    $ cd gprMax
    $ conda env create -f conda_env.yml

This will make sure conda is up-to-date, install Git, get the latest gprMax source code from GitHub, and create an environment for gprMax with all the necessary Python packages.

If you prefer to install Python and the required Python packages manually, i.e. without using Anaconda/Miniconda, look in the ``conda_env.yml`` file for a list of the requirements.

"
5,gprMax-README.md,"2. Install a C compiler which supports OpenMP
","Linux
^^^^^

* `gcc <https://gcc.gnu.org>`_ should be already installed, so no action is required.


macOS
^^^^^

* Xcode (the IDE for macOS) comes with the LLVM (clang) compiler, but it does not currently support OpenMP, so you must install `gcc <https://gcc.gnu.org>`_. That said, it is still useful to have Xcode (with command line tools) installed. It can be downloaded from the App Store. Once Xcode is installed, download and install the `Homebrew package manager <http://brew.sh>`_ and then to install gcc, run:

.. code-block:: bash

    $ brew install gcc

Microsoft Windows
^^^^^^^^^^^^^^^^^

* Download and install `Microsoft Visual C++ 2015 Build Tools <http://download.microsoft.com/download/5/F/7/5F7ACAEB-8363-451F-9425-68A90F98B238/visualcppbuildtools_full.exe>`_ (currently you must use the 2015 version, not 2017). Use the custom installation option and deselect everything apart from the Windows SDK for your version of Windows.

Alternatively if you are using Windows 10 and feeling adventurous you can install the `Windows Subsystem for Linux <https://docs.microsoft.com/en-gb/windows/wsl/about>`_ and then follow the Linux install instructions for gprMax. Note however that currently WSL does not aim to support GUI desktops or applications, e.g. Gnome, KDE, etc....



"
6,gprMax-README.md,"3. Build and install gprMax
","Once you have installed the aforementioned tools follow these steps to build and install gprMax:

* Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`. Run the following commands:

.. code-block:: bash

    (gprMax)$ python setup.py build
    (gprMax)$ python setup.py install

**You are now ready to proceed to running gprMax.**

If you have problems with building gprMax on Microsoft Windows, you may need to add :code:`C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin` to your path environment variable.

"
7,gprMax-README.md,"Running gprMax
","gprMax is designed as a Python package, i.e. a namespace which can contain multiple packages and modules, much like a directory.

Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`.

Basic usage of gprMax is:

.. code-block:: bash

    (gprMax)$ python -m gprMax path_to/name_of_input_file

For example to run one of the test models:

.. code-block:: bash

    (gprMax)$ python -m gprMax user_models/cylinder_Ascan_2D.in

When the simulation is complete you can plot the A-scan using:

.. code-block:: bash

    (gprMax)$ python -m tools.plot_Ascan user_models/cylinder_Ascan_2D.out

Your results should like those from the A-scan from the metal cylinder example in `introductory/basic 2D models section <http://docs.gprmax.com/en/latest/examples_simple_2D.html#view-the-results>`_

When you are finished using gprMax, the conda environment can be deactivated using :code:`conda deactivate`.

"
8,gprMax-README.md,"Optional command line arguments
","====================== ========= ===========
Argument name          Type      Description
====================== ========= ===========
``-n``                 integer   number of times to run the input file. This option can be used to run a series of models, e.g. to create a B-scan with 60 traces: ``(gprMax)$ python -m gprMax user_models/cylinder_Bscan_2D.in -n 60``
``-gpu``               flag/list flag to use NVIDIA GPU or list of NVIDIA GPU device ID(s) for specific GPU card(s), e.g. ``-gpu 0 1``
``-restart``           integer   model number to start/restart simulation from. It would typically be used to restart a series of models from a specific model number, with the ``-n`` argument, e.g. to restart from A-scan 45 when creating a B-scan with 60 traces: ``(gprMax)$ python -m gprMax user_models/cylinder_Bscan_2D.in -n 15 -restart 45``
``-task``              integer   task identifier (model number) when running simulation as a job array on `Open Grid Scheduler/Grid Engine <http://gridscheduler.sourceforge.net/index.html>`_. For further details see the `parallel performance section of the User Guide <http://docs.gprmax.com/en/latest/openmp_mpi.html>`_
``-mpi``               integer   number of Message Passing Interface (MPI) tasks, i.e. master + workers, for MPI task farm. This option is most usefully combined with ``-n`` to allow individual models to be farmed out using a MPI task farm, e.g. to create a B-scan with 60 traces and use MPI to farm out each trace: ``(gprMax)$ python -m gprMax user_models/cylinder_Bscan_2D.in -n 60 -mpi 61``. For further details see the `parallel performance section of the User Guide <http://docs.gprmax.com/en/latest/openmp_mpi.html>`_
``--mpi-no-spawn``     flag      use MPI task farm without spawn mechanism. For further details see the `parallel performance section of the User Guide <http://docs.gprmax.com/en/latest/openmp_mpi.html>`_
``-benchmark``         flag      switch on benchmarking mode. This can be used to benchmark the threading (parallel) performance of gprMax on different hardware. For further details see the `benchmarking section of the User Guide <http://docs.gprmax.com/en/latest/benchmarking.html>`_
``--geometry-only``    flag      build a model and produce any geometry views but do not run the simulation, e.g. to check the geometry of a model is correct: ``(gprMax)$ python -m gprMax user_models/heterogeneous_soil.in --geometry-only``
``--geometry-fixed``   flag      run a series of models where the geometry does not change between models, e.g. a B-scan where *only* the position of simple sources and receivers, moved using ``#src_steps`` and ``#rx_steps``, changes between models.
``--opt-taguchi``      flag      run a series of models using an optimisation process based on Taguchi's method. For further details see the `user libraries section of the User Guide <http://docs.gprmax.com/en/latest/user_libs_opt_taguchi.html>`_
``--write-processed``  flag      write another input file after any Python code and include commands in the original input file have been processed. Useful for checking that any Python code is being correctly processed into gprMax commands.
``-h`` or ``--help``   flag      used to get help on command line options.
====================== ========= ===========

"
9,gprMax-README.md,"Updating gprMax
","* Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`. Run the following commands:

.. code-block:: bash

    (gprMax)$ git pull
    (gprMax)$ python setup.py cleanall
    (gprMax)$ python setup.py build
    (gprMax)$ python setup.py install

This will pull the most recent gprMax source code form GitHub, remove/clean previously built modules, and then build and install the latest version of gprMax.


"
10,gprMax-README.md,"Updating conda and Python packages
","Periodically you should update conda and the required Python packages. With the gprMax environment deactivated and from the top-level gprMax directory, run the following commands:

.. code-block:: bash

    $ conda update conda
    $ conda env update -f conda_env.yml
"
0,kosmtik-README.md, Kosmtik,"[![Join the chat at https://gitter.im/kosmtik/kosmtik](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/kosmtik/kosmtik?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Dependency Status](https://david-dm.org/kosmtik/kosmtik.svg)](https://david-dm.org/kosmtik/kosmtik)
[![Build Status](https://travis-ci.org/kosmtik/kosmtik.svg?branch=master)](https://travis-ci.org/kosmtik/kosmtik)

Very lite but extendable mapping framework to create Mapnik ready maps with
OpenStreetMap data (and more).

For now, only Carto based projects are supported (with .mml or .yml config),
but in the future we hope to plug in MapCSS too.


"
1,kosmtik-README.md, Lite,"Only the core needs:

- project loading
- local configuration management
- tiles server for live feedback when coding
- exports to common formats (Mapnik XML, PNG…)
- hooks everywhere to make easy to extend it with plugins


"
2,kosmtik-README.md, Screenshot,"![screenshot](https://raw.github.com/kosmtik/kosmtik/master/screenshot.png ""Screenshot of Kosmtik"")


"
3,kosmtik-README.md, Install or Update,"Note: Node.js versions are moving very fast, and kosmtik or its dependencies are
hardly totally up to date with latest release. Ideally, you should run the LTS
version of Node.js. You can use a Node.js version manager (like
[NVM](https://github.com/creationix/nvm)) to help.

    npm -g install kosmtik

This might need root/Administrator rights. If you cannot install globally
you can also install locally with

    npm install kosmtik

This will create a `node_modules/kosmtik` folder. You then have to replace all occurences of `kosmtik`
below with `node node_modules/kosmtik/index.js`.

To reinstall all plugins:

    kosmtik plugins --reinstall

"
4,kosmtik-README.md, Usage,"To get command line help, run:

    kosmtik -h

To run a Carto project (or `.yml`, `.yaml`):

    kosmtik serve <path/to/your/project.mml>

Then open your browser at http://127.0.0.1:6789/.


You may also want to install plugins. To see the list of available ones, type:

    kosmtik plugins --available

And then pick one and install it like this:

    kosmtik plugins --install pluginname

For example:

    kosmtik plugins --install kosmtik-map-compare [--install kosmtik-overlay…]


"
5,kosmtik-README.md, Configuration file,"By default Kosmtik places a configuration file into `$HOMEDIR/.config/kosmtik.yml`
where $HOMEDIR is your home directory. You can change that by setting the
environment variable `KOSMTIK_CONFIGPATH` to the appropriate file.

In the configuration file Kosmtik stores information about installed plugins and
you can change certain settings that should be persistent between runs.

Configurable settings are:
* autoReload (true/false)
* backendPolling (true/false)
* cacheVectorTiles (true/false)
* dataInspectorLayers (object with layer names and true/false)
* exportFormats (array of strings)
* showCrosshairs (true/false)

"
6,kosmtik-README.md, Local config,"Because you often need to change the project config to match your
local env, for example to adapt the database connection credentials,
kosmtik comes with an internal plugin to manage that. You have two
options: with a json file named `localconfig.json`, or with a js module
name `localconfig.js`.

Place your localconfig.js or localconfig.json file in the same directory as your
carto project (or `.yml`, `.yaml`).

In both cases, the behaviour is the same, you create some rules to target
the configuration and changes the values. Those rules are started by the
keyword `where`, and you define which changes to apply using `then`
keyword. You can also filter the targeted objects by using the `if` clause.
See the examples below to get it working right now.



"
7,kosmtik-README.md, Example of a json file,"```json
[
    {
        ""where"": ""center"",
        ""then"": [-122.25, 49.25, 10]
    },
    {
        ""where"": ""Layer"",
        ""if"": {
            ""Datasource.type"": ""postgis""
        },
        ""then"": {
            ""Datasource.dbname"": ""vancouver"",
            ""Datasource.password"": """",
            ""Datasource.user"": ""ybon"",
            ""Datasource.host"": """"
        }
    },
    {
        ""where"": ""Layer"",
        ""if"": {
            ""id"": ""hillshade""
        },
        ""then"": {
            ""Datasource.file"": ""/home/ybon/Code/maps/hdm/DEM/data/hillshade.vrt""
        }
    }
]
```

"
8,kosmtik-README.md, Example of a js module,"```javascript
exports.LocalConfig = function (localizer, project) {
    localizer.where('center').then([29.9377, -3.4216, 9]);
    localizer.where('Layer').if({'Datasource.type': 'postgis'}).then({
        ""Datasource.dbname"": ""burundi"",
        ""Datasource.password"": """",
        ""Datasource.user"": ""ybon"",
        ""Datasource.host"": """"
    });
    // You can also do it in pure JS
    project.mml.bounds = [1, 2, 3, 4];
};

```

"
9,kosmtik-README.md, Custom renderers,"By default Kosmtik uses [Carto](https://github.com/mapbox/carto) to render the style. Via plugins
it is possible to use other renderers or Carto implementations. You can switch the renderer installing
the appropriate plugin and by passing the command line option `--renderer NAME`. `NAME` refers to the
renderer name (e.g. `carto` for the default renderer or `magnacarto` for the Magnacarto renderer).

"
10,kosmtik-README.md, Known plugins,"- [kosmtik-overpass-layer](https://github.com/kosmtik/kosmtik-overpass-layer): add Overpass Layer in your carto project
- [kosmtik-fetch-remote](https://github.com/kosmtik/kosmtik-fetch-remote): automagically fetch remote files in your layers
- [kosmtik-place-search](https://github.com/kosmtik/kosmtik-place-search): search places control
- [kosmtik-overlay](https://github.com/kosmtik/kosmtik-overlay): add an overlay above the map
- [kosmtik-open-in-josm](https://github.com/kosmtik/kosmtik-open-in-josm): open JOSM with current view
- [kosmtik-map-compare](https://github.com/kosmtik/kosmtik-map-compare): display a map side-by-side with your work
- [kosmtik-osm-data-overlay](https://github.com/kosmtik/kosmtik-osm-data-overlay): display OSM data on top of your map
- [kosmtik-tiles-export](https://github.com/kosmtik/kosmtik-tiles-export): export a tiles tree from your project
- [kosmtik-mbtiles-export](https://github.com/kosmtik/kosmtik-mbtiles-export): export your project in MBTiles
- [kosmtik-magnacarto](https://github.com/kosmtik/kosmtik-magnacarto): Magnacarto renderer for CartoCSS

Run `kosmtik plugins --available` to get an up to date list.
"
0,tilematrix-README.md,"Tilematrix
","Tilematrix handles geographic web tiles and tile pyramids.

.. image:: https://badge.fury.io/py/tilematrix.svg
    :target: https://badge.fury.io/py/tilematrix

.. image:: https://travis-ci.org/ungarj/tilematrix.svg?branch=master
    :target: https://travis-ci.org/ungarj/tilematrix

.. image:: https://coveralls.io/repos/github/ungarj/tilematrix/badge.svg?branch=master
    :target: https://coveralls.io/github/ungarj/tilematrix?branch=master

.. image:: https://img.shields.io/pypi/pyversions/mapchete.svg


The module is designed to translate between tile indices (zoom, row, column) and
map coordinates (e.g. latitute, longitude).

Tilematrix supports **metatiling** and **tile buffers**. Furthermore it makes
heavy use of shapely_ and it can also generate ``Affine`` objects per tile which
facilitates working with rasterio_ for tile based data reading and writing.

It is very similar to mercantile_ but besides of supporting spherical mercator
tile pyramids, it also supports geodetic (WGS84) tile pyramids.

.. _shapely: http://toblerity.org/shapely/
.. _rasterio: https://github.com/mapbox/rasterio
.. _mercantile: https://github.com/mapbox/mercantile

------------
"
1,tilematrix-README.md,"Installation
","Use ``pip`` to install the latest stable version:

.. code-block:: shell

    pip install tilematrix

Manually install the latest development version

.. code-block:: shell

    pip install -r requirements.txt
    python setup.py install


-------------
"
2,tilematrix-README.md,"Documentation
","* `API documentation <doc/tilematrix.md>`_
* `examples <doc/examples.md>`_

"
3,tilematrix-README.md,"CLI
","This package ships with a command line tool ``tmx`` which provides the following
subcommands:

* ``bounds``: Print bounds of given Tile.
* ``bbox``: Print bounding box geometry of given Tile.
* ``tile``: Print Tile covering given point.
* ``tiles``: Print Tiles covering given bounds.

Geometry outputs can either be formatted as ``WKT`` or ``GeoJSON``. For example
the following command will print a valid ``GeoJSON`` representing all tiles
for zoom level 1 of the ``geodetic`` WMTS grid:

.. code-block:: shell

    $ tmx -f GeoJSON tiles -- 1 -180 -90 180 90
    {
      ""type"": ""FeatureCollection"",
      ""features"": [
        {""geometry"": {""coordinates"": [[[-90.0, 0.0], [-90.0, 90.0], [-180.0, 90.0], [-180.0, 0.0], [-90.0, 0.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 0, ""row"": 0, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[0.0, 0.0], [0.0, 90.0], [-90.0, 90.0], [-90.0, 0.0], [0.0, 0.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 1, ""row"": 0, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[90.0, 0.0], [90.0, 90.0], [0.0, 90.0], [0.0, 0.0], [90.0, 0.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 2, ""row"": 0, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[180.0, 0.0], [180.0, 90.0], [90.0, 90.0], [90.0, 0.0], [180.0, 0.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 3, ""row"": 0, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[-90.0, -90.0], [-90.0, 0.0], [-180.0, 0.0], [-180.0, -90.0], [-90.0, -90.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 0, ""row"": 1, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[0.0, -90.0], [0.0, 0.0], [-90.0, 0.0], [-90.0, -90.0], [0.0, -90.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 1, ""row"": 1, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[90.0, -90.0], [90.0, 0.0], [0.0, 0.0], [0.0, -90.0], [90.0, -90.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 2, ""row"": 1, ""zoom"": 1}, ""type"": ""Feature""},
        {""geometry"": {""coordinates"": [[[180.0, -90.0], [180.0, 0.0], [90.0, 0.0], [90.0, -90.0], [180.0, -90.0]]], ""type"": ""Polygon""}, ""properties"": {""col"": 3, ""row"": 1, ""zoom"": 1}, ""type"": ""Feature""}
      ]
    }



Print ``WKT`` representation of tile ``4 15 23``:

.. code-block:: shell

    $ tmx bbox 4 15 23
    POLYGON ((90 -90, 90 -78.75, 78.75 -78.75, 78.75 -90, 90 -90))


Also, tiles can have buffers around called ``pixelbuffer``:

.. code-block:: shell

    $ tmx --pixelbuffer 10 bbox 4 15 23
    POLYGON ((90.439453125 -90, 90.439453125 -78.310546875, 78.310546875 -78.310546875, 78.310546875 -90, 90.439453125 -90))


Print ``GeoJSON`` representation of tile ``4 15 23`` on a ``mercator`` tile
pyramid:

.. code-block:: shell

    $ tmx -output_format GeoJSON -grid mercator bbox 4 15 15
    {""type"": ""Polygon"", ""coordinates"": [[[20037508.342789203, -20037508.3427892], [20037508.342789203, -17532819.799940553], [17532819.799940553, -17532819.799940553], [17532819.799940553, -20037508.3427892], [20037508.342789203, -20037508.3427892]]]}



-------
"
4,tilematrix-README.md,"License
","MIT License

Copyright (c) 2015, 2016, 2017 `EOX IT Services`_

.. _`EOX IT Services`: https://eox.at/
"
0,gitfolio-README.md, Gitfolio  [![Tweet](https://img.shields.io/twitter/url/https/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=personal%20website%20and%20a%20blog%20for%20every%20github%20user%20&url=https://github.com/imfunniee/gitfolio) ![GitHub release](https://img.shields.io/github/release/imfunniee/gitfolio.svg?style=popout-square) ![npm](https://img.shields.io/npm/dm/gitfolio.svg?style=popout-square) ![GitHub top language](https://img.shields.io/github/languages/top/imfunniee/gitfolio.svg?style=popout-square) ![GitHub last commit](https://img.shields.io/github/last-commit/imfunniee/gitfolio.svg?style=popout-square) ![GitHub](https://img.shields.io/github/license/imfunniee/gitfolio.svg?style=popout-square),"  
"
1,gitfolio-README.md, personal website + blog  for every github user,"Gitfolio will help you get started with a portfolio website where you could showcase your work + a blog that will help you spread your ideas into real world.

Check out this [live demo](https://imfunniee.github.io/gitfolio/) to see gitfolio in action.


"
3,gitfolio-README.md, Let's Install,"Install gitfolio

```sh
npm i gitfolio -g
```

"
4,gitfolio-README.md, Let's Build,"```sh
gitfolio build <username>
```
`<username>` is your username on github. This will build your website using your GitHub username and put it in the `/dist` folder.

To run your website use `run` command

```sh
gitfolio run
```

Open your browser at http://localhost:3000

🎉 Congrats, you just made yourself a personal website!


"
6,gitfolio-README.md, Forks,"To include forks on your personal website just provide `-f` or `--fork` argument while building

```sh
$ gitfolio build <username> -f
```

"
7,gitfolio-README.md, Sorting Repos,"To sort repos provide `--sort [sortBy]` argument while building. Where `[sortBy]` can be `star`, `created`, `updated`, `pushed`,`full_name`. Default: `created`

```sh
$ gitfolio build <username> --sort star
```

"
8,gitfolio-README.md, Ordering Repos,"To order the sorted repos provide `--order [orderBy]` argument while building. Where `[orderBy]` can be `asc` or `desc`. Default: `asc`

```sh
$ gitfolio build <username> --sort star --order desc
```

"
9,gitfolio-README.md, Customize Themes,"Themes are specified using the `--theme [theme-name]` flag when running the `build` command. The available themes are

* `light`
* `dark`
> TODO: Add more themes

For example, the following command will build the website with the dark theme
```sh
$ gitfolio build <username> --theme dark
```

"
10,gitfolio-README.md, Customize background image,"To customize the background image just provide `--background [url]` argument while building

```sh
$ gitfolio build <username> --background https://images.unsplash.com/photo-1557277770-baf0ca74f908?w=1634
```

You could also add in your custom CSS inside `index.css` to give it a more personal feel.


"
11,gitfolio-README.md, Let's Publish,"Head over to GitHub and create a new repository named `username.github.io`, where username is your username. Push the files inside`/dist` folder to repo you just created.

Go To `username.github.io` your site should be up!!


"
12,gitfolio-README.md, Updating,"To update your info, simply run

```sh
$ gitfolio update
```
This will update your info and your repository info.

To Update background or theme you need to run `build` command again.


"
13,gitfolio-README.md, Add a Blog,"To add your first blog run this command.

```sh
$ gitfolio blog my-first-blog
```
> (use ""-"" instead of spaces)

This will create a `my-first-blog` folder inside `blog`. Inside `my-first-blog` you will find an `index.html` file which contains all the necessary elements for writing a blog. Customize the content of the file to write your first blog.

This also adds content to `blog.json` file. This file helps in showcasing your blogs on your personal website as [cards](https://imfunniee.github.io/gitfolio/#blog_section). You could customize the JSON object that corresponds your current blog.

Blog Demo? [here](https://imfunniee.github.io/gitfolio/blog/my-first-post/)

Default JSON Format
```
{
  ""url_title"": ""my-first-blog"", // the title you provide while creating a new blog, this appears in url
  ""title"": ""Lorem ipsum dolor sit amet"", // main title of blog
  ""sub_title"": ""Lorem ipsum dolor sit amet, consectetur adipiscing elit."", // sub-title of blog
  ""top_image"": ""https://images.unsplash.com/photo-1553748024-d1b27fb3f960?w=1450"", // main image of blog
  ""visible"": true // don't worry about this
}
```

More Arguments for Blog

```
--subtitle [subtitle] : gives blog a subtitle (Default : 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.')
--pagetitle [pagetitle] : gives blog page a title
--folder [folder] : give folder a title
```

> (use ""-"" instead of spaces)


"
14,gitfolio-README.md, License,"![GitHub](https://img.shields.io/github/license/imfunniee/gitfolio.svg?style=popout-square)
"
0,two-stream-dyntex-synth-README.md, Two-Stream Convolutional Networks for Dynamic Texture Synthesis,"![Dynamic texture synthesis](teaser.gif ""Dynamic texture synthesis"")

"
1,two-stream-dyntex-synth-README.md, Requirements,"- Tensorflow 1.3 (or latest, although not tested)
- Preferably a Titan X for synthesizing 12 frames
- Appearance-stream [tfmodel](https://drive.google.com/open?id=19KkFi92oWLzuOWnGo6Zsqe-2CCXFAoXZ)
- Dynamics-stream [tfmodel](https://drive.google.com/open?id=1DHnzoNO-iTgMUTbUOLrigEmpPHmn_mT1)
- [Dynamic textures](https://drive.google.com/open?id=0B5T9jWfa9iDySWJHZnpNZ2dHWUk)
- [Static textures](https://drive.google.com/open?id=11yMiPXiuYvLCyoLfQf_dEG6kuav8h6_3) (for dynamics style transfer)

"
2,two-stream-dyntex-synth-README.md, Setup,"1. Store the appearance-stream tfmodel in `./models`.
2. Store the dynamics-stream tfmodel in `./models`. The filepath to this model is your `--dynamics_model` path.

"
3,two-stream-dyntex-synth-README.md, Dynamic texture synthesis,"```
python synthesize.py --type=dts --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL>
```

Store your chosen dynamic texture image sequence in a folder in `/data/dynamic_textures`. This folder is your `--dynamics_target` path.

"
4,two-stream-dyntex-synth-README.md, Example usage,"```
python synthesize.py --type=dts --gpu=0 --runid=""my_cool_fish"" --dynamics_target=data/dynamic_textures/fish --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel
```

"
5,two-stream-dyntex-synth-README.md, Dynamics style transfer,"```
python synthesize.py --type=dst --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL> --appearance_target=data/textures/<IMAGE>
```

Store your chosen static texture in `./data/textures`. The filepath to this texture is your `--appearance_target` path.

"
6,two-stream-dyntex-synth-README.md, Example usage,"```
python synthesize.py --type=dst --gpu=0 --runid=""whoa_water!"" --dynamics_target=data/dynamic_textures/water_4 --appearance_target=data/textures/water_paint_cropped.jpeg --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel
```

"
7,two-stream-dyntex-synth-README.md, Temporally-endless dynamic texture synthesis,"```
python synthesize.py --type=inf --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL>
```

"
8,two-stream-dyntex-synth-README.md, Incremental dynamic texture synthesis,"```
python synthesize.py --type=inc --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL> --appearance_target=data/textures/<IMAGE>
```

Store your chosen static texture in `/data/textures`. The filepath to this texture is your `--appearance_target` path. This texture should be the last frame of a previously generated sequence.

"
9,two-stream-dyntex-synth-README.md, Static texture synthesis,"```
python synthesize.py --type=sta --gpu=<NUMBER> --runid=<NAME> --appearance_target=data/textures/<IMAGE>
```

[Gatys et al.'s](https://arxiv.org/abs/1505.07376) method of texture synthesis.

"
10,two-stream-dyntex-synth-README.md, Notes,"The network's output is saved at `data/out/<RUNID>`.

Use `./useful_scripts/makegif.sh` to create a gif from a folder of images, e.g.,
```
./useful_scripts/makegif.sh ""data/out/calm_water/iter_6000*"" calm_water.gif
```
will create the gif `calm_water.gif` from the images `iter_6000*` in the `calm_water` output folder.

Logs and snapshots are created and stored in `./logs/<RUNID>` and `./snapshots/<RUNID>`, respectively. You can view the loss progress for a particular run in Tensorboard.

"
11,two-stream-dyntex-synth-README.md, Citation,"```
@inproceedings{tesfaldet2018,
  author = {Matthew Tesfaldet and Marcus A. Brubaker and Konstantinos G. Derpanis},
  title = {Two-Stream Convolutional Networks for Dynamic Texture Synthesis},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2018}
}
```

"
12,two-stream-dyntex-synth-README.md, License,"Two-Stream Convolutional Networks for Dynamic Texture Synthesis
Copyright (C) 2018  Matthew Tesfaldet

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.


For questions, please contact Matthew Tesfaldet ([mtesfald@eecs.yorku.ca](mailto:mtesfald@eecs.yorku.ca)).
"
0,PRM-README.md, PyTorch Implementation,"The [pytorch branch](https://github.com/ZhouYanzhao/PRM/tree/pytorch) contains:

* the **pytorch** implementation of Peak Response Mapping (Stimulation and Backprop).
* the PASCAL-VOC demo (training, inference, and visualization).

Please follow the instruction below to install it and run the experiment demo.

"
1,PRM-README.md, Prerequisites,"* System (tested on Ubuntu 14.04LTS and Win10)
* NVIDIA GPU + CUDA CuDNN (CPU mode is also supported but significantly slower)
* [Python>=3.5](https://www.python.org)
* [PyTorch>=0.4](https://pytorch.org)
* [Jupyter Notebook](https://jupyter.org/install.html) and [ipywidgets](https://github.com/jupyter-widgets/ipywidgets) (required by the demo):

    ```bash
    #: enable the widgetsnbextension before you start the notebook server
    jupyter nbextension enable --py --sys-prefix widgetsnbextension
    ```

"
2,PRM-README.md, Installation,"1. Install [Nest](https://github.com/ZhouYanzhao/Nest), a flexible tool for building and sharing deep learning modules:
    
    > I created Nest in the process of refactoring PRM's pytorch implementation. It aims at encouraging code reuse and ships with a bunch of useful features. PRM is now implemented as a set of Nest modules; thus you can easily install and use it as demonstrated below.

    ```bash
    $ pip install git+https://github.com/ZhouYanzhao/Nest.git
    ```
    

2. Install PRM via Nest's CLI tool:

    ```bash
    #: note that data will be saved under your current path
    $ nest module install github@ZhouYanzhao/PRM:pytorch prm
    #: verify the installation
    $ nest module list --filter prm
    #: Output:
    #:
    #: 3 Nest modules found.
    #: [0] prm.fc_resnet50 (1.0.0)
    #: [1] prm.peak_response_mapping (1.0.0)
    #: [2] prm.prm_visualize (1.0.0)
    ```

"
3,PRM-README.md, Run demo,"1. Install Nest's build-in Pytorch modules:

    > To increase reusability, I abstracted some features from the original code, such as network trainer, to build Nest's built-in pytorch module set.
    
    ```bash
    $ nest module install github@ZhouYanzhao/Nest:pytorch pytorch
    ```

2. Download the PASCAL-VOC2012 dataset:

    ```bash
    mkdir ./PRM/demo/datasets
    cd ./PRM/demo/datasets
    #: download and extract data
    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
    tar xvf VOCtrainval_11-May-2012.tar
    ```

3. Run the demo experiment via [demo/main.ipynb](https://github.com/ZhouYanzhao/PRM/tree/pytorch/demo/main.ipynb)

    ![PRM Segmentation](samples.png)

"
4,PRM-README.md, Citation ,"If you find the code useful for your research, please cite:
```bibtex
@INPROCEEDINGS{Zhou2018PRM,
    author = {Zhou, Yanzhao and Zhu, Yi and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
    title = {Weakly Supervised Instance Segmentation using Class Peak Response},
    booktitle = {CVPR},
    year = {2018}
}
```
"
0,tippecanoe-README.md," Linear features (world railroads), visible at all zoom levels","```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_railroads.zip
unzip ne_10m_railroads.zip
ogr2ogr -f GeoJSON ne_10m_railroads.geojson ne_10m_railroads.shp

tippecanoe -zg -o ne_10m_railroads.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping ne_10m_railroads.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--drop-densest-as-needed`: If the tiles are too big at low zoom levels, drop the least-visible features to allow tiles to be created with those features that remain
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

"
1,tippecanoe-README.md," Discontinuous polygon features (buildings of Rhode Island), visible at all zoom levels","```
curl -L -O https://usbuildingdata.blob.core.windows.net/usbuildings-v1-1/RhodeIsland.zip
unzip RhodeIsland.zip

tippecanoe -zg -o RhodeIsland.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping RhodeIsland.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--drop-densest-as-needed`: If the tiles are too big at low or medium zoom levels, drop the least-visible features to allow tiles to be created with those features that remain
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

"
2,tippecanoe-README.md," Continuous polygon features (states and provinces), visible at all zoom levels","```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip
unzip -o ne_10m_admin_1_states_provinces.zip
ogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp

tippecanoe -zg -o ne_10m_admin_1_states_provinces.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

"
3,tippecanoe-README.md," Large point dataset (GPS bus locations), for visualization at all zoom levels","```
curl -L -O ftp://avl-data.sfmta.com/avl_data/avl_raw/sfmtaAVLRawData01012013.csv
sed 's/PREDICTABLE.*/PREDICTABLE/' sfmtaAVLRawData01012013.csv > sfmta.csv
tippecanoe -zg -o sfmta.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping sfmta.csv
```

(The `sed` line is to clean the corrupt CSV header, which contains the wrong number of fields.)

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--drop-densest-as-needed`: If the tiles are too big at low or medium zoom levels, drop the least-visible features to allow tiles to be created with those features that remain
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

"
4,tippecanoe-README.md," Clustered points (world cities), summing the clustered population, visible at all zoom levels","```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_populated_places.zip
unzip -o ne_10m_populated_places.zip
ogr2ogr -f GeoJSON ne_10m_populated_places.geojson ne_10m_populated_places.shp

tippecanoe -zg -o ne_10m_populated_places.mbtiles -r1 --cluster-distance=10 --accumulate-attribute=POP_MAX:sum ne_10m_populated_places.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `-r1`: Do not automatically drop a fraction of points at low zoom levels, since clustering will be used instead
* `--cluster-distance=10`: Cluster together features that are closer than about 10 pixels from each other
* `--accumulate-attribute=POP_MAX:sum`: Sum the `POP_MAX` (population) attribute in features that are clustered together. Other attributes will be arbitrarily taken from the first feature in the cluster.

"
5,tippecanoe-README.md, Show countries at low zoom levels but states at higher zoom levels,"```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip
unzip ne_10m_admin_0_countries.zip
ogr2ogr -f GeoJSON ne_10m_admin_0_countries.geojson ne_10m_admin_0_countries.shp

curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip
unzip -o ne_10m_admin_1_states_provinces.zip
ogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp

tippecanoe -z3 -o countries-z3.mbtiles --coalesce-densest-as-needed ne_10m_admin_0_countries.geojson
tippecanoe -zg -Z4 -o states-Z4.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson
tile-join -o states-countries.mbtiles countries-z3.mbtiles states-Z4.mbtiles
```

Countries:

* `-z3`: Only generate zoom levels 0 through 3
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished

States and Provinces:

* `-Z4`: Only generate zoom levels 4 and beyond
* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

"
6,tippecanoe-README.md, Represent multiple sources (Illinois and Indiana counties) as separate layers,"```
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_17_county10.zip
unzip tl_2010_17_county10.zip
ogr2ogr -f GeoJSON tl_2010_17_county10.geojson tl_2010_17_county10.shp

curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_18_county10.zip
unzip tl_2010_18_county10.zip
ogr2ogr -f GeoJSON tl_2010_18_county10.geojson tl_2010_18_county10.shp

tippecanoe -zg -o counties-separate.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping tl_2010_17_county10.geojson tl_2010_18_county10.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

"
7,tippecanoe-README.md, Merge multiple sources (Illinois and Indiana counties) into the same layer,"```
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_17_county10.zip
unzip tl_2010_17_county10.zip
ogr2ogr -f GeoJSON tl_2010_17_county10.geojson tl_2010_17_county10.shp

curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_18_county10.zip
unzip tl_2010_18_county10.zip
ogr2ogr -f GeoJSON tl_2010_18_county10.geojson tl_2010_18_county10.shp

tippecanoe -zg -o counties-merged.mbtiles -l counties --coalesce-densest-as-needed --extend-zooms-if-still-dropping tl_2010_17_county10.geojson tl_2010_18_county10.geojson
```

As above, but

* `-l counties`: Specify the layer name instead of letting it be derived from the source file names

"
8,tippecanoe-README.md, Selectively remove and replace features (Census tracts) to update a tileset,"```
#: Retrieve and tile California 2000 Census tracts
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2000/tl_2010_06_tract00.zip
unzip tl_2010_06_tract00.zip
ogr2ogr -f GeoJSON tl_2010_06_tract00.shp.json tl_2010_06_tract00.shp
tippecanoe -z11 -o tracts.mbtiles -l tracts tl_2010_06_tract00.shp.json

#: Create a copy of the tileset, minus Alameda County (FIPS code 001)
tile-join -j '{""*"":[""none"",[""=="",""COUNTYFP00"",""001""]]}' -f -o tracts-filtered.mbtiles tracts.mbtiles

#: Retrieve and tile Alameda County Census tracts for 2010
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_06001_tract10.zip
unzip tl_2010_06001_tract10.zip
ogr2ogr -f GeoJSON tl_2010_06001_tract10.shp.json tl_2010_06001_tract10.shp
tippecanoe -z11 -o tracts-added.mbtiles -l tracts tl_2010_06001_tract10.shp.json

#: Merge the filtered tileset and the tileset of new tracts into a final tileset
tile-join -o tracts-final.mbtiles tracts-filtered.mbtiles tracts-added.mbtiles
```

The `-z11` option explicitly specifies the maxzoom, to make sure both the old and new tilesets have the same zoom range.

The `-j` option to `tile-join` specifies a filter, so that only the desired features will be copied to the new tileset.
This filter excludes (using `none`) any features whose FIPS code (`COUNTYFP00`) is the code for Alameda County (`001`).

Options
-------

There are a lot of options. A lot of the time you won't want to use any of them
other than `-o` _output_`.mbtiles` to name the output file, and probably `-f` to
delete the file that already exists with that name.

If you aren't sure what the right maxzoom is for your data, `-zg` will guess one for you
based on the density of features.

Tippecanoe will normally drop a fraction of point features at zooms below the maxzoom,
to keep the low-zoom tiles from getting too big. If you have a smaller data set where
all the points would fit without dropping any of them, use `-r1` to keep them all.
If you do want point dropping, but you still want the tiles to be denser than `-zg`
thinks they should be, use `-B` to set a basezoom lower than the maxzoom.

If some of your tiles are coming out too big in spite of the settings above, you will
often want to use `--drop-densest-as-needed` to drop whatever fraction of the features
is necessary at each zoom level to make that zoom level's tiles work.

If your features have a lot of attributes, use `-y` to keep only the ones you really need.

If your input is formatted as newline-delimited GeoJSON, use `-P` to make input parsing a lot faster.

"
9,tippecanoe-README.md, Output tileset," * `-o` _file_`.mbtiles` or `--output=`_file_`.mbtiles`: Name the output file.
 * `-e` _directory_ or `--output-to-directory`=_directory_: Write tiles to the specified *directory* instead of to an mbtiles file.
 * `-f` or `--force`: Delete the mbtiles file if it already exists instead of giving an error
 * `-F` or `--allow-existing`: Proceed (without deleting existing data) if the metadata or tiles table already exists
   or if metadata fields can't be set. You probably don't want to use this.

"
10,tippecanoe-README.md, Tileset description and attribution," * `-n` _name_ or `--name=`_name_: Human-readable name for the tileset (default file.json)
 * `-A` _text_ or `--attribution=`_text_: Attribution (HTML) to be shown with maps that use data from this tileset.
 * `-N` _description_ or `--description=`_description_: Description for the tileset (default file.mbtiles)

"
11,tippecanoe-README.md, Input files and layer names," * _name_`.json` or _name_`.geojson`: Read the named GeoJSON input file into a layer called _name_.
 * _name_`.json.gz` or _name_`.geojson.gz`: Read the named gzipped GeoJSON input file into a layer called _name_.
 * _name_`.geobuf`: Read the named Geobuf input file into a layer called _name_.
 * _name_`.csv`: Read the named CSV input file into a layer called _name_.
 * `-l` _name_ or `--layer=`_name_: Use the specified layer name instead of deriving a name from the input filename or output tileset. If there are multiple input files
   specified, the files are all merged into the single named layer, even if they try to specify individual names with `-L`.
 * `-L` _name_`:`_file.json_ or `--named-layer=`_name_`:`_file.json_: Specify layer names for individual files. If your shell supports it, you can use a subshell redirect like `-L` _name_`:<(cat dir/*.json)` to specify a layer name for the output of streamed input.
 * `-L{`_layer-json_`}` or `--named-layer={`_layer-json_`}`: Specify an input file and layer options by a JSON object. The JSON object must contain a `""file""` key to specify the filename to read from. (If the `""file""` key is an empty string, it means to read from the standard input stream.) It may also contain a `""layer""` field to specify the name of the layer, and/or a `""description""` field to specify the layer's description in the tileset metadata, and/or a `""format""` field to specify `csv` or `geobuf` file format if it is not obvious from the `name`. Example:

```
tippecanoe -z5 -o world.mbtiles -L'{""file"":""ne_10m_admin_0_countries.json"", ""layer"":""countries"", ""description"":""Natural Earth countries""}'
```

CSV input files currently support only Point geometries, from columns named `latitude`, `longitude`, `lat`, `lon`, `long`, `lng`, `x`, or `y`.

"
12,tippecanoe-README.md, Parallel processing of input," * `-P` or `--read-parallel`: Use multiple threads to read different parts of each GeoJSON input file at once.
   This will only work if the input is line-delimited JSON with each Feature on its
   own line, because it knows nothing of the top-level structure around the Features. Spurious ""EOF"" error
   messages may result otherwise.
   Performance will be better if the input is a named file that can be mapped into memory
   rather than a stream that can only be read sequentially.

If the input file begins with the [RFC 8142](https://tools.ietf.org/html/rfc8142) record separator,
parallel processing of input will be invoked automatically, splitting at record separators rather
than at all newlines.

Parallel processing will also be automatic if the input file is in Geobuf format.

"
13,tippecanoe-README.md, Projection of input," * `-s` _projection_ or `--projection=`_projection_: Specify the projection of the input data. Currently supported are `EPSG:4326` (WGS84, the default) and `EPSG:3857` (Web Mercator). In general you should use WGS84 for your input files if at all possible.

"
14,tippecanoe-README.md, Zoom levels," * `-z` _zoom_ or `--maximum-zoom=`_zoom_: Maxzoom: the highest zoom level for which tiles are generated (default 14)
 * `-zg` or `--maximum-zoom=g`: Guess what is probably a reasonable maxzoom based on the spacing of features.
 * `-Z` _zoom_ or `--minimum-zoom=`_zoom_: Minzoom: the lowest zoom level for which tiles are generated (default 0)
 * `-ae` or `--extend-zooms-if-still-dropping`: Increase the maxzoom if features are still being dropped at that zoom level.
   The detail and simplification options that ordinarily apply only to the maximum zoom level will apply both to the originally
   specified maximum zoom and to any levels added beyond that.
 * `-R` _zoom_`/`_x_`/`_y_ or `--one-tile=`_zoom_`/`_x_`/`_y_: Set the minzoom and maxzoom to _zoom_ and produce only
   the single specified tile at that zoom level.

If you know the precision to which you want your data to be represented,
or the map scale of a corresponding printed map,
this table shows the approximate precision and scale corresponding to various
`-z` options if you use the default `-d` detail of 12:

zoom level | precision (ft) | precision (m) | map scale
---------- | -------------- | ------------- | ---------
`-z0` | 32000 ft | 10000 m | 1:320,000,000
`-z1` | 16000 ft | 5000 m | 1:160,000,000
`-z2` | 8000 ft | 2500 m | 1:80,000,000
`-z3` | 4000 ft | 1250 m | 1:40,000,000
`-z4` | 2000 ft | 600 m | 1:20,000,000
`-z5` | 1000 ft | 300 m | 1:10,000,000
`-z6` | 500 ft | 150 m | 1:5,000,000
`-z7` | 250 ft | 80 m | 1:2,500,000
`-z8` | 125 ft | 40 m | 1:1,250,000
`-z9` | 64 ft | 20 m | 1:640,000
`-z10` | 32 ft | 10 m | 1:320,000
`-z11` | 16 ft | 5 m | 1:160,000
`-z12` | 8 ft | 2 m | 1:80,000
`-z13` | 4 ft | 1 m | 1:40,000
`-z14` | 2 ft | 0.5 m | 1:20,000
`-z15` | 1 ft | 0.25 m | 1:10,000

"
15,tippecanoe-README.md, Tile resolution," * `-d` _detail_ or `--full-detail=`_detail_: Detail at max zoom level (default 12, for tile resolution of 2^12=4096)
 * `-D` _detail_ or `--low-detail=`_detail_: Detail at lower zoom levels (default 12, for tile resolution of 2^12=4096)
 * `-m` _detail_ or `--minimum-detail=`_detail_: Minimum detail that it will try if tiles are too big at regular detail (default 7)

All internal math is done in terms of a 32-bit tile coordinate system, so 1/(2^32) of the size of Earth,
or about 1cm, is the smallest distinguishable distance. If _maxzoom_ + _detail_ > 32, no additional
resolution is obtained than by using a smaller _maxzoom_ or _detail_.

"
16,tippecanoe-README.md, Filtering feature attributes," * `-x` _name_ or `--exclude=`_name_: Exclude the named attributes from all features. You can specify multiple `-x` options to exclude several attributes. (Don't comma-separate names within a single `-x`.)
 * `-y` _name_ or `--include=`_name_: Include the named attributes in all features, excluding all those not explicitly named. You can specify multiple `-y` options to explicitly include several attributes. (Don't comma-separate names within a single `-y`.)
 * `-X` or `--exclude-all`: Exclude all attributes and encode only geometries

"
17,tippecanoe-README.md, Modifying feature attributes," * `-T`_attribute_`:`_type_ or `--attribute-type=`_attribute_`:`_type_: Coerce the named feature _attribute_ to be of the specified _type_.
   The _type_ may be `string`, `float`, `int`, or `bool`.
   If the type is `bool`, then original attributes of `0` (or, if numeric, `0.0`, etc.), `false`, `null`, or the empty string become `false`, and otherwise become `true`.
   If the type is `float` or `int` and the original attribute was non-numeric, it becomes `0`.
   If the type is `int` and the original attribute was floating-point, it is rounded to the nearest integer.
 * `-Y`_attribute_`:`_description_ or `--attribute-description=`_attribute_`:`_description_: Set the `description` for the specified attribute in the tileset metadata to _description_ instead of the usual `String`, `Number`, or `Boolean`.
 * `-E`_attribute_`:`_operation_ or `--accumulate-attribute=`_attribute_`:`_operation_: Preserve the named _attribute_ from features
   that are dropped, coalesced-as-needed, or clustered. The _operation_ may be
   `sum`, `product`, `mean`, `max`, `min`, `concat`, or `comma`
   to specify how the named _attribute_ is accumulated onto the attribute of the same name in a feature that does survive.
 * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.
 * `-aI` or `--convert-stringified-ids-to-numbers`: If a feature ID is the string representation of a number, convert it to a plain number to use as the feature ID.
 * `--use-attribute-for-id=`*name*: Use the attribute with the specified *name* as if it were specified as the feature ID. (If this attribute is a stringified number, you must also use `-aI` to convert it to a number.)

"
18,tippecanoe-README.md, Filtering features by attributes," * `-j` *filter* or `--feature-filter`=*filter*: Check features against a per-layer filter (as defined in the [Mapbox GL Style Specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/#other-filter)) and only include those that match. Any features in layers that have no filter specified will be passed through. Filters for the layer `""*""` apply to all layers. The special variable `$zoom` refers to the current zoom level.
 * `-J` *filter-file* or `--feature-filter-file`=*filter-file*: Like `-j`, but read the filter from a file.

Example: to find the Natural Earth countries with low `scalerank` but high `LABELRANK`:

```
tippecanoe -z5 -o filtered.mbtiles -j '{ ""ne_10m_admin_0_countries"": [ ""all"", [ ""<"", ""scalerank"", 3 ], [ "">"", ""LABELRANK"", 5 ] ] }' ne_10m_admin_0_countries.geojson
```

Example: to retain only major TIGER roads at low zoom levels:

```
tippecanoe -o roads.mbtiles -j '{ ""*"": [ ""any"", [ "">="", ""$zoom"", 11 ], [ ""in"", ""MTFCC"", ""S1100"", ""S1200"" ] ] }' tl_2015_06001_roads.json
```

Tippecanoe also accepts expressions of the form `[ ""attribute-filter"", name, expression ]`, to filter individual feature attributes
instead of entire features. For example, you can exclude the road names at low zoom levels by doing

```
tippecanoe -o roads.mbtiles -j '{ ""*"": [ ""attribute-filter"", ""FULLNAME"", [ "">="", ""$zoom"", 9 ] ] }' tl_2015_06001_roads.json
```

An `attribute-filter` expression itself is always considered to evaluate to `true` (in other words, to retain the feature instead
of dropping it). If you want to use multiple `attribute-filter` expressions, or to use other expressions to remove features from
the same layer, enclose them in an `all` expression so they will all be evaluated.

"
19,tippecanoe-README.md, Dropping a fixed fraction of features by zoom level," * `-r` _rate_ or `--drop-rate=`_rate_: Rate at which dots are dropped at zoom levels below basezoom (default 2.5).
   If you use `-rg`, it will guess a drop rate that will keep at most 50,000 features in the densest tile.
   You can also specify a marker-width with `-rg`*width* to allow fewer features in the densest tile to
   compensate for the larger marker, or `-rf`*number* to allow at most *number* features in the densest tile.
 * `-B` _zoom_ or `--base-zoom=`_zoom_: Base zoom, the level at and above which all points are included in the tiles (default maxzoom).
   If you use `-Bg`, it will guess a zoom level that will keep at most 50,000 features in the densest tile.
   You can also specify a marker-width with `-Bg`*width* to allow fewer features in the densest tile to
   compensate for the larger marker, or `-Bf`*number* to allow at most *number* features in the densest tile.
 * `-al` or `--drop-lines`: Let ""dot"" dropping at lower zooms apply to lines too
 * `-ap` or `--drop-polygons`: Let ""dot"" dropping at lower zooms apply to polygons too
 * `-K` _distance_ or `--cluster-distance=`_distance_: Cluster points (as with `--cluster-densest-as-needed`, but without the experimental discovery process) that are approximately within _distance_ of each other. The units are tile coordinates within a nominally 256-pixel tile, so the maximum value of 255 allows only one feature per tile. Values around 10 are probably appropriate for typical marker sizes. See `--cluster-densest-as-needed` below for behavior.

"
20,tippecanoe-README.md, Dropping a fraction of features to keep under tile size limits," * `-as` or `--drop-densest-as-needed`: If a tile is too large, try to reduce it to under 500K by increasing the minimum spacing between features. The discovered spacing applies to the entire zoom level.
 * `-ad` or `--drop-fraction-as-needed`: Dynamically drop some fraction of features from each zoom level to keep large tiles under the 500K size limit. (This is like `-pd` but applies to the entire zoom level, not to each tile.)
 * `-an` or `--drop-smallest-as-needed`: Dynamically drop the smallest features (physically smallest: the shortest lines or the smallest polygons) from each zoom level to keep large tiles under the 500K size limit. This option will not work for point features.
 * `-aN` or `--coalesce-smallest-as-needed`: Dynamically combine the smallest features (physically smallest: the shortest lines or the smallest polygons) from each zoom level into other nearby features to keep large tiles under the 500K size limit. This option will not work for point features, and will probably not help very much with LineStrings. It is mostly intended for polygons, to maintain the full original area covered by polygons while still reducing the feature count somehow. The attributes of the small polygons are *not* preserved into the combined features (except through `--accumulate-attribute`), only their geometry. Furthermore, the polygons to which nested polygons are coalesced may not necessarily be the immediately enclosing features.
 * `-aD` or `--coalesce-densest-as-needed`: Dynamically combine the densest features from each zoom level into other nearby features to keep large tiles under the 500K size limit. (Again, mostly useful for polygons.)
 * `-aS` or `--coalesce-fraction-as-needed`: Dynamically combine a fraction of features from each zoom level into other nearby features to keep large tiles under the 500K size limit. (Again, mostly useful for polygons.)
 * `-pd` or `--force-feature-limit`: Dynamically drop some fraction of features from large tiles to keep them under the 500K size limit. It will probably look ugly at the tile boundaries. (This is like `-ad` but applies to each tile individually, not to the entire zoom level.) You probably don't want to use this.
 * `-aC` or `--cluster-densest-as-needed`: If a tile is too large, try to reduce its size by increasing the minimum spacing between features, and leaving one placeholder feature from each group.  The remaining feature will be given a `""cluster"": true` attribute to indicate that it represents a cluster, a `""point_count""` attribute to indicate the number of features that were clustered into it, and a `""sqrt_point_count""` attribute to indicate the relative width of a feature to represent the cluster. If the features being clustered are points, the representative feature will be located at the average of the original points' locations; otherwise, one of the original features will be left as the representative.

"
21,tippecanoe-README.md, Dropping tightly overlapping features," * `-g` _gamma_ or `--gamma=_gamma`_: Rate at which especially dense dots are dropped (default 0, for no effect). A gamma of 2 reduces the number of dots less than a pixel apart to the square root of their original number.
 * `-aG` or `--increase-gamma-as-needed`: If a tile is too large, try to reduce it to under 500K by increasing the `-g` gamma. The discovered gamma applies to the entire zoom level. You probably want to use `--drop-densest-as-needed` instead.

"
22,tippecanoe-README.md, Line and polygon simplification," * `-S` _scale_ or `--simplification=`_scale_: Multiply the tolerance for line and polygon simplification by _scale_. The standard tolerance tries to keep
   the line or polygon within one tile unit of its proper location. You can probably go up to about 10 without too much visible difference.
 * `-ps` or `--no-line-simplification`: Don't simplify lines and polygons
 * `-pS` or `--simplify-only-low-zooms`: Don't simplify lines and polygons at maxzoom (but do simplify at lower zooms)
 * `-pn` or `--no-simplification-of-shared-nodes`: Don't simplify away nodes that appear in more than one feature or are used multiple times within the same feature, so that the intersection node will not be lost from intersecting roads. (This will not be effective if you also use `--coalesce` or `--detect-shared-borders`.)
 * `-pt` or `--no-tiny-polygon-reduction`: Don't combine the area of very small polygons into small squares that represent their combined area.

"
23,tippecanoe-README.md, Attempts to improve shared polygon boundaries," * `-ab` or `--detect-shared-borders`: In the manner of [TopoJSON](https://github.com/mbostock/topojson/wiki/Introduction), detect borders that are shared between multiple polygons and simplify them identically in each polygon. This takes more time and memory than considering each polygon individually.
 * `-aL` or `--grid-low-zooms`: At all zoom levels below _maxzoom_, snap all lines and polygons to a stairstep grid instead of allowing diagonals. You will also want to specify a tile resolution, probably `-D8`. This option provides a way to display continuous parcel, gridded, or binned data at low zooms without overwhelming the tiles with tiny polygons, since features will either get stretched out to the grid unit or lost entirely, depending on how they happened to be aligned in the original data. You probably don't want to use this.

"
24,tippecanoe-README.md, Controlling clipping to tile boundaries," * `-b` _pixels_ or `--buffer=`_pixels_: Buffer size where features are duplicated from adjacent tiles. Units are ""screen pixels""—1/256th of the tile width or height. (default 5)
 * `-pc` or `--no-clipping`: Don't clip features to the size of the tile. If a feature overlaps the tile's bounds or buffer at all, it is included completely. Be careful: this can produce very large tilesets, especially with large polygons.
 * `-pD` or `--no-duplication`: As with `--no-clipping`, each feature is included intact instead of cut to tile boundaries. In addition, it is included only in a single tile per zoom level rather than potentially in multiple copies. Clients of the tileset must check adjacent tiles (possibly some distance away) to ensure they have all features.

"
25,tippecanoe-README.md, Reordering features within each tile," * `-pi` or `--preserve-input-order`: Preserve the original input order of features as the drawing order instead of ordering geographically. (This is implemented as a restoration of the original order at the end, so that dot-dropping is still geographic, which means it also undoes `-ao`).
 * `-ac` or `--coalesce`: Coalesce consecutive features that have the same attributes. This can be useful if you have lots of small polygons with identical attributes and you would like to merge them together.
 * `-ao` or `--reorder`: Reorder features to put ones with the same attributes in sequence (instead of ones that are approximately spatially adjacent), to try to get them to coalesce. You probably want to use this if you use `--coalesce`.
 * `-ar` or `--reverse`: Try reversing the directions of lines to make them coalesce and compress better. You probably don't want to use this.
 * `-ah` or `--hilbert`: Put features in Hilbert Curve order instead of the usual Z-Order. This improves the odds that spatially adjacent features will be sequentially adjacent, and should improve density calculations and spatial coalescing. It should be the default eventually.

"
26,tippecanoe-README.md, Adding calculated attributes," * `-ag` or `--calculate-feature-density`: Add a new attribute, `tippecanoe_feature_density`, to each feature, to record how densely features are spaced in that area of the tile. You can use this attribute in the style to produce a glowing effect where points are densely packed. It can range from 0 in the sparsest areas to 255 in the densest.
 * `-ai` or `--generate-ids`: Add an `id` (a feature ID, not an attribute named `id`) to each feature that does not already have one. There is currently no guarantee that the `id` added will be stable between runs or that it will not conflict with manually-assigned feature IDs. Future versions of Tippecanoe may change the mechanism for allocating IDs.

"
27,tippecanoe-README.md, Trying to correct bad source geometry," * `-aw` or `--detect-longitude-wraparound`: Detect when consecutive points within a feature jump to the other side of the world, and try to fix the geometry.
 * `-pw` or `--use-source-polygon-winding`: Instead of respecting GeoJSON polygon ring order, use the original polygon winding in the source data to distinguish inner (clockwise) and outer (counterclockwise) polygon rings.
 * `-pW` or `--reverse-source-polygon-winding`: Instead of respecting GeoJSON polygon ring order, use the opposite of the original polygon winding in the source data to distinguish inner (counterclockwise) and outer (clockwise) polygon rings.
 * `--clip-bounding-box=`*minlon*`,`*minlat*`,`*maxlon*`,`*maxlat*: Clip all features to the specified bounding box.

"
28,tippecanoe-README.md, Setting or disabling tile size limits," * `-M` _bytes_ or `--maximum-tile-bytes=`_bytes_: Use the specified number of _bytes_ as the maximum compressed tile size instead of 500K.
 * `-O` _features_ or `--maximum-tile-features=`_features_: Use the specified number of _features_ as the maximum in a tile instead of 200,000.
 * `-pf` or `--no-feature-limit`: Don't limit tiles to 200,000 features
 * `-pk` or `--no-tile-size-limit`: Don't limit tiles to 500K bytes
 * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.
 * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.
 * `--tile-stats-attributes-limit=`*count*: Include `tilestats` information about at most *count* attributes instead of the default 1000.
 * `--tile-stats-sample-values-limit=`*count*: Calculate `tilestats` attribute statistics based on *count* values instead of the default 1000.
 * `--tile-stats-values-limit=`*count*: Report *count* unique attribute values in `tilestats` instead of the default 100.

"
29,tippecanoe-README.md, Temporary storage," * `-t` _directory_ or `--temporary-directory=`_directory_: Put the temporary files in _directory_.
   If you don't specify, it will use `/tmp`.

"
30,tippecanoe-README.md, Progress indicator," * `-q` or `--quiet`: Work quietly instead of reporting progress or warning messages
 * `-Q` or `--no-progress-indicator`: Don't report progress, but still give warnings
 * `-U` _seconds_ or `--progress-interval=`_seconds_: Don't report progress more often than the specified number of _seconds_.
 * `-v` or `--version`: Report Tippecanoe's version number

"
31,tippecanoe-README.md, Filters," * `-C` _command_ or `--prefilter=`_command_: Specify a shell filter command to be run at the start of assembling each tile
 * `-c` _command_ or `--postfilter=`_command_: Specify a shell filter command to be run at the end of assembling each tile

The pre- and post-filter commands allow you to do optional filtering or transformation on the features of each tile
as it is created. They are shell commands, run with the zoom level, X, and Y as the `$1`, `$2`, and `$3` arguments.
Future versions of Tippecanoe may add additional arguments for more context.

The features are provided to the filter
as a series of newline-delimited GeoJSON objects on the standard input, and `tippecanoe` expects to read another
set of GeoJSON features from the filter's standard output.

The prefilter receives the features at the highest available resolution, before line simplification,
polygon topology repair, gamma calculation, dynamic feature dropping, or other internal processing.
The postfilter receives the features at tile resolution, after simplification, cleaning, and dropping.

The layer name is provided as part of the `tippecanoe` element of the feature and must be passed through
to keep the feature in its correct layer. In the case of the prefilter, the `tippecanoe` element may also
contain `index`, `sequence`, `extent`, and `dropped`, elements, which must be passed through for internal operations like
`--drop-densest-as-needed`, `--drop-smallest-as-needed`, and `--preserve-input-order` to work.

"
32,tippecanoe-README.md, Examples:," * Make a tileset of the Natural Earth countries to zoom level 5, and also copy the GeoJSON features
   to files in a `tiles/z/x/y.geojson` directory hierarchy.

```
tippecanoe -o countries.mbtiles -z5 -C 'mkdir -p tiles/$1/$2; tee tiles/$1/$2/$3.geojson' ne_10m_admin_0_countries.json
```

 * Make a tileset of the Natural Earth countries to zoom level 5, but including only those tiles that
   intersect the [bounding box of Germany](https://www.flickr.com/places/info/23424829).
   (The `limit-tiles-to-bbox` script is [in the Tippecanoe source directory](filters/limit-tiles-to-bbox).)

```
tippecanoe -o countries.mbtiles -z5 -C './filters/limit-tiles-to-bbox 5.8662 47.2702 15.0421 55.0581 $*' ne_10m_admin_0_countries.json
```

 * Make a tileset of TIGER roads in Tippecanoe County, leaving out all but primary and secondary roads (as [classified by TIGER](https://www.census.gov/geo/reference/mtfcc.html)) below zoom level 11.

```
tippecanoe -o roads.mbtiles -c 'if [ $1 -lt 11 ]; then grep ""\""MTFCC\"": \""S1[12]00\""""; else cat; fi' tl_2016_18157_roads.json
```

Environment
-----------

Tippecanoe ordinarily uses as many parallel threads as the operating system claims that CPUs are available.
You can override this number by setting the `TIPPECANOE_MAX_THREADS` environmental variable.

GeoJSON extension
-----------------

Tippecanoe defines a GeoJSON extension that you can use to specify the minimum and/or maximum zoom level
at which an individual feature will be included in the vector tileset being produced.
If you have a feature like this:

```
{
    ""type"" : ""Feature"",
    ""tippecanoe"" : { ""maxzoom"" : 9, ""minzoom"" : 4 },
    ""properties"" : { ""FULLNAME"" : ""N Vasco Rd"" },
    ""geometry"" : {
        ""type"" : ""LineString"",
        ""coordinates"" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]
    }
}
```

with a `tippecanoe` object specifiying a `maxzoom` of 9 and a `minzoom` of 4, the feature
will only appear in the vector tiles for zoom levels 4 through 9. Note that the `tippecanoe`
object belongs to the Feature, not to its `properties`. If you specify a `minzoom` for a feature,
it will be preserved down to that zoom level even if dot-dropping with `-r` would otherwise have
dropped it.

You can also specify a layer name in the `tippecanoe` object, which will take precedence over
the filename or name specified using `--layer`, like this:

```
{
    ""type"" : ""Feature"",
    ""tippecanoe"" : { ""layer"" : ""streets"" },
    ""properties"" : { ""FULLNAME"" : ""N Vasco Rd"" },
    ""geometry"" : {
        ""type"" : ""LineString"",
        ""coordinates"" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]
    }
}
```

If your source GeoJSON only has `minzoom`, `maxzoom` and/or `layer` within `properties` you can use [ndjson-cli](https://github.com/mbostock/ndjson-cli/blob/master/README.md) to move them into the required `tippecanoe` object by piping the GeoJSON like this:

```sh
ndjson-map 'd.tippecanoe = { minzoom: d.properties.minzoom, maxzoom: d.properties.maxzoom, layer: d.properties.layer }, delete d.properties.minzoom, delete d.properties.maxzoom, delete d.properties.layer, d'
```

Geometric simplifications
-------------------------

At every zoom level, line and polygon features are subjected to Douglas-Peucker
simplification to the resolution of the tile.

For point features, it drops 1/2.5 of the dots for each zoom level above the
point base zoom (which is normally the same as the `-z` max zoom, but can be
a different zoom specified with `-B` if you have precise but sparse data).
I don't know why 2.5 is the appropriate number, but the densities of many different
data sets fall off at about this same rate. You can use -r to specify a different rate.

You can use the gamma option to thin out especially dense clusters of points.
For any area where dots are closer than one pixel together (at whatever zoom level),
a gamma of 3, for example, will reduce these clusters to the cube root of their original density.

For line features, it drops any features that are too small to draw at all.
This still leaves the lower zooms too dark (and too dense for the 500K tile limit,
in some places), so I need to figure out an equitable way to throw features away.

Unless you specify `--no-tiny-polygon-reduction`,
any polygons that are smaller than a minimum area (currently 4 square subpixels) will
have their probability diffused, so that some of them will be drawn as a square of
this minimum size and others will not be drawn at all, preserving the total area that
all of them should have had together.

Features in the same tile that share the same type and attributes are coalesced
together into a single geometry if you use `--coalesce`. You are strongly encouraged to use -x to exclude
any unnecessary attributes to reduce wasted file size.

If a tile is larger than 500K, it will try encoding that tile at progressively
lower resolutions before failing if it still doesn't fit.

Development
-----------

Requires sqlite3 and zlib (should already be installed on MacOS). Rebuilding the manpage
uses md2man (`gem install md2man`).

Linux:

    sudo apt-get install build-essential libsqlite3-dev zlib1g-dev

Then build:

    make

and perhaps

    make install

Tippecanoe now requires features from the 2011 C++ standard. If your compiler is older than
that, you will need to install a newer one. On MacOS, updating to the lastest XCode should
get you a new enough version of `clang++`. On Linux, you should be able to upgrade `g++` with

```
sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test
sudo apt-get update -y
sudo apt-get install -y g++-5
export CXX=g++-5
```

Docker Image
------------

A tippecanoe Docker image can be built from source and executed as a task to
automatically install dependencies and allow tippecanoe to run on any system
supported by Docker.

```docker
$ docker build -t tippecanoe:latest .
$ docker run -it --rm \
  -v /tiledata:/data \
  tippecanoe:latest \
  tippecanoe --output=/data/output.mbtiles /data/example.geojson
```

The commands above will build a Docker image from the source and compile the
latest version. The image supports all tippecanoe flags and options.

Examples
------

Check out [some examples of maps made with tippecanoe](MADE_WITH.md)

Name
----

The name is [a joking reference](http://en.wikipedia.org/wiki/Tippecanoe_and_Tyler_Too) to a ""tiler"" for making map tiles.

tile-join
=========

Tile-join is a tool for copying and merging vector mbtiles files and for
joining new attributes from a CSV file to existing features in them.

It reads the tiles from an
existing .mbtiles file or a directory of tiles, matches them against the
records of the CSV (if one is specified), and writes out a new tileset.

If you specify multiple source mbtiles files or source directories of tiles,
all the sources are read and their combined contents are written to the new
mbtiles output. If they define the same layers or the same tiles, the layers
or tiles are merged.

The options are:

"
33,tippecanoe-README.md, Output tileset," * `-o` *out.mbtiles* or `--output=`*out.mbtiles*: Write the new tiles to the specified .mbtiles file.
 * `-e` *directory* or `--output-to-directory=`*directory*: Write the new tiles to the specified directory instead of to an mbtiles file.
 * `-f` or `--force`: Remove *out.mbtiles* if it already exists.

"
34,tippecanoe-README.md, Tileset description and attribution," * `-A` *attribution* or `--attribution=`*attribution*: Set the attribution string.
 * `-n` *name* or `--name=`*name*: Set the tileset name.
 * `-N` *description* or `--description=`*description*: Set the tileset description.

"
35,tippecanoe-README.md, Layer filtering and naming," * `-l` *layer* or `--layer=`*layer*: Include the named layer in the output. You can specify multiple `-l` options to keep multiple layers. If you don't specify, they will all be retained.
 * `-L` *layer* or `--exclude-layer=`*layer*: Remove the named layer from the output. You can specify multiple `-L` options to remove multiple layers.
 * `-R`*old*`:`*new* or `--rename-layer=`*old*`:`*new*: Rename the layer named *old* to be named *new* instead. You can specify multiple `-R` options to rename multiple layers. Renaming happens before filtering.

"
36,tippecanoe-README.md, Zoom levels," * `-z` _zoom_ or `--maximum-zoom=`_zoom_: Don't copy tiles from higher zoom levels than the specified zoom
 * `-Z` _zoom_ or `--minimum-zoom=`_zoom_: Don't copy tiles from lower zoom levels than the specified zoom

"
37,tippecanoe-README.md, Merging attributes from a CSV file," * `-c` *match*`.csv` or `--csv=`*match*`.csv`: Use *match*`.csv` as the source for new attributes to join to the features. The first line of the file should be the key names; the other lines are values. The first column is the one to match against the existing features; the other columns are the new data to add.

"
38,tippecanoe-README.md, Filtering features and feature attributes," * `-x` *key* or `--exclude=`*key*: Remove attributes of type *key* from the output. You can use this to remove the field you are matching against if you no longer need it after joining, or to remove any other attributes you don't want.
 * `-X` or `--exclude-all`: Remove all attributes from the output.
 * `-i` or `--if-matched`: Only include features that matched the CSV.
 * `-j` *filter* or `--feature-filter`=*filter*: Check features against a per-layer filter (as defined in the [Mapbox GL Style Specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/#other-filter)) and only include those that match. Any features in layers that have no filter specified will be passed through. Filters for the layer `""*""` apply to all layers.
 * `-J` *filter-file* or `--feature-filter-file`=*filter-file*: Like `-j`, but read the filter from a file.
 * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.

"
39,tippecanoe-README.md, Setting or disabling tile size limits," * `-pk` or `--no-tile-size-limit`: Don't skip tiles larger than 500K.
 * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.
 * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.

Because tile-join just copies the geometries to the new .mbtiles without processing them
(except to rescale the extents if necessary),
it doesn't have any of tippecanoe's recourses if the new tiles are bigger than the 500K tile limit.
If a tile is too big and you haven't specified `-pk`, it is just left out of the new tileset.

Example
-------

Imagine you have a tileset of census blocks:

```sh
curl -O http://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_06001_tabblock10.zip
unzip tl_2010_06001_tabblock10.zip
ogr2ogr -f GeoJSON tl_2010_06001_tabblock10.json tl_2010_06001_tabblock10.shp
./tippecanoe -o tl_2010_06001_tabblock10.mbtiles tl_2010_06001_tabblock10.json
```

and a CSV of their populations:

```sh
curl -O http://www2.census.gov/census_2010/01-Redistricting_File--PL_94-171/California/ca2010.pl.zip
unzip -p ca2010.pl.zip cageo2010.pl |
awk 'BEGIN {
    print ""GEOID10,population""
}
(substr($0, 9, 3) == ""750"") {
    print ""\"""" substr($0, 28, 2) substr($0, 30, 3) substr($0, 55, 6) substr($0, 62, 4) ""\"","" (0 + substr($0, 328, 9))
}' > population.csv
```

which looks like this:

```
GEOID10,population
""060014277003018"",0
""060014283014046"",0
""060014284001020"",0
...
""060014507501001"",202
""060014507501002"",119
""060014507501003"",193
""060014507501004"",85
...
```

Then you can join those populations to the geometries and discard the no-longer-needed ID field:

```sh
./tile-join -o population.mbtiles -x GEOID10 -c population.csv tl_2010_06001_tabblock10.mbtiles
```

tippecanoe-enumerate
====================

The `tippecanoe-enumerate` utility lists the tiles that an `mbtiles` file defines.
Each line of the output lists the name of the `mbtiles` file and the zoom, x, and y
coordinates of one of the tiles. It does basically the same thing as

    select zoom_level, tile_column, (1 << zoom_level) - 1 - tile_row from tiles;

on the file in sqlite3.

tippecanoe-decode
=================

The `tippecanoe-decode` utility turns vector mbtiles back to GeoJSON. You can use it either
on an entire file:

    tippecanoe-decode file.mbtiles

or on an individual tile:

    tippecanoe-decode file.mbtiles zoom x y
    tippecanoe-decode file.vector.pbf zoom x y

Unless you use `-c`, the output is a set of nested FeatureCollections identifying each
tile and layer separately. Note that the same features generally appear at all zooms,
so the output for the file will have many copies of the same features at different
resolutions.

"
40,tippecanoe-README.md, Options," * `-s` _projection_ or `--projection=`*projection*: Specify the projection of the output data. Currently supported are EPSG:4326 (WGS84, the default) and EPSG:3857 (Web Mercator).
 * `-z` _maxzoom_ or `--maximum-zoom=`*maxzoom*: Specify the highest zoom level to decode from the tileset
 * `-Z` _minzoom_ or `--minimum-zoom=`*minzoom*: Specify the lowest zoom level to decode from the tileset
 * `-l` _layer_ or `--layer=`*layer*: Decode only layers with the specified names. (Multiple `-l` options can be specified.)
 * `-c` or `--tag-layer-and-zoom`: Include each feature's layer and zoom level as part of its `tippecanoe` object rather than as a FeatureCollection wrapper
 * `-S` or `--stats`: Just report statistics about each tile's size and the number of features in it, as a JSON structure.
 * `-f` or `--force`: Decode tiles even if polygon ring order or closure problems are detected

tippecanoe-json-tool
====================

Extracts GeoJSON features or standalone geometries as line-delimited JSON objects from a larger JSON file,
following the same extraction rules that Tippecanoe uses when parsing JSON.

    tippecanoe-json-tool file.json [... file.json]

Optionally also wraps them in a FeatureCollection or GeometryCollection as appropriate.

Optionally extracts an attribute from the GeoJSON `properties` for sorting.

Optionally joins a sorted CSV of new attributes to a sorted GeoJSON file.

The reason for requiring sorting is so that it is possible to work on CSV and GeoJSON files that are larger
than can comfortably fit in memory by streaming through them in parallel, in the same way that the Unix
`join` command does. The Unix `sort` command can be used to sort large files to prepare them for joining.

The sorting interface is weird, and future version of `tippecanoe-json-tool` will replace it with
something better.

"
41,tippecanoe-README.md, Options," * `-w` or `--wrap`: Add the FeatureCollection or GeometryCollection wrapper.
 * `-e` *attribute* or `--extract=`*attribute*: Extract the named attribute as a prefix to each feature.
   The formatting makes excessive use of `\u` quoting so that it follows JSON string rules but will still
   be sorted correctly by tools that just do ASCII comparisons.
 * `-c` *file.csv* or `--csv=`*file.csv*: Join attributes from the named sorted CSV file, using its first column as the join key. Geometries will be passed through even if they do not match the CSV; CSV lines that do not match a geometry will be discarded.
 * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.

"
42,tippecanoe-README.md, Example,"Join Census LEHD ([Longitudinal Employer-Household Dynamics](https://lehd.ces.census.gov/)) employment data to a file of Census block geography
for Tippecanoe County, Indiana.

Download Census block geometry, and convert to GeoJSON:

```
$ curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_18157_tabblock10.zip
$ unzip tl_2010_18157_tabblock10.zip
$ ogr2ogr -f GeoJSON tl_2010_18157_tabblock10.json tl_2010_18157_tabblock10.shp
```

Download Indiana employment data, and fix name of join key in header

```
$ curl -L -O https://lehd.ces.census.gov/data/lodes/LODES7/in/wac/in_wac_S000_JT00_2015.csv.gz
$ gzip -dc in_wac_S000_JT00_2015.csv.gz | sed '1s/w_geocode/GEOID10/' > in_wac_S000_JT00_2015.csv
```

Sort GeoJSON block geometry so it is ordered by block ID. If you don't do this, you will get a
""GeoJSON file is out of sort"" error.

```
$ tippecanoe-json-tool -e GEOID10 tl_2010_18157_tabblock10.json | LC_ALL=C sort > tl_2010_18157_tabblock10.sort.json
```

Join block geometries to employment attributes:

```
$ tippecanoe-json-tool -c in_wac_S000_JT00_2015.csv tl_2010_18157_tabblock10.sort.json > blocks-wac.json
```
"
0,facebookresearch-ResNeXt-README.md, ResNeXt: Aggregated Residual Transformations for Deep Neural Networks,"By [Saining Xie](http://vcl.ucsd.edu/~sxie), [Ross Girshick](http://www.rossgirshick.info/), [Piotr Dollár](https://pdollar.github.io/), [Zhuowen Tu](http://pages.ucsd.edu/~ztu/), [Kaiming He](http://kaiminghe.com)

UC San Diego, Facebook AI Research

"
1,facebookresearch-ResNeXt-README.md, Table of Contents,"0. [Introduction](#introduction)
0. [Citation](#citation)
0. [Requirements and Dependencies](#requirements-and-dependencies)
0. [Training](#training)
0. [ImageNet Pretrained Models](#imagenet-pretrained-models)
0. [Third-party re-implementations](#third-party-re-implementations)

"
2,facebookresearch-ResNeXt-README.md, News,"* Congrats to the ILSVRC 2017 classification challenge winner [WMW](http://image-net.org/challenges/LSVRC/2017/results).
ResNeXt is the foundation of their new SENet architecture (a **ResNeXt-152 (64 x 4d)** with the Squeeze-and-Excitation module)!
* Check out Figure 6 in the new [Memory-Efficient Implementation of DenseNets](https://arxiv.org/pdf/1707.06990.pdf) paper for a comparision between ResNeXts and DenseNets. <sub>（*DenseNet cosine is DenseNet trained with cosine learning rate schedule.*）</sub>
<p align=""center"">
<img src=""http://vcl.ucsd.edu/resnext/resnextvsdensenet.png"" width=""480"">
</p>


"
3,facebookresearch-ResNeXt-README.md, Introduction,"This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).

[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.


![teaser](http://vcl.ucsd.edu/resnext/teaser.png)
"
4,facebookresearch-ResNeXt-README.md," Figure: Training curves on ImageNet-1K. (Left): ResNet/ResNeXt-50 with the same complexity (~4.1 billion FLOPs, ~25 million parameters); (Right): ResNet/ResNeXt-101 with the same complexity (~7.8 billion FLOPs, ~44 million parameters).","-----

"
5,facebookresearch-ResNeXt-README.md, Citation,"If you use ResNeXt in your research, please cite the paper:
```
@article{Xie2016,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  journal={arXiv preprint arXiv:1611.05431},
  year={2016}
}
```

"
6,facebookresearch-ResNeXt-README.md, Requirements and Dependencies,"See the fb.resnet.torch [installation instructions](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md) for a step-by-step guide.
- Install [Torch](http://torch.ch/docs/getting-started.html) on a machine with CUDA GPU
- Install [cuDNN v4 or v5](https://developer.nvidia.com/cudnn) and the Torch [cuDNN bindings](https://github.com/soumith/cudnn.torch/tree/R4)
- Download the [ImageNet](http://image-net.org/download-images) dataset and [move validation images](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset) to labeled subfolders

"
7,facebookresearch-ResNeXt-README.md, Training,"Please follow [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch) for the general usage of the code, including [how](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained) to use pretrained ResNeXt models for your own task.

There are two new hyperparameters need to be specified to determine the bottleneck template:

**-baseWidth** and **-cardinality**

"
8,facebookresearch-ResNeXt-README.md, 1x Complexity Configurations Reference Table,"| baseWidth | cardinality |
|---------- | ----------- |
| 64        | 1           |
| 40        | 2           |
| 24        | 4           |
| 14        | 8           |
| 4         | 32          |


To train ResNeXt-50 (32x4d) on 8 GPUs for ImageNet:
```bash
th main.lua -dataset imagenet -bottleneckType resnext_C -depth 50 -baseWidth 4 -cardinality 32 -batchSize 256 -nGPU 8 -nThreads 8 -shareGradInput true -data [imagenet-folder]
```

To reproduce CIFAR results (e.g. ResNeXt 16x64d for cifar10) on 8 GPUs:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 128 -nGPU 8 -nThreads 8 -shareGradInput true
```
To get comparable results using 2/4 GPUs, you should change the batch size and the corresponding learning rate:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 64 -nGPU 4 -LR 0.05 -nThreads 8 -shareGradInput true
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 32 -nGPU 2 -LR 0.025 -nThreads 8 -shareGradInput true
```
Note: CIFAR datasets will be automatically downloaded and processed for the first time. Note that in the arXiv paper CIFAR results are based on pre-activated bottleneck blocks and a batch size of 256. We found that better CIFAR test acurracy can be achieved using original bottleneck blocks and a batch size of 128.

"
9,facebookresearch-ResNeXt-README.md, ImageNet Pretrained Models,"ImageNet pretrained models are licensed under CC BY-NC 4.0.

[![CC BY-NC 4.0](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc/4.0/)

"
10,facebookresearch-ResNeXt-README.md, Single-crop (224x224) validation error rate,"| Network             | GFLOPS | Top-1 Error |  Download   |
| ------------------- | ------ | ----------- | ------------|
| ResNet-50 (1x64d)   |  ~4.1  |  23.9        | [Original ResNet-50](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)       |
| ResNeXt-50 (32x4d)  |  ~4.1  |  22.2        | [Download (191MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_50_32x4d.t7)       |
| ResNet-101 (1x64d)  |  ~7.8  |  22.0        | [Original ResNet-101](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)      |
| ResNeXt-101 (32x4d) |  ~7.8  |  21.2        | [Download (338MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_32x4d.t7)      |
| ResNeXt-101 (64x4d) |  ~15.6 |  20.4        | [Download (638MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_64x4d.t7)       |

"
11,facebookresearch-ResNeXt-README.md, Third-party re-implementations,"Besides our torch implementation, we recommend to see also the following third-party re-implementations and extensions:

1. Training code in PyTorch [code](https://github.com/prlz77/ResNeXt.pytorch)
1. Converting ImageNet pretrained model to PyTorch model and source. [code](https://github.com/clcarwin/convert_torch_to_pytorch)
1. Training code in MXNet and pretrained ImageNet models [code](https://github.com/dmlc/mxnet/tree/master/example/image-classification#imagenet-1k)
1. Caffe prototxt, pretrained ImageNet models (with ResNeXt-152), curves [code](https://github.com/cypw/ResNeXt-1)[code](https://github.com/terrychenism/ResNeXt)
"
1,mapshaper-README.md, Introduction,"Mapshaper is software for editing Shapefile, GeoJSON, [TopoJSON](https://github.com/mbostock/topojson/wiki), CSV and several other data formats, written in JavaScript.

The `mapshaper` command line program supports essential map making tasks like simplifying shapes, editing attribute data, clipping, erasing, dissolving, filtering and more.

The web UI supports interactive simplification, attribute data editing, and running cli commands in a built-in console. Visit the public website at [www.mapshaper.org](http://www.mapshaper.org) or use the web UI locally via the `mapshaper-gui` script.

See the [project wiki](https://github.com/mbloch/mapshaper/wiki) for more documentation on how to use mapshaper.

To suggest improvements, add an [issue](https://github.com/mbloch/mapshaper/issues).

To learn about recent updates, read the [changelog](https://github.com/mbloch/mapshaper/releases).

"
2,mapshaper-README.md, Command line tool,"The `mapshaper` command line program has been used successfully under Mac OS X, Linux and Windows.

The project wiki has an [introduction](https://github.com/mbloch/mapshaper/wiki/Introduction-to-the-Command-Line-Tool) to using the command line tool that includes many simple examples.

For a detailed reference, see the [Command Reference](https://github.com/mbloch/mapshaper/wiki/Command-Reference).



"
3,mapshaper-README.md, Interactive web interface,"The web UI works in recent desktop versions of Chrome, Firefox, Safari and Internet Explorer. Safari before v10.1 and IE before v10 are not supported.

The mapshaper distribution includes the script `mapshaper-gui`, which runs mapshaper's web interface locally. You can also visit [mapshaper.org](http://www.mapshaper.org) to use mapshaper online.

All processing is done in the browser, so your data stays private, even when using the public website.

"
4,mapshaper-README.md, Large file support,"**Web interface**

Firefox is able to load Shapefiles and GeoJSON files larger than 1GB. Chrome has improved in recent versions, but is still prone to out-of-memory errors when importing files larger than several hundred megabytes.

**Command line interface**

When working with very large files, mapshaper may become unresponsive or crash with the message ""JavaScript heap out of memory.""

One option is to run `mapshaper-xl` (added in v0.4.63), which allocates more memory than the standard `mapshaper` program.

Another solution is to run Node directly with the `--max-old-space-size` option. The following example (Mac or Linux) allocates 8GB of memory:
```bash
$ node  --max-old-space-size=8192 `which mapshaper` <mapshaper commands>
```

#:#:#: Installation

Mapshaper requires [Node.js](http://nodejs.org).

With Node installed, you can install the latest release version of mapshaper using npm. Install with the ""-g"" flag to make the executable scripts available systemwide.

```bash
npm install -g mapshaper
```

To install and run the latest development code from github:

```bash
git clone git@github.com:mbloch/mapshaper.git
cd mapshaper
npm install
bin/mapshaper     "
5,mapshaper-README.md, run the command line program,bin/mapshaper-gui 
6,mapshaper-README.md, use the web UI locally,"```

"
7,mapshaper-README.md, Building and testing,"Run the `build` script to build both the cli and web UI modules.

Run `npm test` in the project directory to run mapshaper's tests.

"
8,mapshaper-README.md, License,"This software is licensed under [MPL 2.0](http://www.mozilla.org/MPL/2.0/).

According to Mozilla's [FAQ](http://www.mozilla.org/MPL/2.0/FAQ.html), ""The MPL's ‘file-level’ copyleft is designed to encourage contributors to share modifications they make to your code, while still allowing them to combine your code with code under other licenses (open or proprietary) with minimal restrictions.""



"
9,mapshaper-README.md, Acknowledgements,"My colleagues at The New York Times, for countless suggestions, bug reports and general helpfulness.

Mark Harrower, for collaborating on the original ""MapShaper"" program at the University of Wisconsin&ndash;Madison.
"
0,reduxjs-react-redux-README.md, Installation,"React Redux requires **React 16.8.3 or later.**

```
npm install --save react-redux
```

This assumes that you’re using [npm](http://npmjs.com/) package manager 
with a module bundler like [Webpack](https://webpack.js.org/) or 
[Browserify](http://browserify.org/) to consume [CommonJS 
modules](https://webpack.js.org/api/module-methods/#commonjs).

If you don’t yet use [npm](http://npmjs.com/) or a modern module bundler, and would rather prefer a single-file [UMD](https://github.com/umdjs/umd) build that makes `ReactRedux` available as a global object, you can grab a pre-built version from [cdnjs](https://cdnjs.com/libraries/react-redux). We *don’t* recommend this approach for any serious application, as most of the libraries complementary to Redux are only available on [npm](http://npmjs.com/).

"
1,reduxjs-react-redux-README.md, React Native,"As of React Native 0.18, React Redux 5.x should work with React Native. If you have any issues with React Redux 5.x on React Native, run `npm ls react` and make sure you don’t have a duplicate React installation in your `node_modules`. We recommend that you use `npm@3.x` which is better at avoiding these kinds of issues.


"
2,reduxjs-react-redux-README.md, Documentation,"The React Redux docs are now published at **https://react-redux.js.org** .

We're currently expanding and rewriting our docs content - check back soon for more updates!

"
3,reduxjs-react-redux-README.md, How Does It Work?,"We do a deep dive on how React Redux works in [this readthesource episode](https://www.youtube.com/watch?v=VJ38wSFbM3A).  

Also, the post [The History and Implementation of React-Redux](https://blog.isquaredsoftware.com/2018/11/react-redux-history-implementation/) 
explains what it does, how it works, and how the API and implementation have evolved over time.

Enjoy!

"
4,reduxjs-react-redux-README.md, License,"MIT
"
0,pysal-README.md,"Python Spatial Analysis Library
","[![image](https://travis-ci.org/pysal/pysal.svg)](https://travis-ci.org/pysal)

[![image](https://coveralls.io/repos/pysal/pysal/badge.svg?branch=master)](https://coveralls.io/r/pysal/pysal?branch=master)

[![image](https://badges.gitter.im/pysal/pysal.svg)](https://gitter.im/pysal/pysal)

[![image](https://readthedocs.org/projects/pip/badge/?version=latest)](http://pysal.readthedocs.io/en/latest/index.html)

[![LISA Maps of US County Homicide Rates](https://farm2.staticflickr.com/1699/23937788493_1b9d147b9f_z.jpg)](http://nbviewer.ipython.org/urls/gist.githubusercontent.com/darribas/657e0568df7a63362762/raw/pysal_lisa_maps.ipynb)

*Above: Local Indicators of Spatial Association for Homicide Rates in US
Counties 1990.*

PySAL, the Python spatial analysis library, is an open source
cross-platform library for geospatial data science with an emphasis on
geospatial vector data written in Python. It supports the development of
high level applications for spatial analysis, such as

-   detection of spatial clusters, hot-spots, and outliers
-   construction of graphs from spatial data
-   spatial regression and statistical modeling on geographically
    embedded networks
-   spatial econometrics
-   exploratory spatio-temporal data analysis

"
1,pysal-README.md,"PySAL Components
","-   **explore** - modules to conduct exploratory analysis of spatial and spatio-temporal data, including statistical testing on points, networks, and
    polygonal lattices.  Also includes methods for spatial inequality and distributional dynamics.
-   **viz** - visualize patterns in spatial data to detect clusters,
    outliers, and hot-spots.
-   **model** - model spatial relationships in data with a variety of
    linear, generalized-linear, generalized-additive, and nonlinear
    models.
-   **lib** - solve a wide variety of computational geometry problems:
    -   graph construction from polygonal lattices, lines, and points.
    -   construction and interactive editing of spatial weights matrices
        & graphs
    -   computation of alpha shapes, spatial indices, and
        spatial-topological relationships
    -   reading and writing of sparse graph data, as well as pure python
        readers of spatial vector data.

"
2,pysal-README.md,"Installation
","PySAL is available through
[Anaconda](https://www.continuum.io/downloads) (in the defaults or
conda-forge channel) and [Enthought
Canopy](https://www.enthought.com/products/canopy/). We recommend
installing PySAL from conda-forge:

``` {.sourceCode .bash}
conda install pysal
```

PySAL can be installed using pip:

``` {.sourceCode .bash}
pip install pysal
```

As of version 2.0.0 PySAL has shifted to Python 3 only.

Users who need an older stable version of PySAL that is Python 2
compatible can install version 1.14.3 through pip or conda:

``` {.sourceCode .bash}
conda install pysal==1.14.3
```

"
3,pysal-README.md,"Documentation
","For help on using PySAL, check out the following resources:

-   [User
    Guide](https://pysal.readthedocs.io/en/latest/)
-   [Tutorials and Short
    Courses](https://github.com/pysal/notebooks)

"
4,pysal-README.md,"Development
","As of version 2.0.0, PySAL is now a collection of affiliated geographic data
science packages. Changes to the code for any of the subpackages should be
directed at the respective [upstream
repositories](http://github.com/pysal/help), and not made here. Infrastructural
changes for the meta-package, like those for tooling, building the package, and
code standards, will be considered.

Development is hosted on [github](https://github.com/pysal/pysal).

Discussions of development as well as help for users occurs on the
[developer list](http://groups.google.com/group/pysal-dev) as well as
[gitter](https://gitter.im/pysal/pysal?).

"
5,pysal-README.md,"Getting Involved
","If you are interested in contributing to PySAL please see our
[development guidelines](https://github.com/pysal/pysal/wiki).

"
6,pysal-README.md,"Bug reports
","To search for or report bugs, please see PySAL\'s
[issues](http://github.com/pysal/pysal/issues).

"
7,pysal-README.md,"License information
","See the file \""LICENSE.txt\"" for information on the history of this
software, terms & conditions for usage, and a DISCLAIMER OF ALL
WARRANTIES.
"
1,vid2vid-README.md, [Project](https://tcwang0509.github.io/vid2vid/) | [YouTube(short)](https://youtu.be/5zlcXTCpQqM) | [YouTube(full)](https://youtu.be/GrP_aOSXt5U) | [arXiv](https://arxiv.org/abs/1808.06601) | [Paper(full)](https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf),"Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). <br><br>
[Video-to-Video Synthesis](https://tcwang0509.github.io/vid2vid/)  
 [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyuliu.net/)<sup>1</sup>, [Jun-Yan Zhu](http://people.csail.mit.edu/junyanz/)<sup>2</sup>, [Guilin Liu](https://liuguilin1225.github.io/)<sup>1</sup>, Andrew Tao<sup>1</sup>, [Jan Kautz](http://jankautz.com/)<sup>1</sup>, [Bryan Catanzaro](http://catanzaro.name/)<sup>1</sup>  
 <sup>1</sup>NVIDIA Corporation, <sup>2</sup>MIT CSAIL  
 In Neural Information Processing Systems (**NeurIPS**) 2018  

"
2,vid2vid-README.md, Video-to-Video Translation,"- Label-to-Streetview Results
<p align='center'>  
  <img src='imgs/city_change_styles.gif' width='440'/>  
  <img src='imgs/city_change_labels.gif' width='440'/>
</p>

- Edge-to-Face Results
<p align='center'>
  <img src='imgs/face.gif' width='440'/>
  <img src='imgs/face_multiple.gif' width='440'/>
</p>

- Pose-to-Body Results
<p align='center'>
  <img src='imgs/pose.gif' width='550'/>
</p>

- Frame Prediction Results
<p align='center'>
  <img src='imgs/framePredict.gif' width='550'/>
</p>

"
3,vid2vid-README.md, Prerequisites,"- Linux or macOS
- Python 3
- NVIDIA GPU + CUDA cuDNN
- PyTorch 0.4


"
5,vid2vid-README.md, Installation,"- Install python libraries [dominate](https://github.com/Knio/dominate) and requests.
```bash
pip install dominate requests
```
- If you plan to train with face datasets, please install dlib.
```bash
pip install dlib
```
- If you plan to train with pose datasets, please install [DensePose](https://github.com/facebookresearch/DensePose) and/or [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).
- Clone this repo:
```bash
git clone https://github.com/NVIDIA/vid2vid
cd vid2vid
```
- Docker Image
If you have difficulty building the repo, a docker image can be found in the `docker` folder.

"
6,vid2vid-README.md, Testing ,"- Please first download example dataset by running `python scripts/download_datasets.py`.
- Next, download and compile a snapshot of [FlowNet2](https://github.com/NVIDIA/flownet2-pytorch) by running `python scripts/download_flownet2.py`.
- Cityscapes    
  - Please download the pre-trained Cityscapes model by:
    ```bash
    python scripts/street/download_models.py
    ```
  - To test the model (`bash ./scripts/street/test_2048.sh`):
    ```bash
    #:!./scripts/street/test_2048.sh
    python test.py --name label2city_2048 --label_nc 35 --loadSize 2048 --n_scales_spatial 3 --use_instance --fg --use_single_G
    ```
    The test results will be saved in: `./results/label2city_2048/test_latest/`.

  - We also provide a smaller model trained with single GPU, which produces slightly worse performance at 1024 x 512 resolution.
    - Please download the model by
    ```bash
    python scripts/street/download_models_g1.py
    ```
    - To test the model (`bash ./scripts/street/test_g1_1024.sh`):
    ```bash
    #:!./scripts/street/test_g1_1024.sh
    python test.py --name label2city_1024_g1 --label_nc 35 --loadSize 1024 --n_scales_spatial 3 --use_instance --fg --n_downsample_G 2 --use_single_G
    ```
  - You can find more example scripts in the `scripts/street/` directory.

- Faces
  - Please download the pre-trained model by:
    ```bash
    python scripts/face/download_models.py
    ```
  - To test the model (`bash ./scripts/face/test_512.sh`):
    ```bash
    #:!./scripts/face/test_512.sh
    python test.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --use_single_G
    ```
    The test results will be saved in: `./results/edge2face_512/test_latest/`.

"
7,vid2vid-README.md, Dataset,"- Cityscapes
  - We use the Cityscapes dataset as an example. To train a model on the full dataset, please download it from the [official website](https://www.cityscapes-dataset.com/) (registration required).
  - We apply a pre-trained segmentation algorithm to get the corresponding semantic maps (train_A) and instance maps (train_inst).
  - Please add the obtained images to the `datasets` folder in the same way the example images are provided.
- Face
  - We use the [FaceForensics](http://niessnerlab.org/projects/roessler2018faceforensics.html) dataset. We then use landmark detection to estimate the face keypoints, and interpolate them to get face edges.
- Pose
  - We use random dancing videos found on YouTube. We then apply DensePose / OpenPose to estimate the poses for each frame.

"
8,vid2vid-README.md, Training with Cityscapes dataset,"- First, download the FlowNet2 checkpoint file by running `python scripts/download_models_flownet2.py`.
- Training with 8 GPUs:
  - We adopt a coarse-to-fine approach, sequentially increasing the resolution from 512 x 256, 1024 x 512, to 2048 x 1024.
  - Train a model at 512 x 256 resolution (`bash ./scripts/street/train_512.sh`)
  ```bash
  #:!./scripts/street/train_512.sh
  python train.py --name label2city_512 --label_nc 35 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 6 --use_instance --fg
  ```
  - Train a model at 1024 x 512 resolution (must train 512 x 256 first) (`bash ./scripts/street/train_1024.sh`):
  ```bash
  #:!./scripts/street/train_1024.sh
  python train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512
  ```
If you have TensorFlow installed, you can see TensorBoard logs in `./checkpoints/label2city_1024/logs` by adding `--tf_log` to the training scripts.

- Training with a single GPU:
  - We trained our models using multiple GPUs. For convenience, we provide some sample training scripts (train_g1_XXX.sh) for single GPU users, up to 1024 x 512 resolution. Again a coarse-to-fine approach is adopted (256 x 128, 512 x 256, 1024 x 512). Performance is not guaranteed using these scripts.
  - For example, to train a 256 x 128 video with a single GPU (`bash ./scripts/street/train_g1_256.sh`)
  ```bash
  #:!./scripts/street/train_g1_256.sh
  python train.py --name label2city_256_g1 --label_nc 35 --loadSize 256 --use_instance --fg --n_downsample_G 2 --num_D 1 --max_frames_per_gpu 6 --n_frames_total 6
  ```

- Training at full (2k x 1k) resolution
  - To train the images at full resolution (2048 x 1024) requires 8 GPUs with at least 24G memory (`bash ./scripts/street/train_2048.sh`). If only GPUs with 12G/16G memory are available, please use the script `./scripts/street/train_2048_crop.sh`, which will crop the images during training. Performance is not guaranteed with this script.

"
9,vid2vid-README.md, Training with face datasets,"- If you haven't, please first download example dataset by running `python scripts/download_datasets.py`.
- Run the following command to compute face landmarks for training dataset: 
  ```bash
  python data/face_landmark_detection.py train
  ```
- Run the example script (`bash ./scripts/face/train_512.sh`)
  ```bash
  python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12  
  ```
- For single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing `max_frames_per_gpu`.
- More examples scripts can be found in `scripts/face/`.
- Please refer to [More Training/Test Details](https://github.com/NVIDIA/vid2vid#more-trainingtest-details) for more explanations about training flags.


"
10,vid2vid-README.md, Training with pose datasets,"- If you haven't, please first download example dataset by running `python scripts/download_datasets.py`.
- Example DensePose and OpenPose results are included. If you plan to use your own dataset, please generate these results and put them in the same way the example dataset is provided.
- Run the example script (`bash ./scripts/pose/train_256p.sh`)
  ```bash
  python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0,1,2,3,4,5,6,7 --batchSize 8 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4
  ```
- Again, for single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing `max_frames_per_gpu`.
- More examples scripts can be found in `scripts/pose/`.
- Please refer to [More Training/Test Details](https://github.com/NVIDIA/vid2vid#more-trainingtest-details) for more explanations about training flags.

"
11,vid2vid-README.md, Training with your own dataset,"- If your input is a label map, please generate label maps which are one-channel whose pixel values correspond to the object labels (i.e. 0,1,...,N-1, where N is the number of labels). This is because we need to generate one-hot vectors from the label maps. Please use `--label_nc N` during both training and testing.
- If your input is not a label map, please specify `--input_nc N` where N is the number of input channels (The default is 3 for RGB images).
- The default setting for preprocessing is `scaleWidth`, which will scale the width of all training images to `opt.loadSize` (1024) while keeping the aspect ratio. If you want a different setting, please change it by using the `--resize_or_crop` option. For example, `scaleWidth_and_crop` first resizes the image to have width `opt.loadSize` and then does random cropping of size `(opt.fineSize, opt.fineSize)`. `crop` skips the resizing step and only performs random cropping. `scaledCrop` crops the image while retraining the original aspect ratio. `randomScaleHeight` will randomly scale the image height to be between `opt.loadSize` and `opt.fineSize`. If you don't want any preprocessing, please specify `none`, which will do nothing other than making sure the image is divisible by 32.

"
12,vid2vid-README.md, More Training/Test Details,"- We generate frames in the video sequentially, where the generation of the current frame depends on previous frames. To generate the first frame for the model, there are 3 different ways:  
  - 1. Using another generator which was trained on generating single images (e.g., pix2pixHD) by specifying `--use_single_G`. This is the option we use in the test scripts.
  - 2. Using the first frame in the real sequence by specifying `--use_real_img`. 
  - 3. Forcing the model to also synthesize the first frame by specifying `--no_first_img`. This must be trained separately before inference.
- The way we train the model is as follows: suppose we have 8 GPUs, 4 for generators and 4 for discriminators, and we want to train 28 frames. Also, assume each GPU can generate only one frame. The first GPU generates the first frame, and pass it to the next GPU, and so on. After the 4 frames are generated, they are passed to the 4 discriminator GPUs to compute the losses. Then the last generated frame becomes input to the next batch, and the next 4 frames in the training sequence are loaded into GPUs. This is repeated 7 times (4 x 7 = 28), to train all the 28 frames.
- Some important flags:
  - `n_gpus_gen`: the number of GPUs to use for generators (while the others are used for discriminators). We separate generators and discriminators into different GPUs since when dealing with high resolutions, even one frame cannot fit in a GPU. If the number is set to `-1`, there is no separation and all GPUs are used for both generators and discriminators (only works for low-res images).
  - `n_frames_G`: the number of input frames to feed into the generator network; i.e., `n_frames_G - 1` is the number of frames we look into the past. the default is 3 (conditioned on previous two frames).
  - `n_frames_D`: the number of frames to feed into the temporal discriminator. The default is 3.
  - `n_scales_spatial`: the number of scales in the spatial domain. We train from the coarsest scale and all the way to the finest scale. The default is 3.
  - `n_scales_temporal`: the number of scales for the temporal discriminator. The finest scale takes in the sequence in the original frame rate. The coarser scales subsample the frames by a factor of `n_frames_D` before feeding the frames into the discriminator. For example, if `n_frames_D = 3` and `n_scales_temporal = 3`, the discriminator effectively sees 27 frames. The default is 3.
  - `max_frames_per_gpu`: the number of frames in one GPU during training. If you run into out of memory error, please first try to reduce this number. If your GPU memory can fit more frames, try to make this number bigger to make training faster. The default is 1.
  - `max_frames_backpropagate`: the number of frames that loss backpropagates to previous frames. For example, if this number is 4, the loss on frame n will backpropagate to frame n-3. Increasing this number will slightly improve the performance, but also cause training to be less stable. The default is 1.
  - `n_frames_total`: the total number of frames in a sequence we want to train with. We gradually increase this number during training.
  - `niter_step`: for how many epochs do we double `n_frames_total`. The default is 5.  
  - `niter_fix_global`: if this number if not 0, only train the finest spatial scale for this number of epochs before starting to fine-tune all scales.
  - `batchSize`: the number of sequences to train at a time. We normally set batchSize to 1 since often, one sequence is enough to occupy all GPUs. If you want to do batchSize > 1, currently only `batchSize == n_gpus_gen` is supported.
  - `no_first_img`: if not specified, the model will assume the first frame is given and synthesize the successive frames. If specified, the model will also try to synthesize the first frame instead.
  - `fg`: if specified, use the foreground-background separation model as stated in the paper. The foreground labels must be specified by `--fg_labels`.
  - `no_flow`: if specified, do not use flow warping and directly synthesize frames. We found this usually still works reasonably well when the background is static, while saving memory and training time.
  - `sparse_D`: if specified, only apply temporal discriminator on sparse frames in the sequence. This helps save memory while having little effect on performance.
- For other flags, please see `options/train_options.py` and `options/base_options.py` for all the training flags; see `options/test_options.py` and `options/base_options.py` for all the test flags.

- Additional flags for edge2face examples:
  - `no_canny_edge`: do not use canny edges for background as input.
  - `no_dist_map`: by default, we use distrance transform on the face edge map as input. This flag will make it directly use edge maps.

- Additional flags for pose2body examples:
  - `densepose_only`: use only densepose results as input. Please also remember to change `input_nc` to be 3.
  - `openpose_only`: use only openpose results as input. Please also remember to change `input_nc` to be 3.
  - `add_face_disc`: add an additional discriminator that only works on the face region.
  - `remove_face_labels`: remove densepose results for face, and add noise to openpose face results, so the network can get more robust to different face shapes. This is important if you plan to do inference on half-body videos (if not, usually this flag is unnecessary).
  - `random_drop_prob`: the probability to randomly drop each pose segment during training, so the network can get more robust to missing poses at inference time. Default is 0.05.
  - `basic_point_only`: if specified, only use basic joint keypoints for OpenPose output, without using any hand or face keypoints.

"
13,vid2vid-README.md, Citation,"If you find this useful for your research, please cite the following paper.

```
@inproceedings{wang2018vid2vid,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {Video-to-Video Synthesis},
   booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},   
   year      = {2018},
}
```

"
14,vid2vid-README.md, Acknowledgments,"We thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the segmentation maps for us. We also thank Lisa Rhee for allowing us to use her dance videos for training. We thank William S. Peebles for proofreading the paper.</br>
This code borrows heavily from [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) and [pix2pixHD](https://github.com/NVIDIA/pix2pixHD).
"
0,scikit-learn-scikit-learn-README.md,"scikit-learn
","scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.

The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the `About us <http://scikit-learn.org/dev/about.html#authors>`_ page
for a list of core contributors.

It is currently maintained by a team of volunteers.

Website: http://scikit-learn.org


"
1,scikit-learn-scikit-learn-README.md,"Installation
","Dependencies
~~~~~~~~~~~~

scikit-learn requires:

- Python (>= 3.5)
- NumPy (>= 1.11.0)
- SciPy (>= 0.17.0)
- joblib (>= 0.11)

**Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.**
scikit-learn 0.21 and later require Python 3.5 or newer.

Scikit-learn plotting capabilities (i.e., functions start with ""plot_"") require
Matplotlib (>= 1.5.1). For running the examples Matplotlib >= 1.5.1 is
required. A few examples require scikit-image >= 0.12.3, a few examples require
pandas >= 0.18.0.

User installation
~~~~~~~~~~~~~~~~~

If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using ``pip``   ::

    pip install -U scikit-learn

or ``conda``::

    conda install scikit-learn

The documentation includes more detailed `installation instructions <http://scikit-learn.org/stable/install.html>`_.


"
2,scikit-learn-scikit-learn-README.md,"Changelog
","See the `changelog <http://scikit-learn.org/dev/whats_new.html>`__
for a history of notable changes to scikit-learn.

"
3,scikit-learn-scikit-learn-README.md,"Development
","We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
`Development Guide <http://scikit-learn.org/stable/developers/index.html>`_
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.

Important links
~~~~~~~~~~~~~~~

- Official source code repo: https://github.com/scikit-learn/scikit-learn
- Download releases: https://pypi.org/project/scikit-learn/
- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues

Source code
~~~~~~~~~~~

You can check the latest sources with the command::

    git clone https://github.com/scikit-learn/scikit-learn.git

Contributing
~~~~~~~~~~~~

To learn more about making a contribution to scikit-learn, please see our
`Contributing guide
<https://scikit-learn.org/dev/developers/contributing.html>`_.

Testing
~~~~~~~

After installation, you can launch the test suite from outside the
source directory (you will need to have ``pytest`` >= 3.3.0 installed)::

    pytest sklearn

See the web page http://scikit-learn.org/dev/developers/advanced_installation.html#testing
for more information.

    Random number generation can be controlled during testing by setting
    the ``SKLEARN_SEED`` environment variable.

Submitting a Pull Request
~~~~~~~~~~~~~~~~~~~~~~~~~

Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: http://scikit-learn.org/stable/developers/index.html


"
4,scikit-learn-scikit-learn-README.md,"Project History
","The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the  `About us <http://scikit-learn.org/dev/about.html#authors>`_ page
for a list of core contributors.

The project is currently maintained by a team of volunteers.

**Note**: `scikit-learn` was previously referred to as `scikits.learn`.


"
5,scikit-learn-scikit-learn-README.md,"Help and Support
","Documentation
~~~~~~~~~~~~~

- HTML documentation (stable release): http://scikit-learn.org
- HTML documentation (development version): http://scikit-learn.org/dev/
- FAQ: http://scikit-learn.org/stable/faq.html

Communication
~~~~~~~~~~~~~

- Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn
- IRC channel: ``#scikit-learn`` at ``webchat.freenode.net``
- Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn
- Website: http://scikit-learn.org

Citation
~~~~~~~~

If you use scikit-learn in a scientific publication, we would appreciate citations: http://scikit-learn.org/stable/about.html#citing-scikit-learn
"
0,mplstereonet-README.md,mplstereonet,"``mplstereonet`` provides lower-hemisphere equal-area and equal-angle stereonets
for matplotlib.

.. image:: http://joferkington.github.com/mplstereonet/images/equal_area_equal_angle_comparison.png
    :alt: Comparison of equal angle and equal area stereonets.
    :align: center
    :target: https://github.com/joferkington/mplstereonet/blob/master/examples/equal_area_equal_angle_comparison.py


"
1,mplstereonet-README.md,"Install
","``mplstereonet`` can be installed from PyPi using ``pip`` by::

    pip install mplstereonet

Alternatively, you can download the source and install locally using (from the
main directory of the repository)::

    python setup.py install

If you're planning on developing ``mplstereonet`` or would like to experiment
with making local changes, consider setting up a development installation so
that your changes are reflected when you import the package::

    python setup.py develop

"
2,mplstereonet-README.md,Basic Usage,"In most cases, you'll want to ``import mplstereonet`` and then make an axes
with ``projection=""stereonet""`` (By default, this is an equal-area stereonet).
Alternately, you can use ``mplstereonet.subplots``, which functions identically
to ``matplotlib.pyplot.subplots``, but creates stereonet axes.

As an example::

    import matplotlib.pyplot as plt
    import mplstereonet

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='stereonet')

    strike, dip = 315, 30
    ax.plane(strike, dip, 'g-', linewidth=2)
    ax.pole(strike, dip, 'g^', markersize=18)
    ax.rake(strike, dip, -25)
    ax.grid()

    plt.show()

.. image:: http://joferkington.github.com/mplstereonet/images/basic.png
    :alt: A basic stereonet with a plane, pole to the plane, and rake along the plane
    :align: center
    :target: https://github.com/joferkington/mplstereonet/blob/master/examples/basic.py
    
Planes, lines, poles, and rakes can be plotted using axes methods (e.g.
``ax.line(plunge, bearing)`` or ``ax.rake(strike, dip, rake_angle)``).

All planar measurements are expected to follow the right-hand-rule to indicate
dip direction. As an example, 315/30S would be 135/30 following the right-hand
rule.

"
3,mplstereonet-README.md,Density Contouring,"``mplstereonet`` also provides a few different methods of producing contoured
orientation density diagrams.

The ``ax.density_contour`` and ``ax.density_contourf`` axes methods provide density
contour lines and filled density contours, respectively.  ""Raw"" density grids
can be produced with the ``mplstereonet.density_grid`` function.

As a basic example::

    import matplotlib.pyplot as plt
    import numpy as np
    import mplstereonet
    
    fig, ax = mplstereonet.subplots()
    
    strike, dip = 90, 80
    num = 10
    strikes = strike + 10 * np.random.randn(num)
    dips = dip + 10 * np.random.randn(num)
    
    cax = ax.density_contourf(strikes, dips, measurement='poles')
                              
    ax.pole(strikes, dips)
    ax.grid(True)
    fig.colorbar(cax)
    
    plt.show()

.. image:: http://joferkington.github.com/mplstereonet/images/contouring.png
    :alt: Orientation density contours.
    :align: center
    :target: https://github.com/joferkington/mplstereonet/blob/master/examples/contouring.py


By default, a modified Kamb method with exponential smoothing [Vollmer1995]_ is
used to estimate the orientation density distribution. Other methods (such as
the ""traditional"" Kamb [Kamb1956]_ and ""Schmidt"" (a.k.a. 1%) methods) are
available as well. The method and expected count (in standard deviations) can
be controlled by the ``method`` and ``sigma`` keyword arguments, respectively.

.. image:: http://joferkington.github.com/mplstereonet/images/contour_angelier_data.png
    :alt: Orientation density contours.
    :align: center
    :target: https://github.com/joferkington/mplstereonet/blob/master/examples/contour_angelier_data.py

"
4,mplstereonet-README.md,Utilities,"``mplstereonet`` also includes a number of utilities to parse structural
measurements in either quadrant or azimuth form such that they follow the
right-hand-rule. 

For an example, see parsing_example.py_::

    Parse quadrant azimuth measurements
    ""N30E"" --> 30.0
    ""E30N"" --> 60.0
    ""W10S"" --> 260.0
    ""N 10 W"" --> 350.0
    
    Parse quadrant strike/dip measurements.
    Note that the output follows the right-hand-rule.
    ""215/10"" --> Strike: 215.0, Dip: 10.0
    ""215/10E"" --> Strike: 35.0, Dip: 10.0
    ""215/10NW"" --> Strike: 215.0, Dip: 10.0
    ""N30E/45NW"" --> Strike: 210.0, Dip: 45.0
    ""E10N   20 N"" --> Strike: 260.0, Dip: 20.0
    ""W30N/46.7 S"" --> Strike: 120.0, Dip: 46.7
    
    Similarly, you can parse rake measurements that don't follow the RHR.
    ""N30E/45NW 10NE"" --> Strike: 210.0, Dip: 45.0, Rake: 170.0
    ""210 45 30N"" --> Strike: 210.0, Dip: 45.0, Rake: 150.0
    ""N30E/45NW raking 10SW"" --> Strike: 210.0, Dip: 45.0, Rake: 10.0

Additionally, you can find plane intersections and make other calculations by
combining utility functions.  See plane_intersection.py_ and
parse_anglier_data.py_ for examples.

"
5,mplstereonet-README.md,"References
",".. [Kamb1956] Kamb, 1959. Ice Petrofabric Observations from Blue Glacier,
       Washington, in Relation to Theory and Experiment. Journal of
       Geophysical Research, Vol. 64, No. 11, pp. 1891--1909.

.. [Vollmer1995] Vollmer, 1995. C Program for Automatic Contouring of Spherical
       Orientation Data Using a Modified Kamb Method. Computers &
       Geosciences, Vol. 21, No. 1, pp. 31--49.

.. _parsing_example.py: https://github.com/joferkington/mplstereonet/blob/master/examples/parsing_example.py

.. _plane_intersection.py: https://github.com/joferkington/mplstereonet/blob/master/examples/plane_intersection.py

.. _parse_anglier_data.py: https://github.com/joferkington/mplstereonet/blob/master/examples/parse_angelier_data.py
"
0,gdal-docker-README.md, GDAL Docker Images,"NB: As of GDAL version 1.11.2 the image has been renamed from `homme/gdal` to
`geodata/gdal`.

This is an Ubuntu derived image containing the Geospatial Data Abstraction
Library (GDAL) compiled with a broad range of drivers. The build process is
based on that defined in the
[GDAL TravisCI tests](https://github.com/OSGeo/gdal/blob/trunk/.travis.yml).

Each branch in the git repository corresponds to a supported GDAL version
(e.g. `1.11.2`) with the master branch following GDAL master. These branch names
are reflected in the image tags on the Docker Index (e.g. branch `1.11.2`
corresponds to the image `geodata/gdal:1.11.2`).

"
1,gdal-docker-README.md, Usage,"Running the container without any arguments will by default output the GDAL
version string as well as the supported raster and vector formats:

    docker run geodata/gdal

The following command will open a bash shell in an Ubuntu based environment
with GDAL available:

    docker run -t -i geodata/gdal /bin/bash

You will most likely want to work with data on the host system from within the
docker container, in which case run the container with the -v option. Assuming
you have a raster called `test.tif` in your current working directory on your
host system, running the following command should invoke `gdalinfo` on
`test.tif`:

    docker run -v $(pwd):/data geodata/gdal gdalinfo test.tif

This works because the current working directory is set to `/data` in the
container, and you have mapped the current working directory on your host to
`/data`.

Note that the image tagged `latest`, GDAL represents the latest code *at the
time the image was built*. If you want to include the most up-to-date commits
then you need to build the docker image yourself locally along these lines:

    docker build -t geodata/gdal:local git://github.com/geo-data/gdal-docker/
"
0,RDN-README.md, Residual Dense Network for Image Super-Resolution,"This repository is for RDN introduced in the following paper

[Yulun Zhang](http://yulunzhang.com/), [Yapeng Tian](http://yapengtian.org/), [Yu Kong](http://www1.ece.neu.edu/~yukong/), [Bineng Zhong](https://scholar.google.de/citations?user=hvRBydsAAAAJ&hl=en), and [Yun Fu](http://www1.ece.neu.edu/~yunfu/), ""Residual Dense Network for Image Super-Resolution"", CVPR 2018 (spotlight), [[arXiv]](https://arxiv.org/abs/1802.08797) 

[Yulun Zhang](http://yulunzhang.com/), [Yapeng Tian](http://yapengtian.org/), [Yu Kong](http://www1.ece.neu.edu/~yukong/), [Bineng Zhong](https://scholar.google.de/citations?user=hvRBydsAAAAJ&hl=en), and [Yun Fu](http://www1.ece.neu.edu/~yunfu/), ""Residual Dense Network for Image Restoration"", arXiv 2018, [[arXiv]](https://arxiv.org/abs/1812.10477) 


The code is built on [EDSR (Torch)](https://github.com/LimBee/NTIRE2017) and tested on Ubuntu 14.04 environment (Torch7, CUDA8.0, cuDNN5.1) with Titan X/1080Ti/Xp GPUs.

Other implementations: [PyTorch_version](https://github.com/thstkdgus35/EDSR-PyTorch) has been implemented by Nguyễn Trần Toàn (trantoan060689@gmail.com) and merged into [EDSR_PyTorch](https://github.com/thstkdgus35/EDSR-PyTorch). [TensorFlow_version](https://github.com/hengchuan/RDN-TensorFlow) by hengchuan.

"
1,RDN-README.md, Contents,"1. [Introduction](#introduction)
2. [Train](#train)
3. [Test](#test)
4. [Results](#results)
5. [Citation](#citation)
6. [Acknowledgements](#acknowledgements)

"
2,RDN-README.md, Introduction,"A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Speciﬁcally, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.

![RDB](/Figs/RDB.png)
Figure 1. Residual dense block (RDB) architecture.
![RDN](/Figs/RDN.png)
Figure 2. The architecture of our proposed residual dense network (RDN).

"
4,RDN-README.md, Prepare training data ,"1. Download DIV2K training data (800 training + 100 validtion images) from [DIV2K dataset](https://data.vision.ee.ethz.ch/cvl/DIV2K/) or [SNU_CVLab](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar).

2. Place all the HR images in 'Prepare_TrainData/DIV2K/DIV2K_HR'.

3. Run 'Prepare_TrainData_HR_LR_BI/BD/DN.m' in matlab to generate LR images for BI, BD, and DN models respectively.

4. Run 'th png_to_t7.lua' to convert each .png image to .t7 file in new folder 'DIV2K_decoded'.

5. Specify the path of 'DIV2K_decoded' to '-datadir' in 'RDN_TrainCode/code/opts.lua'.

For more informaiton, please refer to [EDSR(Torch)](https://github.com/LimBee/NTIRE2017).

"
5,RDN-README.md, Begin to train,"1. (optional) Download models for our paper and place them in '/RDN_TrainCode/experiment/model'.

    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).

2. Cd to 'RDN_TrainCode/code', run the following scripts to train models.

    **You can use scripts in file 'TrainRDN_scripts' to train models for our paper.**

    ```bash
    #: BI, scale 2, 3, 4
    #: BIX2F64D18C6G64P48, input=48x48, output=96x96
    th main.lua -scale 2 -netType RDN -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true

    #: BIX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX2.t7
    th main.lua -scale 3 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true  -preTrained ../experiment/model/RDN_BIX2.t7

    #: BIX4F64D18C6G64P32, input=32x32, output=128x128, fine-tune on RDN_BIX2.t7
    th main.lua -scale 4 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 128 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true -nEpochs 1000 -preTrained ../experiment/model/RDN_BIX2.t7 

    #: BD, scale 3
    #: BDX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7
    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BD -splitBatch 4 -trainOnly true -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7

    #: DN, scale 3
    #: DNX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7
    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel DN -splitBatch 4 -trainOnly true  -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7
    ```
    Only RDN_BIX2.t7 was trained using 48x48 input patches. All other models were trained using 32x32 input patches in order to save training time.
    However, smaller input patch size in training would lower the performance to some degree. We also set '-trainOnly true' to save GPU memory.
"
7,RDN-README.md, Quick start,"1. Download models for our paper and place them in '/RDN_TestCode/model'.

    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).

2. Run 'TestRDN.lua'

    **You can use scripts in file 'TestRDN_scripts' to produce results for our paper.**

    ```bash
    #: No self-ensemble: RDN
    #: BI degradation model, X2, X3, X4
    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble false -dataset Set5
    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble false -dataset Set5
    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble false -dataset Set5
    #: BD degradation model, X3
    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble false -dataset Set5
    #: DN degradation model, X3
    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble false -dataset Set5


    #: With self-ensemble: RDN+
    #: BI degradation model, X2, X3, X4
    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble true -dataset Set5
    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble true -dataset Set5
    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble true -dataset Set5
    #: BD degradation model, X3
    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble true -dataset Set5
    #: DN degradation model, X3
    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble true -dataset Set5
    ```

"
8,RDN-README.md, The whole test pipeline,"1. Prepare test data.

    Place the original test sets (e.g., Set5, other test sets are available from [GoogleDrive](https://drive.google.com/drive/folders/1xyiuTr6ga6ni-yfTP7kyPHRmfBakWovo?usp=sharing) or [Baidu](https://pan.baidu.com/s/1yBI_-rknXT2lm1UAAB_bag)) in 'OriginalTestData'.

    Run 'Prepare_TestData_HR_LR.m' in Matlab to generate HR/LR images with different degradation models.
2. Conduct image SR. 

    See **Quick start**
3. Evaluate the results.

    Run 'Evaluate_PSNR_SSIM.m' to obtain PSNR/SSIM values for paper.



"
9,RDN-README.md, Results,"![PSNR_SSIM_BI](/Figs/PSNR_SSIM_BI.png)
Table 1. Benchmark results with BI degradation model. Average PSNR/SSIM values for scaling factor ×2, ×3, and ×4.

![PSNR_SSIM_BD_DN](/Figs/PSNR_SSIM_BD_DN.png)
Table 2. Benchmark results with BD and DN degradation models. Average PSNR/SSIM values for scaling factor ×3.

"
10,RDN-README.md, Citation,"If you find the code helpful in your resarch or work, please cite the following papers.
```
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}

@inproceedings{zhang2018residual,
    title={Residual Dense Network for Image Super-Resolution},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={CVPR},
    year={2018}
}

@article{zhang2018rdnir,
    title={Residual Dense Network for Image Restoration},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={arXiv},
    year={2018}
}

```
"
11,RDN-README.md, Acknowledgements,"This code is built on [EDSR (Torch)](https://github.com/LimBee/NTIRE2017). We thank the authors for sharing their codes of EDSR [Torch version](https://github.com/LimBee/NTIRE2017) and [PyTorch version](https://github.com/thstkdgus35/EDSR-PyTorch).
"
0,pyansys-README.md, create a square area using keypoints,"    ansys.Prep7()
    ansys.K(1, 0, 0, 0)
    ansys.K(2, 1, 0, 0)
    ansys.K(3, 1, 1, 0)
    ansys.K(4, 0, 1, 0)    
    ansys.L(1, 2)
    ansys.L(2, 3)
    ansys.L(3, 4)
    ansys.L(4, 1)
    ansys.Al(1, 2, 3, 4)
    ansys.Aplot()
    ansys.Save()
    ansys.Exit()

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/aplot.png
    :width: 500pt


Loading and Plotting an ANSYS Archive File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ANSYS archive files containing solid elements (both legacy and current), can be loaded using Archive and then converted to a vtk object.


.. code:: python

    import pyansys
    from pyansys import examples
    
    "
1,pyansys-README.md, Sample *.cdb,"    filename = examples.hexarchivefile
    
    "
2,pyansys-README.md, Read ansys archive file,"    archive = pyansys.Archive(filename)
    
    "
3,pyansys-README.md, Print raw data from cdb,"    for key in archive.raw:
       print(""%s : %s"" % (key, archive.raw[key]))
    
    "
4,pyansys-README.md, Create a vtk unstructured grid from the raw data and plot it,"    grid = archive.parse_vtk()
    grid.plot()
    
    "
5,pyansys-README.md, write this as a vtk xml file ,"    grid.Write('hex.vtu')

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/hexbeam.png
    :width: 500pt

You can then load this vtk file using ``pyvista`` or another program that uses VTK.
    
.. code:: python

    "
6,pyansys-README.md, Load this from vtk,"    import pyvista as pv
    grid = pv.UnstructuredGrid('hex.vtu')
    grid.plot()


Loading the Result File
~~~~~~~~~~~~~~~~~~~~~~~
This example reads in binary results from a modal analysis of a beam from ANSYS.

.. code:: python

    "
7,pyansys-README.md, Load the reader from pyansys,"    import pyansys
    from pyansys import examples
    
    "
8,pyansys-README.md, Sample result file,"    rstfile = examples.rstfile
    
    "
9,pyansys-README.md, Create result object by loading the result file,"    result = pyansys.read_binary(rstfile)
    
    "
10,pyansys-README.md, Beam natural frequencies,"    freqs = result.time_values

.. code:: python

    >>> print(freq)
    [ 7366.49503969  7366.49503969 11504.89523664 17285.70459456
      17285.70459457 20137.19299035]
    
    "
11,pyansys-README.md, Get the 1st bending mode shape.  Results are ordered based on the sorted ,    
12,pyansys-README.md, node numbering.  Note that results are zero indexed,"    nnum, disp = result.nodal_solution(0)
    
.. code:: python

    >>> print(disp)
    [[ 2.89623914e+01 -2.82480489e+01 -3.09226692e-01]
     [ 2.89489249e+01 -2.82342416e+01  2.47536161e+01]
     [ 2.89177130e+01 -2.82745126e+01  6.05151053e+00]
     [ 2.88715048e+01 -2.82764960e+01  1.22913304e+01]
     [ 2.89221536e+01 -2.82479511e+01  1.84965333e+01]
     [ 2.89623914e+01 -2.82480489e+01  3.09226692e-01]
     ...


Plotting Nodal Results
~~~~~~~~~~~~~~~~~~~~~~
As the geometry of the model is contained within the result file, you can plot the result without having to load any additional geometry.  Below, displacement for the first mode of the modal analysis beam is plotted using ``VTK``.

.. code:: python
    
    "
13,pyansys-README.md, Plot the displacement of Mode 0 in the x direction,"    result.plot_nodal_solution(0, 'x', label='Displacement')


.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/hexbeam_disp.png
    :width: 500pt


Results can be plotted non-interactively and screenshots saved by setting up the camera and saving the result.  This can help with the visualization and post-processing of a batch result.

First, get the camera position from an interactive plot:

.. code:: python

    >>> cpos = result.plot_nodal_solution(0)
    >>> print(cpos)
    [(5.2722879880979345, 4.308737919176047, 10.467694436036483),
     (0.5, 0.5, 2.5),
     (-0.2565529433509593, 0.9227952809887077, -0.28745339908049733)]

Then generate the plot:

.. code:: python

    result.plot_nodal_solution(0, 'x', label='Displacement', cpos=cpos,
                             screenshot='hexbeam_disp.png',
                             window_size=[800, 600], interactive=False)

Stress can be plotted as well using the below code.  The nodal stress is computed in the same manner that ANSYS uses by to determine the stress at each node by averaging the stress evaluated at that node for all attached elements.  For now, only component stresses can be displayed.

.. code:: python
    
    "
14,pyansys-README.md, Display node averaged stress in x direction for result 6,"    result.plot_nodal_stress(5, 'Sx')

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/beam_stress.png
    :width: 500pt


Nodal stress can also be generated non-interactively with:

.. code:: python

    result.plot_nodal_stress(5, 'Sx', cpos=cpos, screenshot=beam_stress.png,
                           window_size=[800, 600], interactive=False)


Animating a Modal Solution
~~~~~~~~~~~~~~~~~~~~~~~~~~
Mode shapes from a modal analsyis can be animated using ``animate_nodal_solution``:

.. code:: python

    result.animate_nodal_solution(0)

If you wish to save the animation to a file, specify the movie_filename and animate it with:

.. code:: python

    result.animate_nodal_solution(0, movie_filename='/tmp/movie.mp4', cpos=cpos)

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/beam_mode_shape.gif
    :width: 500pt


Reading a Full File
-------------------
This example reads in the mass and stiffness matrices associated with the above example.

.. code:: python

    "
15,pyansys-README.md, Load the reader from pyansys,"    import pyansys
    from scipy import sparse
    
    "
16,pyansys-README.md, load the full file,"    fobj = pyansys.FullReader('file.full')
    dofref, k, m = fobj.load_km()  "
17,pyansys-README.md, returns upper triangle only,    
18,pyansys-README.md," make k, m full, symmetric matricies","    k += sparse.triu(k, 1).T
    m += sparse.triu(m, 1).T

If you have ``scipy`` installed, you can solve the eigensystem for its natural frequencies and mode shapes.

.. code:: python

    from scipy.sparse import linalg

    "
19,pyansys-README.md, condition the k matrix,    
20,pyansys-README.md," to avoid getting the ""Factor is exactly singular"" error","    k += sparse.diags(np.random.random(k.shape[0])/1E20, shape=k.shape)

    "
21,pyansys-README.md, Solve,"    w, v = linalg.eigsh(k, k=20, M=m, sigma=10000)
    "
22,pyansys-README.md, System natural frequencies,"    f = (np.real(w))**0.5/(2*np.pi)
    
    print('First four natural frequencies')
    for i in range(4):
        print '{:.3f} Hz'.format(f[i])
    
.. code::

    First four natural frequencies
    1283.200 Hz
    1283.200 Hz
    5781.975 Hz
    6919.399 Hz


License and Acknowledgments
---------------------------
``pyansys`` is licensed under the MIT license.

ANSYS documentation and functions build from html provided by `Sharcnet <https://www.sharcnet.ca/Software/Ansys/>`_.  Thanks!

This module, ``pyansys`` makes no commercial claim over ANSYS whatsoever.  This tool extends the functionality of ``ANSYS`` by adding a python interface in both file interface as well as interactive scripting without changing the core behavior or license of the original software.  The use of the interactive APDL control of ``pyansys`` requires a legally licensed local copy of ANSYS.
"
0,rasterio-README.md, Read raster bands directly to Numpy arrays.,"    #
    with rasterio.open('tests/data/RGB.byte.tif') as src:
        r, g, b = src.read()

    "
1,rasterio-README.md, Combine arrays in place. Expecting that the sum will,    
2,rasterio-README.md," temporarily exceed the 8-bit integer range, initialize it as",    
3,rasterio-README.md, a 64-bit float (the numpy default) array. Adding other,    
4,rasterio-README.md," arrays to it in-place converts those arrays ""up"" and",    
5,rasterio-README.md, preserves the type of the total array.,"    total = np.zeros(r.shape)
    for band in r, g, b:
        total += band
    total /= 3

    "
6,rasterio-README.md, Write the product as a raster band to a new 8-bit file. For,    
7,rasterio-README.md," the new file's profile, we start with the meta attributes of",    
8,rasterio-README.md," the source file, but then change the band count to 1, set the",    
9,rasterio-README.md," dtype to uint8, and specify LZW compression.","    profile = src.profile
    profile.update(dtype=rasterio.uint8, count=1, compress='lzw')

    with rasterio.open('example-total.tif', 'w', **profile) as dst:
        dst.write(total.astype(rasterio.uint8), 1)

The output:

.. image:: http://farm6.staticflickr.com/5501/11393054644_74f54484d9_z_d.jpg
   :width: 640
   :height: 581

API Overview
============

Rasterio gives access to properties of a geospatial raster file.

.. code-block:: python

    with rasterio.open('tests/data/RGB.byte.tif') as src:
        print(src.width, src.height)
        print(src.crs)
        print(src.transform)
        print(src.count)
        print(src.indexes)

    "
10,rasterio-README.md, Printed:,    
11,rasterio-README.md," (791, 718)",    
12,rasterio-README.md," {u'units': u'm', u'no_defs': True, u'ellps': u'WGS84', u'proj': u'utm', u'zone': 18}",    
13,rasterio-README.md," Affine(300.0379266750948, 0.0, 101985.0,",    
14,rasterio-README.md,"        0.0, -300.041782729805, 2826915.0)",    
15,rasterio-README.md, 3,    
16,rasterio-README.md," [1, 2, 3]","A rasterio dataset also provides methods for getting extended array slices given
georeferenced coordinates.


.. code-block:: python

    with rasterio.open('tests/data/RGB.byte.tif') as src:
        print src.window(**src.window_bounds(((100, 200), (100, 200))))

    "
17,rasterio-README.md, Printed:,    
18,rasterio-README.md," ((100, 200), (100, 200))","Rasterio CLI
============

Rasterio's command line interface, named ""rio"", is documented at `cli.rst
<https://github.com/mapbox/rasterio/blob/master/docs/cli.rst>`__. Its ``rio
insp`` command opens the hood of any raster dataset so you can poke around
using Python.

.. code-block:: pycon

    $ rio insp tests/data/RGB.byte.tif
    Rasterio 0.10 Interactive Inspector (Python 3.4.1)
    Type ""src.meta"", ""src.read(1)"", or ""help(src)"" for more information.
    >>> src.name
    'tests/data/RGB.byte.tif'
    >>> src.closed
    False
    >>> src.shape
    (718, 791)
    >>> src.crs
    {'init': 'epsg:32618'}
    >>> b, g, r = src.read()
    >>> b
    masked_array(data =
     [[-- -- -- ..., -- -- --]
     [-- -- -- ..., -- -- --]
     [-- -- -- ..., -- -- --]
     ...,
     [-- -- -- ..., -- -- --]
     [-- -- -- ..., -- -- --]
     [-- -- -- ..., -- -- --]],
                 mask =
     [[ True  True  True ...,  True  True  True]
     [ True  True  True ...,  True  True  True]
     [ True  True  True ...,  True  True  True]
     ...,
     [ True  True  True ...,  True  True  True]
     [ True  True  True ...,  True  True  True]
     [ True  True  True ...,  True  True  True]],
           fill_value = 0)

    >>> np.nanmin(b), np.nanmax(b), np.nanmean(b)
    (0, 255, 29.94772668847656)

Rio Plugins
-----------

Rio provides the ability to create subcommands using plugins.  See
`cli.rst <https://github.com/mapbox/rasterio/blob/master/docs/cli.rst#rio-plugins>`__
for more information on building plugins.

See the
`plugin registry <https://github.com/mapbox/rasterio/wiki/Rio-plugin-registry>`__
for a list of available plugins.


Installation
============

Please install Rasterio in a `virtual environment
<https://www.python.org/dev/peps/pep-0405/>`__ so that its requirements don't
tamper with your system's Python.

SSL certs
---------

The Linux wheels on PyPI are built on CentOS and libcurl expects certs to be in
/etc/pki/tls/certs/ca-bundle.crt. Ubuntu's certs, for example, are in
a different location. You may need to use the CURL_CA_BUNDLE environment
variable to specify the location of SSL certs on your computer. On an Ubuntu
system set the variable as shown below.

.. code-block:: console

    $ export CURL_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt


Dependencies
------------

Rasterio has a C library dependency: GDAL >=1.11. GDAL itself depends on some other libraries provided by most major operating systems and also
depends on the non standard GEOS and PROJ4 libraries. How to meet these
requirement will be explained below.

Rasterio's Python dependencies are listed in its requirements.txt file.

Development also requires (see requirements-dev.txt) Cython and other packages.

Binary Distributions
--------------------

Use a binary distributions that directly or indirectly provide GDAL if
possible.

Linux
+++++

Rasterio distributions are available from UbuntuGIS and Anaconda's conda-forge
channel.

`Manylinux1 <https://github.com/pypa/manylinux>`__ wheels are available on PyPI.

OS X
++++

Binary distributions with GDAL, GEOS, and PROJ4 libraries included are available
for OS X versions 10.7+ starting with Rasterio version 0.17. To install,
run ``pip install rasterio``. These binary wheels are preferred by newer
versions of pip.

If you don't want these wheels and want to install from a source distribution,
run ``pip install rasterio --no-binary rasterio`` instead.

The included GDAL library is fairly minimal, providing only the format drivers
that ship with GDAL and are enabled by default. To get access to more formats,
you must build from a source distribution (see below).

Windows
+++++++

Binary wheels for rasterio and GDAL are created by Christoph Gohlke and are
available from his website.

To install rasterio, simply download both binaries for your system (`rasterio
<http://www.lfd.uci.edu/~gohlke/pythonlibs/#rasterio>`__ and `GDAL
<http://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal>`__) and run something like
this from the downloads folder:

.. code-block:: console

    $ pip install -U pip
    $ pip install GDAL-2.0.2-cp27-none-win32.whl
    $ pip install rasterio-0.34.0-cp27-cp27m-win32.whl

You can also install rasterio with conda using Anaconda's conda-forge channel.

.. code-block:: console

    $ conda install -c conda-forge rasterio 


Source Distributions
--------------------

Rasterio is a Python C extension and to build you'll need a working compiler
(XCode on OS X etc). You'll also need Numpy preinstalled; the Numpy headers are
required to run the rasterio setup script. Numpy has to be installed (via the
indicated requirements file) before rasterio can be installed. See rasterio's
Travis `configuration
<https://github.com/mapbox/rasterio/blob/master/.travis.yml>`__ for more
guidance.

Linux
+++++

The following commands are adapted from Rasterio's Travis-CI configuration.

.. code-block:: console

    $ sudo add-apt-repository ppa:ubuntugis/ppa
    $ sudo apt-get update
    $ sudo apt-get install gdal-bin libgdal-dev
    $ pip install -U pip
    $ pip install rasterio

Adapt them as necessary for your Linux system.

OS X
++++

For a Homebrew based Python environment, do the following.

.. code-block:: console

    $ brew update
    $ brew install gdal
    $ pip install -U pip
    $ pip install --no-use-wheel rasterio

Alternatively, you can install GDAL binaries from `kyngchaos
<http://www.kyngchaos.com/software/frameworks#gdal_complete>`__.  You will then
need to add the installed location ``/Library/Frameworks/GDAL.framework/Programs``
to your system path.

Windows
+++++++

You can download a binary distribution of GDAL from `here
<http://www.gisinternals.com/release.php>`__.  You will also need to download
the compiled libraries and headers (include files).

When building from source on Windows, it is important to know that setup.py
cannot rely on gdal-config, which is only present on UNIX systems, to discover
the locations of header files and libraries that rasterio needs to compile its
C extensions. On Windows, these paths need to be provided by the user. You
will need to find the include files and the library files for gdal and use
setup.py as follows.

.. code-block:: console

    $ python setup.py build_ext -I<path to gdal include files> -lgdal_i -L<path to gdal library>
    $ python setup.py install

We have had success compiling code using the same version of Microsoft's
Visual Studio used to compile the targeted version of Python (more info on
versions used `here
<https://docs.python.org/devguide/setup.html#windows>`__.).

Note: The GDAL dll (gdal111.dll) and gdal-data directory need to be in your
Windows PATH otherwise rasterio will fail to work.


Support
=======

The primary forum for questions about installation and usage of Rasterio is
https://rasterio.groups.io/g/main. The authors and other users will answer
questions when they have expertise to share and time to explain. Please take
the time to craft a clear question and be patient about responses.

Please do not bring these questions to Rasterio's issue tracker, which we want
to reserve for bug reports and other actionable issues.

While Rasterio's repo is in the Mapbox GitHub organization, Mapbox's Support
team is focused on customer support for its commercial platform and Rasterio
support requests may be perfunctorily closed with or without a link to
https://rasterio.groups.io/g/main. It's better to bring questions directly to
the main Rasterio group at groups.io.

Development and Testing
=======================

See `CONTRIBUTING.rst <CONTRIBUTING.rst/>`__.

Documentation
=============

See `docs/ <docs/>`__.

License
=======

See `LICENSE.txt <LICENSE.txt>`__.

Authors
=======

See `AUTHORS.txt <AUTHORS.txt>`__.

Changes
=======

See `CHANGES.txt <CHANGES.txt>`__.

Who is Using Rasterio?
======================

See `here <https://libraries.io/pypi/rasterio/usage>`__.
"
0,tilelive-mapnik-README.md, tilelive-mapnik,"Renderer backend for [tilelive.js](http://github.com/mapbox/tilelive.js) that
uses [node-mapnik](http://github.com/mapnik/node-mapnik) to render tiles and
grids from a Mapnik XML file. `tilelive-mapnik` implements the
[Tilesource API](https://github.com/mapbox/tilelive.js/blob/master/API.md).

[![Build Status](https://secure.travis-ci.org/mapbox/tilelive-mapnik.png)](http://travis-ci.org/mapbox/tilelive-mapnik)
[![Build status](https://ci.appveyor.com/api/projects/status/6am7la0hiaei8qop)](https://ci.appveyor.com/project/Mapbox/tilelive-mapnik)

"
1,tilelive-mapnik-README.md, Installation,"    npm install tilelive-mapnik

Though `tilelive` is not a dependency of `tilelive-mapnik` you will want to
install it to actually make use of `tilelive-mapnik` through a reasonable
API.


"
2,tilelive-mapnik-README.md, Usage,"```javascript
var tilelive = require('tilelive');
require('tilelive-mapnik').registerProtocols(tilelive);

tilelive.load('mapnik:///path/to/file.xml', function(err, source) {
    if (err) throw err;

    // Interface is in XYZ/Google coordinates.
    // Use `y = (1 << z) - 1 - y` to flip TMS coordinates.
    source.getTile(0, 0, 0, function(err, tile, headers) {
        // `err` is an error object when generation failed, otherwise null.
        // `tile` contains the compressed image file as a Buffer
        // `headers` is a hash with HTTP headers for the image.
    });

    // The `.getGrid` is implemented accordingly.
});
```

Note that grid generation will only work when there's metadata inside a
`<Parameters>` object in the Mapnik XML.

The key fields are `interactivity_layer` and `interactivity_fields`. See an
[example in the tests](https://github.com/mapbox/tilelive-mapnik/blob/4e9cbf8347eba7c3c2b7e8fd4270ea39f9cc7af5/test/data/test.xml#L6-L7). These `Parameters` are normally added by the application that creates the XML,
in this case [CartoCSS](https://github.com/mapbox/carto/blob/55fbafe0d0e8ec00515c5782a3664c15502f0437/lib/carto/renderer.js#L152-L189)
"
1,neural-motifs-README.md," Like this work, or scene understanding in general? You might be interested in checking out my brand new dataset VCR: Visual Commonsense Reasoning, at [visualcommonsense.com](https://visualcommonsense.com)!","This repository contains data and code for the paper [Neural Motifs: Scene Graph Parsing with Global Context (CVPR 2018)](https://arxiv.org/abs/1711.06640v2) For the project page (as well as links to the baseline checkpoints), check out [rowanzellers.com/neuralmotifs](https://rowanzellers.com/neuralmotifs). If the paper significantly inspires you, we request that you cite our work:

"
2,neural-motifs-README.md, Bibtex,"```
@inproceedings{zellers2018scenegraphs,
  title={Neural Motifs: Scene Graph Parsing with Global Context},
  author={Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle = ""Conference on Computer Vision and Pattern Recognition"",  
  year={2018}
}
```
"
3,neural-motifs-README.md, Setup,"0. Install python3.6 and pytorch 3. I recommend the [Anaconda distribution](https://repo.continuum.io/archive/). To install PyTorch if you haven't already, use
 ```conda install pytorch=0.3.0 torchvision=0.2.0 cuda90 -c pytorch```.
 
1. Update the config file with the dataset paths. Specifically:
    - Visual Genome (the VG_100K folder, image_data.json, VG-SGG.h5, and VG-SGG-dicts.json). See data/stanford_filtered/README.md for the steps I used to download these.
    - You'll also need to fix your PYTHONPATH: ```export PYTHONPATH=/home/rowan/code/scene-graph``` 

2. Compile everything. run ```make``` in the main directory: this compiles the Bilinear Interpolation operation for the RoIs as well as the Highway LSTM.

3. Pretrain VG detection. The old version involved pretraining COCO as well, but we got rid of that for simplicity. Run ./scripts/pretrain_detector.sh
Note: You might have to modify the learning rate and batch size, particularly if you don't have 3 Titan X GPUs (which is what I used). [You can also download the pretrained detector checkpoint here.](https://drive.google.com/open?id=11zKRr2OF5oclFL47kjFYBOxScotQzArX)

4. Train VG scene graph classification: run ./scripts/train_models_sgcls.sh 2 (will run on GPU 2). OR, download the MotifNet-cls checkpoint here: [Motifnet-SGCls/PredCls](https://drive.google.com/open?id=12qziGKYjFD3LAnoy4zDT3bcg5QLC0qN6).
5. Refine for detection: run ./scripts/refine_for_detection.sh 2 or download the [Motifnet-SGDet](https://drive.google.com/open?id=1thd_5uSamJQaXAPVGVOUZGAOfGCYZYmb) checkpoint.
6. Evaluate: Refer to the scripts ./scripts/eval_models_sg[cls/det].sh.

"
4,neural-motifs-README.md, help,"Feel free to open an issue if you encounter trouble getting it to work!
"
0,map-vectorizer-README.md, An NYPL Labs project,"**Author:** Mauricio Giraldo Arteaga [@mgiraldo] / NYPL Labs [@nypl_labs]

Additional contributor: Thomas Levine [@thomaslevine]

A open-source map vectorizer. Provided **as is** by [NYPL Labs](http://www.nypl.org/collections/labs). Project based on a workflow suggested by Michael Resig.

The output of this process can be verified by volunteers with the [Building Inspector](https://github.com/nypl/building-inspector). 

[A paper](map-poly-paper.pdf) on this process was [published in the MapInteract '13 Proceedings of the 1st ACM SIGSPATIAL International Workshop on MapInteraction](http://dl.acm.org/citation.cfm?id=2534932&CFID=966849878&CFTOKEN=93984154) where it won the Best Paper Award.

"
1,map-vectorizer-README.md, Like OCR for maps,"This project aims to automate the manual process of geographic polygon and attribute data extraction from maps (i.e. georectified images) including those from [insurance atlases](http://digitalcollections.nypl.org/search/index?filters%5Btitle_uuid_s%5D%5B%5D=Maps%20of%20the%20city%20of%20New%20York%7C%7C323e4180-c603-012f-0c9f-58d385a7bc34&keywords=&layout=false#/?scroll=24) published in the 19th and early 20th centuries. [Here is some background](http://www.nypl.org/blog/2012/06/13/nyc-historical-gis-project) on why we're doing this and [here is one of the maps](http://digitalcollections.nypl.org/items/510d47e0-c7cc-a3d9-e040-e00a18064a99) we're extracting polygons from. This [example map layer](http://maps.nypl.org/warper/layers/859) shows what these atlases look like once geo-rectified, i.e. geographically normalized.

[The New York Public Library](http://www.nypl.org) has **hundreds of atlases** with **tens of thousands** of these sheets and there is no way we can extract data manually in a reasonable amount of time.

Just so you get an idea, it took NYPL staff coordinating a small army of volunteers **three years to produce** 170,000 polygons with attributes (from just four of hundreds of atlases at NYPL).

It now takes a period of time closer to **24 hours** to generate a comparable number of polygons with some basic metadata.

The goal is to extract the following data (✔ = mostly solved so far, ✢ = in progress):

* ✔ shape
* ✔ color
* ✢ dot presence
* ✢ dot count
* ✢ dot type (full vs outline)
* skylights
* numbers (not optimistic, but maybe **one of you** knows how extract numbers from these images)

"
2,map-vectorizer-README.md, Example input,"![Example input map](https://raw.github.com/NYPL/map-vectorizer/master/example_input.png)

"
3,map-vectorizer-README.md, Example output,"![The resulting shapefile output superimposed](https://raw.github.com/NYPL/map-vectorizer/master/example_output.png)

"
4,map-vectorizer-README.md, Extra feature detection,"![Extra feature detection for the polygon](https://raw.github.com/NYPL/map-vectorizer/master/feature_detection.png)

"
5,map-vectorizer-README.md, Dependencies,"A few things to be installed in your system in order to work properly. So far it has been **tested on Mac OS X Lion** so these instructions apply to that configuration only. I am sure you will be able to adapt it to your current configuration.

* [Python] with [OpenCV] and [PIL] 
    * If you use [PIP](https://pypi.python.org/pypi) (recommended) you will get the necessary Python packages with: `pip install -r requirements.txt`
* [R] - Make sure it is in your PATH (so you can run it via command-line by typing `R`).
* You'll need the following R packages. On OS X simply navigate to `Packages & Data`, choose your CRAN mirror region, then search for and install:
    * `alphahull` (you will need `tripack`, `sgeostat`, `splancs` as dependencies)
    * `igraph`
    * `shapefiles`
    * `rgdal` (download the [binary for your OS](http://cran.r-project.org/web/packages/rgdal/index.html) then run `R CMD INSTALL --configure-args="""" path/to/rgdal.tar.gz`)
    * You can also install the requirements by running this in the R CLI (by typing `R` in a terminal window):

```
    install.packages('rgdal')
    install.packages('alphahull')
    install.packages('igraph')
    install.packages('shapefiles')
```

* Test that everything in R is installed, on the CLI you should be able to run this with no errors:

```
    library(rgdal)
    library(alphahull)
    library(igraph)
    library(shapefiles)
    quit() #: this will quit R
```

* [GIMP]
* [GDAL Tools], on OS X try [version 1.9](http://www.kyngchaos.com/files/software/frameworks/GDAL_Complete-1.9.dmg). Per [MapBox](https://www.mapbox.com/tilemill/docs/guides/gdal/): The first time you install the GDAL package there is one additional step to make sure you can access these programs. In Mac OS, Open the Terminal application and run the following commands:

```
    echo 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH' >> ~/.bash_profile
    source ~/.bash_profile
```

* It is also a good idea to install [QGIS] to test your results

"
6,map-vectorizer-README.md, First run,"These step by step instructions should work as-is. If not, **check all the above are working** before submitting an issue.

1. Take note of the path where the GIMP executable is installed (the default value in the vectorizer is the Mac OS location: `/Applications/Gimp.app/Contents/MacOS/gimp-2.8`).
2. Run the script on the provided test GeoTIFF:
`python vectorize_map.py test.tif`
3. Accept the GIMP folder location or input a different one and press ENTER.

**NOTE:** The vectorizer has problems with *filenames that contain spaces*. This will be supported eventually.

This should take about 70 seconds to process. **If it takes less there might be an error** (or your machine rulez). Take a look at the console output to find the possible culprit.

If it works, you will see a `test` folder with a `test-traced` set of files (`.shp`, `.dbf`, `.prj` and `.shx`) and two log files.

"
7,map-vectorizer-README.md, Configuring,"`vectorize_map.py` supports a few configuration options.

```bash
usage: vectorize_map.py [-h] --gimp-path GIMP_PATH [--chunksize CHUNKSIZE]
                        [--image-processing-configuration-file VECTORIZE_CONFIG]
                        <input file or dir>
```

"
8,map-vectorizer-README.md, Required argument,"- `<input file or dir>` path to file (or folder with files) to vectorize

"
9,map-vectorizer-README.md, Semi-optional arguments,"This are really required because 1) you won't have GIMP installed in the same folder as us and 2) your maps look different from the test map and you will want to have a config file.

- `--gimp-path GIMP_PATH` path to GIMP executable (defaults to `/Applications/Gimp.app/Contents/MacOS/gimp-2.8`)
- `--image-processing-configuration-file VECTORIZE_CONFIG, -p VECTORIZE_CONFIG` path to map image processing configuration file (defaults to `vectorize_config_default.txt`)

"
10,map-vectorizer-README.md, Optional arguments,"- `-h, --help` show help message and exit
- `--chunksize CHUNKSIZE` (ignore this but determines how to split temp file… but really, ignore it)

"
11,map-vectorizer-README.md, Customizing The Vectorizer to your own maps,"The Vectorizer was made to work with the [NYPL map atlases](http://digitalcollections.nypl.org/search/index?filters%5Btitle_uuid_s%5D%5B%5D=Maps%20of%20the%20city%20of%20New%20York%7C%7C323e4180-c603-012f-0c9f-58d385a7bc34&keywords=&layout=false#/?scroll=24). It is likely that your maps have different quality and colors. In order for this to work in your maps, you first need to do some minor config adjustments to generate a proper threshold file for your set (assuming it is a map set similar to the provided example `test.tif`):

1. Your map needs to be in **WGS84 projection**. Other projections might be supported in the future. Use `gdalwarp` to make this conversion like so:

```
gdalwarp -t_srs ""EPSG:4326"" input.tif output.tif
```

1. Open a representative from map (or maps) in GIMP
1. With the color picker, **select the color that most represents the paper/background color** (using a 5-pixel averaging pick would be best). Make note of the **red, green and blue values** (0-255).
1. Do the **same for the building colors** (like the pink, green, blue in the example).

You now want to produce a neat black-white image where **lines are black and all the rest is white**:

1. Apply `Colors > Brightness-Contrast...` looking to make the lines darker and buildings/paper brighter. The default values are **-50 brightness** and **95 contrast**. These may or may not work for you. Make note of the values that work best.
1. Now apply `Colors > Threshold...`. This takes a black and a white value. Anything darker/lighter than these values will become black/white respectively. The default values are **160 black** and **255 white**. Make note of the values that work best.

You now have the configuration values for your maps (map color list, brightness-contrast values, threshold values). Open `vectorize_config_default.txt` and replace the default values with your custom values. Save it as `vectorize_config.txt` (and *keep the default just in case*). Your config file should look like:

```
BRIGHTNESS_VALUE,CONTRAST_VALUE,BLACK_VALUE,WHITE_VALUE, brightness-contrast-thresholdblack-thresholdwhite
PAPER_RED,PAPER_GREEN,PAPER_BLUE,paper
BLDG_RED,BLDG_GREEN,BLDG_BLUE,somebuildingcolor
BLDG_RED,BLDG_GREEN,BLDG_BLUE,someotherbuildingcolor
...
```

It should **always start** with brightness/contrast/threshold in the first line and paper in the second line. There should also be **at least one building color**. You can add as many building colors as you wish (since our maps at NYPL are hand-colored, colors are not uniform so we have lighter/darker versions to compensate that).

When you run the vectorizer again, it will find this config file and use those values instead of the defaults.

It is likely that the vectorizer won't produce excellent results in the first try. It is a matter of adjusting these color values to generalize as much as possible to your map set.


"
12,map-vectorizer-README.md, Templates and other files,"`map_vectorizer/templates` contains images that we use in OpenCV to match crosses and other marks on maps.

`test.tif` is a file for testing.

The other images in the root are for the readme.

`simplify_map.R` is used by `vectorize_map.py`

"
13,map-vectorizer-README.md, Other scripts,"*bin/consolidator.py* for consolidating multiple outputs of a `vectorize_map.py` process into a single shapefile/GeoJSON. For example, when you run `vectorize_map.py` on a folder full of GeoTIFFs and then need to group all the individual folder outputs into a master file.

Usage: `./bin/consolidator.py path/to/folder` (no trailing slash)

"
14,map-vectorizer-README.md, Acknowledgements,"* Michael Resig
* Chris Garrard for his [sample code to assemble and disassemble shapefiles](http://cosmicproject.org/OGR/cris_example_write.html)
* Barry Rowlingson for his [tutorial on converting alpha shapes to polygons](http://rpubs.com/geospacedman/alphasimple)

"
15,map-vectorizer-README.md, Change log,"* 0.11: Including OpenCV in the `requirements.txt`.
* 0.10: Refactored Python code for better parameter management and cleaner folder structure.
* 0.9: Vectorizer now produces centroids (`calculate_centroids.py` is less necessary now).
* 0.8: Documented `consolidator.py`. Minor bug fixes.
* 0.7: Calculating average color with PIL instead of ImageMagick. Removed ImageMagick dependency, added PIL as dependency.
* 0.6: Fixed bug introduced in 0.5. Removed mention to GIMP preferences in README.
* 0.5: Added support for absolute paths.
* 0.4: Added a config file (rename `vectorize_config_default.txt` to `vectorize_config.txt`).
* 0.3: Added `consolidator.py` to assemble a set of shapefiles in a folder into a single file.
* 0.2: Added very rough OpenCV circle and cross detection (not working very well but it is a starting point).
* 0.1: Added GeoJSON output.

[@mgiraldo]: https://twitter.com/mgiraldo
[@nypl_labs]: https://twitter.com/nypl_labs
[@thomaslevine]: https://twitter.com/thomaslevine
[Python]: http://www.python.org/
[OpenCV]: http://opencv.org/
[PIL]: http://pythonware.com/products/pil/
[R]: http://www.r-project.org/
[GIMP]: http://www.gimp.org/
[GDAL Tools]: http://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries
[QGIS]: http://qgis.org/
"
0,microsoft-malmo-README.md, Malmö ,"Project Malmö is a platform for Artificial Intelligence experimentation and research built on top of Minecraft. We aim to inspire a new generation of research into challenging new problems presented by this unique environment.

[![Join the chat at https://gitter.im/Microsoft/malmo](https://badges.gitter.im/Microsoft/malmo.svg)](https://gitter.im/Microsoft/malmo?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/Microsoft/malmo.svg?branch=master)](https://travis-ci.org/Microsoft/malmo) [![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/Microsoft/malmo/blob/master/LICENSE.txt)
----
    
"
2,microsoft-malmo-README.md, MalmoEnv ,"MalmoEnv implements an Open AI ""gym""-like environment in Python without any native code (communicating directly with Java Minecraft). If you only need this functionallity then please see [MalmoEnv](https://github.com/Microsoft/malmo/tree/master/MalmoEnv). This will most likely be the preferred way to develop with Malmo Minecraft going forward.

If you wish to use the ""native"" Malmo implementation, either install the ""Malmo native Python wheel"" (if available for your platform) or a pre-built binary release (more on these options below). Building Malmo yourself from source is always an option!

Advantages:
    
1. No native code - you don't have to build or install platform dependent code.
2. A single network connection is used to run missions. No dynamic ports means it's more virtualization friendly.
3. A simpler multi-agent coordination protocol. 
One Minecraft client instance, one single port is used to start missions.
4. Less impedance miss-match with the gym api.

Disadvantages:

1. The existing Malmo examples are not supported (as API used is different). 
Marlo envs should work with this [port](https://github.com/AndKram/marLo/tree/malmoenv).
2. The API is more limited (e.g. selecting video options) - can edit mission xml directly.

"
3,microsoft-malmo-README.md, Malmo as a native Python wheel ,"On common Windows, MacOSX and Linux variants it is possible to use ```pip3 install malmo``` to install Malmo as a python with native code package: [Pip install for Malmo](https://github.com/Microsoft/malmo/blob/master/scripts/python-wheel/README.md). Once installed, the malmo Python module can be used to download source and examples and start up Minecraft with the Malmo game mod. 

Alternatively, a pre-built version of Malmo can be installed as follows:

1. [Download the latest *pre-built* version, for Windows, Linux or MacOSX.](https://github.com/Microsoft/malmo/releases)   
      NOTE: This is _not_ the same as downloading a zip of the source from Github. _Doing this **will not work** unless you are planning to build the source code yourself (which is a lengthier process). If you get errors along the lines of ""`ImportError: No module named MalmoPython`"" it will probably be because you have made this mistake._

2. Install the dependencies for your OS: [Windows](doc/install_windows.md), [Linux](doc/install_linux.md), [MacOSX](doc/install_macosx.md).

3. Launch Minecraft with our Mod installed. Instructions below.

4. Launch one of our sample agents, as Python, C#, C++ or Java. Instructions below.

5. Follow the [Tutorial](https://github.com/Microsoft/malmo/blob/master/Malmo/samples/Python_examples/Tutorial.pdf) 

6. Explore the [Documentation](http://microsoft.github.io/malmo/). This is also available in the readme.html in the release zip.

7. Read the [Blog](http://microsoft.github.io/malmo/blog) for more information.

If you want to build from source then see the build instructions for your OS: [Windows](doc/build_windows.md), [Linux](doc/build_linux.md), [MacOSX](doc/build_macosx.md).

----

"
4,microsoft-malmo-README.md, Problems: ,"We're building up a [Troubleshooting](https://github.com/Microsoft/malmo/wiki/Troubleshooting) page of the wiki for frequently encountered situations. If that doesn't work then please ask a question on our [chat page](https://gitter.im/Microsoft/malmo) or open a [new issue](https://github.com/Microsoft/malmo/issues/new).

----

"
5,microsoft-malmo-README.md, Launching Minecraft with our Mod: ,"Minecraft needs to create windows and render to them with OpenGL, so the machine you do this from must have a desktop environment.

Go to the folder where you unzipped the release, then:

`cd Minecraft`  
`launchClient` (On Windows)  
`./launchClient.sh` (On Linux or MacOSX)

or, e.g. `launchClient -port 10001` to launch Minecraft on a specific port.

on Linux or MacOSX: `./launchClient.sh -port 10001`

*NB: If you run this from a terminal, the bottom line will say something like ""Building 95%"" - ignore this - don't wait for 100%! As long as a Minecraft game window has opened and is displaying the main menu, you are good to go.*

By default the Mod chooses port 10000 if available, and will search upwards for a free port if not, up to 11000.
The port chosen is shown in the Mod config page.

To change the port while the Mod is running, use the `portOverride` setting in the Mod config page.

The Mod and the agents use other ports internally, and will find free ones in the range 10000-11000 so if administering
a machine for network use these TCP ports should be open.

----

"
7,microsoft-malmo-README.md, Running a Python agent: ,"```
cd Python_Examples
python3 run_mission.py
``` 

"
8,microsoft-malmo-README.md, Running a C++ agent: ,"`cd Cpp_Examples`

To run the pre-built sample:

`run_mission` (on Windows)  
`./run_mission` (on Linux or MacOSX)

To build the sample yourself:

`cmake .`  
`cmake --build .`  
`./run_mission` (on Linux or MacOSX)  
`Debug\run_mission.exe` (on Windows)

"
9,microsoft-malmo-README.md, Running a C agent: ,"To run the pre-built sample (on Windows):

`cd CSharp_Examples`  
`CSharpExamples_RunMission.exe`

To build the sample yourself, open CSharp_Examples/RunMission.csproj in Visual Studio.

Or from the command-line:

`cd CSharp_Examples`

Then, on Windows:  
```
msbuild RunMission.csproj /p:Platform=x64
bin\x64\Debug\CSharpExamples_RunMission.exe
```

"
10,microsoft-malmo-README.md, Running a Java agent: ,"`cd Java_Examples`  
`java -cp MalmoJavaJar.jar:JavaExamples_run_mission.jar -Djava.library.path=. JavaExamples_run_mission` (on Linux or MacOSX)  
`java -cp MalmoJavaJar.jar;JavaExamples_run_mission.jar -Djava.library.path=. JavaExamples_run_mission` (on Windows)

"
11,microsoft-malmo-README.md, Running an Atari agent: (Linux only) ,"```
cd Python_Examples
python3 ALE_HAC.py
```

----

"
12,microsoft-malmo-README.md, Citations ,"Please cite Malmo as:

Johnson M., Hofmann K., Hutton T., Bignell D. (2016) [_The Malmo Platform for Artificial Intelligence Experimentation._](http://www.ijcai.org/Proceedings/16/Papers/643.pdf) [Proc. 25th International Joint Conference on Artificial Intelligence](http://www.ijcai.org/Proceedings/2016), Ed. Kambhampati S., p. 4246. AAAI Press, Palo Alto, California USA. https://github.com/Microsoft/malmo

----

"
13,microsoft-malmo-README.md, Code of Conduct ,"This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
0,mplleaflet-README.md, mplleaflet,"mplleaflet is a Python library that converts a [matplotlib](http://matplotlib.org) plot into a webpage
containing a pannable, zoomable [Leaflet](http://leafletjs.com) map. It can also [embed the Leaflet map in an IPython notebook](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb). The goal of mplleaflet is to enable use of Python and matplotlib for visualizing geographic data on [slippy maps](http://wiki.openstreetmap.org/wiki/Slippy_Map) without having to write any Javascript or HTML. You also don't need to worry about choosing the base map content i.e., coastlines, roads, etc.

*Only one line of code is needed to convert a plot into a web map.*
`mplleaflet.show()`

The library is heavily inspired by [mpld3](https://github.com/jakevdp/mpld3) and uses  [mplexporter](https://github.com/mpld3/mplexporter) to do most of the heavy lifting to walk through Figure objects.

"
2,mplleaflet-README.md, Basic usage,"The simplest use is to just create your plot using matplotlib commands and call `mplleaflet.show()`.

```
>>> import matplotlib.pyplot as plt
... #: Load longitude, latitude data
>>> plt.hold(True)
#: Plot the data as a blue line with red squares on top
#: Just plot longitude vs. latitude
>>> plt.plot(longitude, latitude, 'b') #: Draw blue line
>>> plt.plot(longitude, latitude, 'rs') #: Draw red squares
```
![matplotlib x,y plot](examples/images/simple_plot.png)

Normally, displaying data as longitude, latitude will cause a cartographer to cry. That's totally fine with mplleaflet, Leaflet will project your data properly.

```
#: Convert to interactive Leaflet map
>>> import mplleaflet
>>> mplleaflet.show()
```

[Click to view final web page](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/readme_example.html)

![Leaflet map preview](examples/images/simple_plot_map_preview.jpg)

Disclaimer: Displaying data in spherical mercator might also cause a cartographer to cry.

`show()` allows you to specify different tile layer URLs, CRS/EPSG codes, output files, etc. 

"
3,mplleaflet-README.md, IPython Notebook embedding,"Just use  `mplleaflet.display()` to embed the interactive Leaflet map in an IPython notebook.
[Click here to see a live example.](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb)

"
4,mplleaflet-README.md, Other examples,"* [basic_plot.py](examples/basic_plot.py): Simple line/point plotting. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/basic_plot.html).
* [quiver.py](examples/quiver.py): Demonstrates use of quiver() to plot 2-D arrows. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/quiver.html).
* [contour.py](examples/contour.py): Compute contour curves. This example demonstrates plotting in a different CRS and letting mplleaflet convert the output. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/contour.html).
* [Embedded IPython notebook example](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb)

"
5,mplleaflet-README.md, Why mplleaflet?,"Other Python libraries, [basemap](http://matplotlib.org/basemap/) and
[folium](https://github.com/wrobstory/folium), exist to create maps in Python. However mplleaflet allows you to leverage  all matplotlib capability without having to set up the background basemap. You can use `plot()` to style points and lines, and you can also use more complex functions like `contour()`, `quiver()`, etc. Furthermore, with mplleaflet you no longer have to worry about setting up the basemap. Displaying continents or roads is determined automatically by the zoom level required to view the physical size of the data. You should use a different library if you need fine control over the basemap, or need a geographic projection other than spherical mercator.

"
6,mplleaflet-README.md, Installation,"Install `mplleaflet` from PyPI using `$ pip install mplleaflet`.

"
7,mplleaflet-README.md, Development,"If developing for `mplleaflet`, `mplexporter` is a git submodule with its
Python package files placed under the `mplleaflet` package. The Makefile
copies the files into the appropriate location.

```
$ git submodule init
$ git submodule update
$ make
$ pip install -e .

```

"
8,mplleaflet-README.md, Dependencies,"* [jinja2](http://jinja.pocoo.org/)

Optional
* [pyproj](https://code.google.com/p/pyproj/) Only needed if you only use non-WGS-84 projections.
* [GeoPandas](https://github.com/kjordahl/geopandas) To make your life easier.
"
0,pose-residual-network-pytorch-README.md, Pose Residual Network,"This repository contains a PyTorch implementation of the Pose Residual Network (PRN) presented in our ECCV 2018 paper: 

Muhammed Kocabas, Salih Karagoz, Emre Akbas. MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network. In ECCV, 2018. [arxiv](https://arxiv.org/abs/1807.04067)

PRN is described in Section 3.2 of the  paper.

"
1,pose-residual-network-pytorch-README.md, Getting Started,"We have tested our method on [Coco Dataset](http://cocodataset.org)

"
2,pose-residual-network-pytorch-README.md, Prerequisites,"```
python
pytorch
numpy
tqdm
pycocotools
progress
scikit-image
```

"
3,pose-residual-network-pytorch-README.md, Installing,"1. Clone this repository 
`git clone https://github.com/salihkaragoz/pose-residual-network-pytorch.git`

2. Install [Pytorch](https://pytorch.org/)

3. `pip install -r src/requirements.txt`

4. To download COCO dataset train2017 and val2017 annotations run: `bash data/coco.sh`. (data size: ~240Mb)

"
4,pose-residual-network-pytorch-README.md, Training,"`python train.py`

For more options look at opt.py

"
5,pose-residual-network-pytorch-README.md, Testing,"1. Download pre-train [model](https://drive.google.com/file/d/1OhdMllLGnpRAk6Wexw8LzXF_EHiolVj1/view?usp=sharing)

2. `python test.py --test_cp=PathToPreTrainModel/PRN.pth.tar`

"
6,pose-residual-network-pytorch-README.md, Results,"Results on COCO val2017 Ground Truth data.

```
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.892
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.978
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.921
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.883
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.912
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.982
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.937
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.902
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.944

```

"
8,pose-residual-network-pytorch-README.md, Citation,"If you find this code useful for your research, please consider citing our paper:
```
@Inproceedings{kocabas18prn,
  Title          = {Multi{P}ose{N}et: Fast Multi-Person Pose Estimation using Pose Residual Network},
  Author         = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
  Booktitle      = {European Conference on Computer Vision (ECCV)},
  Year           = {2018}
}
```
"
0,react-README.md, [React](https://reactjs.org/) &middot; [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/facebook/react/blob/master/LICENSE) [![npm version](https://img.shields.io/npm/v/react.svg?style=flat)](https://www.npmjs.com/package/react) [![CircleCI Status](https://circleci.com/gh/facebook/react.svg?style=shield&circle-token=:circle-token)](https://circleci.com/gh/facebook/react) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://reactjs.org/docs/how-to-contribute.htmlyour-first-pull-request),"React is a JavaScript library for building user interfaces.

* **Declarative:** React makes it painless to create interactive UIs. Design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes. Declarative views make your code more predictable, simpler to understand, and easier to debug.
* **Component-Based:** Build encapsulated components that manage their own state, then compose them to make complex UIs. Since component logic is written in JavaScript instead of templates, you can easily pass rich data through your app and keep state out of the DOM.
* **Learn Once, Write Anywhere:** We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code. React can also render on the server using Node and power mobile apps using [React Native](https://facebook.github.io/react-native/).

[Learn how to use React in your own project](https://reactjs.org/docs/getting-started.html).

"
1,react-README.md, Installation,"React has been designed for gradual adoption from the start, and **you can use as little or as much React as you need**:

* Use [Online Playgrounds](https://reactjs.org/docs/getting-started.html#online-playgrounds) to get a taste of React.
* [Add React to a Website](https://reactjs.org/docs/add-react-to-a-website.html) as a `<script>` tag in one minute.
* [Create a New React App](https://reactjs.org/docs/create-a-new-react-app.html) if you're looking for a powerful JavaScript toolchain.

You can use React as a `<script>` tag from a [CDN](https://reactjs.org/docs/cdn-links.html), or as a `react` package on [npm](https://www.npmjs.com/).

"
2,react-README.md, Documentation,"You can find the React documentation [on the website](https://reactjs.org/docs).  

Check out the [Getting Started](https://reactjs.org/docs/getting-started.html) page for a quick overview.

The documentation is divided into several sections:

* [Tutorial](https://reactjs.org/tutorial/tutorial.html)
* [Main Concepts](https://reactjs.org/docs/hello-world.html)
* [Advanced Guides](https://reactjs.org/docs/jsx-in-depth.html)
* [API Reference](https://reactjs.org/docs/react-api.html)
* [Where to Get Support](https://reactjs.org/community/support.html)
* [Contributing Guide](https://reactjs.org/docs/how-to-contribute.html)

You can improve it by sending pull requests to [this repository](https://github.com/reactjs/reactjs.org).

"
3,react-README.md, Examples,"We have several examples [on the website](https://reactjs.org/). Here is the first one to get you started:

```jsx
function HelloMessage({ name }) {
  return <div>Hello {name}</div>;
}

ReactDOM.render(
  <HelloMessage name=""Taylor"" />,
  document.getElementById('container')
);
```

This example will render ""Hello Taylor"" into a container on the page.

You'll notice that we used an HTML-like syntax; [we call it JSX](https://reactjs.org/docs/introducing-jsx.html). JSX is not required to use React, but it makes code more readable, and writing it feels like writing HTML. If you're using React as a `<script>` tag, read [this section](https://reactjs.org/docs/add-react-to-a-website.html#optional-try-react-with-jsx) on integrating JSX; otherwise, the [recommended JavaScript toolchains](https://reactjs.org/docs/create-a-new-react-app.html) handle it automatically.

"
4,react-README.md, Contributing,"The main purpose of this repository is to continue to evolve React core, making it faster and easier to use. Development of React happens in the open on GitHub, and we are grateful to the community for contributing bugfixes and improvements. Read below to learn how you can take part in improving React.

"
5,react-README.md, [Code of Conduct](https://code.fb.com/codeofconduct),"Facebook has adopted a Code of Conduct that we expect project participants to adhere to. Please read [the full text](https://code.fb.com/codeofconduct) so that you can understand what actions will and will not be tolerated.

"
6,react-README.md, [Contributing Guide](https://reactjs.org/contributing/how-to-contribute.html),"Read our [contributing guide](https://reactjs.org/contributing/how-to-contribute.html) to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React.

"
7,react-README.md, Good First Issues,"To help you get your feet wet and get you familiar with our contribution process, we have a list of [good first issues](https://github.com/facebook/react/labels/good%20first%20issue) that contain bugs which have a relatively limited scope. This is a great place to get started.

"
8,react-README.md, License,"React is [MIT licensed](./LICENSE).
"
0,da-faster-rcnn-README.md,  Domain Adaptive Faster R-CNN for Object Detection in the Wild ,"This is the implementation of our CVPR 2018 work 'Domain Adaptive Faster R-CNN for Object Detection in the Wild'. The aim is to improve the cross-domain robustness of object detection, in the screnario where training and test data are drawn from different distributions. The original paper can be found [here](https://arxiv.org/pdf/1803.03243.pdf). 

If you find it helpful for your research, please consider citing:

    @inproceedings{chen2018domain,
      title={Domain Adaptive Faster R-CNN for Object Detection in the Wild},
      author={Chen, Yuhua and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
      booktitle = {Computer Vision and Pattern Recognition (CVPR)},
      year={2018}
    }

If you encounter any problems with the code, please contact me at yuhua[dot]chen[at]vision[dot]ee[dot]ethz[dot]ch

"
1,da-faster-rcnn-README.md, Acknowledgment,"The implementation is built on the python implementation of Faster RCNN [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)

"
2,da-faster-rcnn-README.md, Usage,"1. Build Caffe and pycaffe (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))

2. Build the Cython modules
    ```Shell
    cd $FRCN_ROOT/lib
    make
    
3. Follow the instrutions of [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn) to download related data.
    
4. Prepare the dataset, source domain data should start with the filename 'source_', and target domain data with 'target_'.

5. To train the Domain Adaptive Faster R-CNN:
    ```Shell
    cd $FRCN_ROOT
    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  {NUM_ITER}  --cfg  {CONFIGURATION_FILE}
    
"
3,da-faster-rcnn-README.md, Example,"An example of adapting from **Cityscapes** dataset to **Foggy Cityscapes** dataset is provided:
1. Download the datasets from [here](https://www.cityscapes-dataset.com/downloads/). Specifically, we will use **gtFine_trainvaltest.zip**, **leftImg8bit_trainvaltest.zip** and **leftImg8bit_trainvaltest_foggy.zip**.

2. Prepare the data using the scripts in 'prepare_data/prepare_data.m'.

3. Train the Domain Adaptive Faster R-CNN:
    ```Shell
    cd $FRCN_ROOT
    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  70000  --cfg  models/da_faster_rcnn/faster_rcnn_end2end.yml
    
3. Test the trained model:
    ```Shell
    cd $FRCN_ROOT
    ./tools/test_net.py --gpu {GPU_ID} --def models/da_faster_rcnn/test.prototxt --net output/faster_rcnn_end2end/voc_2007_trainval/vgg16_da_faster_rcnn_iter_70000.caffemodel --imdb voc_2007_test --cfg models/da_faster_rcnn/faster_rcnn_end2end.yml

"
4,da-faster-rcnn-README.md, Other Implementation,"[Detectron-DA-Faster-RCNN](https://github.com/krumo/Detectron-DA-Faster-RCNN) in Caffe2(Detectron)
"
0,DBNet-README.md, News!,"__DBNet Autonomous Driving Data (prepared & raw) are released [here](http://www.dbehavior.net/download.aspx)!__
___We are going to organize DBNet challenges for CVPR/ICCV/ECCV Workshops. The instructions of DBNet-2018 challenge will be open soon. Stay tuned!___
-->

"
1,DBNet-README.md, Contents,"1. [Introduction](#introduction)
2. [Requirements](#requirements)
3. [Quick Start](#quick-start)
4. [Baseline](#baseline)
5. [Contributors](#contributors)
6. [Citation](#citation)
7. [License](#license)

"
2,DBNet-README.md, Introduction,"This work is based on our [research paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html), which appears in CVPR 2018. We propose a large-scale dataset for driving behavior learning, namely, DBNet. You can also check our [dataset webpage](http://www.dbehavior.net/) for a deeper introduction.

In this repository, we release __demo code__ and __partial prepared data__ for training with only images, as well as leveraging feature maps or point clouds. The prepared data are accessible [here](https://drive.google.com/open?id=14RPdVTwBTuCTo0tFeYmL_SyN8fD0g6Hc). (__More demo models and scripts are released soon!__)

"
3,DBNet-README.md, Requirements,"* **Tensorflow 1.2.0**
* Python 2.7
* CUDA 8.0+ (For GPU)
* Python Libraries: numpy, scipy and __laspy__

The code has been tested with Python 2.7, Tensorflow 1.2.0, CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04. But it may work on more machines (directly or through mini-modification), pull-requests or test report are well welcomed.

"
5,DBNet-README.md, Training,"To train a model to predict vehicle speeds and steering angles:

    python train.py --model nvidia_pn --batch_size 16 --max_epoch 125 --gpu 0

The names of the models are consistent with our [paper](http://www.dbehavior.net/publications.html).
Log files and network parameters will be saved to `logs` folder in default.

To see HELP for the training script:

    python train.py -h

We can use TensorBoard to view the network architecture and monitor the training progress.

    tensorboard --logdir logs

"
6,DBNet-README.md, Evaluation    ,"After training, you could evaluate the performance of models using `evaluate.py`. To plot the figures or calculate AUC, you may need to have matplotlib library installed.

    python evaluate.py --model_path logs/nvidia_pn/model.ckpt

"
7,DBNet-README.md, Prediction,"To get the predictions of test data:

    python predict.py

The results are saved in `results/results` (every segment) and `results/behavior_pred.txt` (merged) by default.
To change the storation location:

    python predict.py --result_dir specified_dir

The result directory will be created automatically if it doesn't exist.

"
8,DBNet-README.md, Baseline,"<table style=""undefined;table-layout: fixed; width: 512px""><colgroup><col style=""width: 68px""><col style=""width: 106px""><col style=""width: 66px""><col style=""width: 88px""><col style=""width: 54px""><col style=""width: 46px""><col style=""width: 38px""><col style=""width: 46px""></colgroup><tr><th>Method</th><th colspan=""2"">Setting</th><th>Accuracy</th><th>AUC</th><th>ME</th><th>AE</th><th>AME</th></tr><tr><td rowspan=""2"">nvidia-pn</td><td rowspan=""2"">Videos + Laser Points</td><td>angle</td><td>70.65% (&lt;5)</td><td>0.7799 </td><td>29.46</td><td>4.23</td><td>20.88</td></tr><tr><td>speed</td><td>82.21% (&lt;3)</td><td>0.8701</td><td>18.56</td><td>1.80</td><td>9.68</td></tr></table>

This baseline is run on __dbnet-2018 challenge data__ and only __nvidia\_pn__ is tested. To measure difficult architectures comprehensively, several metrics are set, including accuracy under different thresholds, area under curve (__AUC__), max error (__ME__), mean error (__AE__) and mean of max errors (__AME__).

The implementations of these metrics could be found in `evaluate.py`.

"
9,DBNet-README.md, Contributors,"DBNet was developed by [MVIG](http://www.mvig.org/), Shanghai Jiao Tong University* and [SCSC](http://scsc.xmu.edu.cn/) Lab, Xiamen University* (*alphabetical order*).

"
10,DBNet-README.md, Citation,"If you find our work useful in your research, please consider citing:

	@InProceedings{DBNet2018,
	  author = {Yiping Chen and Jingkang Wang and Jonathan Li and Cewu Lu and Zhipeng Luo and HanXue and Cheng Wang},
	  title = {LiDAR-Video Driving Dataset: Learning Driving Policies Effectively},
	  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	  month = {June},
	  year = {2018}
	}

"
11,DBNet-README.md, License,"Our code is released under Apache 2.0 License. The copyright of DBNet could be checked [here](http://www.dbehavior.net/contact.html).
"
0,tensorflow-magenta-README.md, Getting Started,"* [Installation](#installation)
* [Using Magenta](#using-magenta)
* [Playing a MIDI Instrument](#playing-a-midi-instrument)
* [Development Environment (Advanced)](#development-environment)

"
1,tensorflow-magenta-README.md, Installation,"Magenta maintains a [pip package](https://pypi.python.org/pypi/magenta) for easy
installation. We recommend using Anaconda to install it, but it can work in any
standard Python environment. We support both Python 2 (>= 2.7) and Python 3 (>= 3.5).
These instructions will assume you are using Anaconda.

Note that if you want to enable GPU support, you should follow the [GPU Installation](#gpu-installation) instructions below.

"
2,tensorflow-magenta-README.md, Automated Install (w/ Anaconda),"If you are running Mac OS X or Ubuntu, you can try using our automated
installation script. Just paste the following command into your terminal.

```bash
curl https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh
bash /tmp/magenta-install.sh
```

After the script completes, open a new terminal window so the environment
variable changes take effect.

The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!

Note that you will need to run `source activate magenta` to use Magenta every
time you open a new terminal window.

"
3,tensorflow-magenta-README.md, Manual Install (w/o Anaconda),"If the automated script fails for any reason, or you'd prefer to install by
hand, do the following steps.

Install the Magenta pip package:

```bash
pip install magenta
```

**NOTE**: In order to install the `rtmidi` package that we depend on, you may need to install headers for some sound libraries. On Linux, this command should install the necessary packages:

```bash
sudo apt-get install build-essential libasound2-dev libjack-dev
```

The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!

"
4,tensorflow-magenta-README.md, GPU Installation,"If you have a GPU installed and you want Magenta to use it, you will need to
follow the [Manual Install](#manual-install) instructions, but with a few
modifications.

First, make sure your system meets the [requirements to run tensorflow with GPU support](
https://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support).

Next, follow the [Manual Install](#manual-install) instructions, but install the
`magenta-gpu` package instead of the `magenta` package:

```bash
pip install magenta-gpu
```

The only difference between the two packages is that `magenta-gpu` depends on
`tensorflow-gpu` instead of `tensorflow`.

Magenta should now have access to your GPU.

"
5,tensorflow-magenta-README.md, Using Magenta,"You can now train our various models and use them to generate music, audio, and images. You can
find instructions for each of the models by exploring the [models directory](magenta/models).

To get started, create your own melodies with TensorFlow using one of the various configurations of our [Melody RNN](magenta/models/melody_rnn) model; a recurrent neural network for predicting melodies.

"
6,tensorflow-magenta-README.md, Playing a MIDI Instrument,"After you've trained one of the models above, you can use our [MIDI interface](magenta/interfaces/midi) to play with it interactively.

We also have created several [demos](https://github.com/tensorflow/magenta-demos) that provide a UI for this interface, making it easier to use (e.g., the browser-based [AI Jam](https://github.com/tensorflow/magenta-demos/tree/master/ai-jam-js)).

"
7,tensorflow-magenta-README.md, Development Environment,"If you want to develop on Magenta, you'll need to set up the full Development Environment.

First, clone this repository:

```bash
git clone https://github.com/tensorflow/magenta.git
```

Next, install the dependencies by changing to the base directory and executing the setup command:

```bash
pip install -e .
```

You can now edit the files and run scripts by calling Python as usual. For example, this is how you would run the `melody_rnn_generate` script from the base directory:

```bash
python magenta/models/melody_rnn/melody_rnn_generate --config=...
```

You can also install the (potentially modified) package with:

```bash
pip install .
```

Before creating a pull request, please also test your changes with:

```bash
pip install pytest-pylint
pytest
```

"
8,tensorflow-magenta-README.md, PIP Release,"To build a new version for pip, bump the version and then run:

```bash
python setup.py test
python setup.py bdist_wheel --universal
python setup.py bdist_wheel --universal --gpu
twine upload dist/magenta-N.N.N-py2.py3-none-any.whl
twine upload dist/magenta_gpu-N.N.N-py2.py3-none-any.whl
```
"
0,vue-README.md, What's the difference between Patreon and OpenCollective?,"Funds donated via Patreon go directly to support Evan You's full-time work on Vue.js. Funds donated via OpenCollective are managed with transparent expenses and will be used for compensating work and expenses for core team members or sponsoring community events. Your name/logo will receive proper recognition and exposure by donating on either platform.

<h3 align=""center"">Special Sponsors</h3>
<!--special start-->

<p align=""center"">
  <a href=""https://stdlib.com/"" target=""_blank"">
    <img width=""260px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/stdlib.png"">
  </a>
</p>
  
<!--special end-->

<h3 align=""center"">Platinum Sponsors</h3>

<!--platinum start-->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://bit.dev/?utm_source=vue&utm_medium=vue&utm_campaign=vue&utm_term=vue&utm_content=vue"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/bit.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""http://tooltwist.com/"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/tooltwist.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://vueschool.io/?utm_source=Vuejs.org&utm_medium=Banner&utm_campaign=Sponsored%20Banner&utm_content=V1"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vueschool.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://vehikl.com/"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vehikl.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.nativescript.org/vue?utm_source=vue-js-org&utm_medium=website&utm_campaign=nativescript-awareness"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/nativescript.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://teeplusplusclth.com/"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/tee__.png"">
        </a>
      </td>
    </tr><tr></tr>
  </tbody>
</table>
<!--platinum end-->

<!--special-china start-->
<h3 align=""center"">Platinum Sponsors (China)</h3>
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""http://www.dcloud.io/?hmsr=vuejsorg&hmpl=&hmcu=&hmkw=&hmci="" target=""_blank"">
          <img width=""177px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/dcloud.gif"">
        </a>
      </td>
    </tr><tr></tr>
  </tbody>
</table>
<!--special-china end-->

<h3 align=""center"">Gold Sponsors</h3>

<!--gold start-->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.vuemastery.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vuemastery.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://laravel.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/laravel.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://chaitin.cn/en/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/chaitin.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://htmlburger.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/html_burger.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.frontenddeveloperlove.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/frontend_love.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://onsen.io/vue/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/onsen_ui.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://neds.com.au/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/neds.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://icons8.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/icons_8.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://vuejobs.com/?ref=vuejs"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vuejobs.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.valuecoders.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/valuecoders.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://tidelift.com/subscription/npm/vue"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/tidelift.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""http://www.syncfusion.com/?utm_source=vuejs&utm_medium=list&utm_campaign=vuejsjslistcy19"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/syncfusion.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://opteo.com/vue"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/opteo.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://devsquad.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/devsquad.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.firesticktricks.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/firestick_tricks.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://intygrate.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/intygrate.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://isleofcode.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/isle_of_code.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://passionatepeople.io/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/passionate_people.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""http://en.shopware.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/shopware_ag.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://x-team.com/join/?utm_source=vuejsorg&utm_medium=sponsor&utm_campaign=vuejsorg-patreon"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/x_team.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.vpnranks.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vpnranks.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.simplyswitch.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/energy_comparison.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.bacancytechnology.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/bacancy_technology.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.bestvpn.co/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/bestvpn_co.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://blokt.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/blokt_cryptocurrency_news.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.jqwidgets.com/vue/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/jqwidgets_ltd.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.y8.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/y8.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://js.devexpress.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/devexpress.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://fastcoding.jp/javascript/ "" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/fastcoding_inc.svg"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://usave.co.uk/utilities/broadband"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/usave.png"">
        </a>
      </td>
    </tr><tr></tr>
  </tbody>
</table>
<!--gold end-->

<h3 align=""center"">Sponsors via <a href=""https://opencollective.com/vuejs"">Open Collective</a></h3>

<h4 align=""center"">Platinum</h4>

<a href=""https://opencollective.com/vuejs/tiers/platinum-sponsors/0/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/platinum-sponsors/0/avatar.svg""></a>
<a href=""https://opencollective.com/vuejs/tiers/platinum-sponsors/1/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/platinum-sponsors/1/avatar.svg""></a>

<h4 align=""center"">Gold</h4>

<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/0/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/0/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/1/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/1/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/2/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/2/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/3/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/3/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/4/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/4/avatar.svg"" height=""60px""></a>

---

"
1,vue-README.md, Introduction,"Vue (pronounced `/vjuː/`, like view) is a **progressive framework** for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.

"
2,vue-README.md, Browser Compatibility,"Vue.js supports all browsers that are [ES5-compliant](http://kangax.github.io/compat-table/es5/) (IE8 and below are not supported).

"
3,vue-README.md, Ecosystem,"| Project | Status | Description |
|---------|--------|-------------|
| [vue-router]          | [![vue-router-status]][vue-router-package] | Single-page application routing |
| [vuex]                | [![vuex-status]][vuex-package] | Large-scale state management |
| [vue-cli]             | [![vue-cli-status]][vue-cli-package] | Project scaffolding |
| [vue-loader]          | [![vue-loader-status]][vue-loader-package] | Single File Component (`*.vue` file) loader for webpack |
| [vue-server-renderer] | [![vue-server-renderer-status]][vue-server-renderer-package] | Server-side rendering support |
| [vue-class-component] | [![vue-class-component-status]][vue-class-component-package] | TypeScript decorator for a class-based API |
| [vue-rx]              | [![vue-rx-status]][vue-rx-package] | RxJS integration |
| [vue-devtools]        | [![vue-devtools-status]][vue-devtools-package] | Browser DevTools extension |

[vue-router]: https://github.com/vuejs/vue-router
[vuex]: https://github.com/vuejs/vuex
[vue-cli]: https://github.com/vuejs/vue-cli
[vue-loader]: https://github.com/vuejs/vue-loader
[vue-server-renderer]: https://github.com/vuejs/vue/tree/dev/packages/vue-server-renderer
[vue-class-component]: https://github.com/vuejs/vue-class-component
[vue-rx]: https://github.com/vuejs/vue-rx
[vue-devtools]:  https://github.com/vuejs/vue-devtools

[vue-router-status]: https://img.shields.io/npm/v/vue-router.svg
[vuex-status]: https://img.shields.io/npm/v/vuex.svg
[vue-cli-status]: https://img.shields.io/npm/v/@vue/cli.svg
[vue-loader-status]: https://img.shields.io/npm/v/vue-loader.svg
[vue-server-renderer-status]: https://img.shields.io/npm/v/vue-server-renderer.svg
[vue-class-component-status]: https://img.shields.io/npm/v/vue-class-component.svg
[vue-rx-status]: https://img.shields.io/npm/v/vue-rx.svg
[vue-devtools-status]: https://img.shields.io/chrome-web-store/v/nhdogjmejiglipccpnnnanhbledajbpd.svg

[vue-router-package]: https://npmjs.com/package/vue-router
[vuex-package]: https://npmjs.com/package/vuex
[vue-cli-package]: https://npmjs.com/package/@vue/cli
[vue-loader-package]: https://npmjs.com/package/vue-loader
[vue-server-renderer-package]: https://npmjs.com/package/vue-server-renderer
[vue-class-component-package]: https://npmjs.com/package/vue-class-component
[vue-rx-package]: https://npmjs.com/package/vue-rx
[vue-devtools-package]: https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd

"
4,vue-README.md, Documentation,"To check out [live examples](https://vuejs.org/v2/examples/) and docs, visit [vuejs.org](https://vuejs.org).

"
5,vue-README.md, Questions,"For questions and support please use [the official forum](http://forum.vuejs.org) or [community chat](https://chat.vuejs.org/). The issue list of this repo is **exclusively** for bug reports and feature requests.

"
6,vue-README.md, Issues,"Please make sure to read the [Issue Reporting Checklist](https://github.com/vuejs/vue/blob/dev/.github/CONTRIBUTING.md#issue-reporting-guidelines) before opening an issue. Issues not conforming to the guidelines may be closed immediately.

"
7,vue-README.md, Changelog,"Detailed changes for each release are documented in the [release notes](https://github.com/vuejs/vue/releases).

"
8,vue-README.md, Stay In Touch,"- [Twitter](https://twitter.com/vuejs)
- [Blog](https://medium.com/the-vue-point)
- [Job Board](https://vuejobs.com/?ref=vuejs)

"
9,vue-README.md, Contribution,"Please make sure to read the [Contributing Guide](https://github.com/vuejs/vue/blob/dev/.github/CONTRIBUTING.md) before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to [this curated list](https://github.com/vuejs/awesome-vue)!

Thank you to all the people who already contributed to Vue!

<a href=""https://github.com/vuejs/vue/graphs/contributors""><img src=""https://opencollective.com/vuejs/contributors.svg?width=890"" /></a>


"
10,vue-README.md, License,"[MIT](http://opensource.org/licenses/MIT)

Copyright (c) 2013-present, Yuxi (Evan) You
"
0,sequelize-sequelize-README.md, Sequelize,"[![npm version](https://badgen.net/npm/v/sequelize)](https://www.npmjs.com/package/sequelize)
[![Travis Build Status](https://badgen.net/travis/sequelize/sequelize?icon=travis)](https://travis-ci.org/sequelize/sequelize)
[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/9l1ypgwsp5ij46m3/branch/master?svg=true)](https://ci.appveyor.com/project/sushantdhiman/sequelize/branch/master)
[![npm downloads](https://badgen.net/npm/dm/sequelize)](https://www.npmjs.com/package/sequelize)
[![codecov](https://badgen.net/codecov/c/github/sequelize/sequelize?icon=codecov)](https://codecov.io/gh/sequelize/sequelize)
[![Last commit](https://badgen.net/github/last-commit/sequelize/sequelize)](https://github.com/sequelize/sequelize)
[![Merged PRs](https://badgen.net/github/merged-prs/sequelize/sequelize)](https://github.com/sequelize/sequelize)
[![GitHub stars](https://badgen.net/github/stars/sequelize/sequelize)](https://github.com/sequelize/sequelize)
[![Bountysource](https://www.bountysource.com/badge/team?team_id=955&style=bounties_received)](https://www.bountysource.com/teams/sequelize/issues?utm_source=Sequelize&utm_medium=shield&utm_campaign=bounties_received)
[![Slack Status](http://sequelize-slack.herokuapp.com/badge.svg)](http://sequelize-slack.herokuapp.com/)
[![node](https://badgen.net/npm/node/sequelize)](https://www.npmjs.com/package/sequelize)
[![License](https://badgen.net/github/license/sequelize/sequelize)](https://github.com/sequelize/sequelize/blob/master/LICENSE)
[![semantic-release](https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg)](https://github.com/semantic-release/semantic-release)

Sequelize is a promise-based Node.js ORM for Postgres, MySQL, MariaDB, SQLite and Microsoft SQL Server. It features solid transaction support, relations, eager and lazy loading, read replication and more.

Sequelize follows [SEMVER](http://semver.org). Supports Node v6 and above to use ES6 features.

New to Sequelize? Take a look at the [Tutorials and Guides](https://sequelize.org/master). You might also be interested in the [API Reference](https://sequelize.org/master/identifiers).

"
1,sequelize-sequelize-README.md, v5 Release,"You can find the upgrade guide and changelog [here](https://sequelize.org/master/manual/upgrade-to-v5.html).

"
2,sequelize-sequelize-README.md, Table of Contents,"- [Installation](#installation)
- [Documentation](#documentation)
- [Responsible disclosure](#responsible-disclosure)
- [Resources](#resources)

"
3,sequelize-sequelize-README.md, Installation,"```bash
$ npm install --save sequelize #: This will install v5

#: And one of the following:
$ npm install --save pg pg-hstore #: Postgres
$ npm install --save mysql2
$ npm install --save mariadb
$ npm install --save sqlite3
$ npm install --save tedious #: Microsoft SQL Server
```

"
4,sequelize-sequelize-README.md, Documentation,"- [v5 Documentation](https://sequelize.org/master)
- [v4 Documentation](https://sequelize.org/v4)
- [v3 Documentation](https://sequelize.org/v3)
- [Contributing](https://github.com/sequelize/sequelize/blob/master/CONTRIBUTING.md)

"
5,sequelize-sequelize-README.md, Responsible disclosure,"If you have any security issue to report, contact project maintainers privately. You can find contact information in [CONTACT.md](https://github.com/sequelize/sequelize/blob/master/CONTACT.md).

"
6,sequelize-sequelize-README.md, Resources,"- [Changelog](https://github.com/sequelize/sequelize/releases)
- [Slack](http://sequelize-slack.herokuapp.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/sequelize.js)

"
7,sequelize-sequelize-README.md, Tools,"- [Sequelize & TypeScript](https://sequelize.org/master/manual/typescript.html)
- [Enhanced TypeScript with decorators](https://github.com/RobinBuschmann/sequelize-typescript)
- [Sequelize & GraphQL](https://github.com/mickhansen/graphql-sequelize)
- [Add-ons & Plugins](https://sequelize.org/master/manual/resources.html)
- [Sequelize CLI](https://github.com/sequelize/cli)

"
8,sequelize-sequelize-README.md, Learning,"- [Getting Started](https://sequelize.org/master/manual/getting-started)
- [Express Example](https://github.com/sequelize/express-example)

"
9,sequelize-sequelize-README.md, Translations,"- [English v5](https://sequelize.org/master) (OFFICIAL)
- [中文文档 v4/v5](https://github.com/demopark/sequelize-docs-Zh-CN) (UNOFFICIAL)
"
1,DID-MDN-README.md, Density-aware Single Image De-raining using a Multi-stream Dense Network,"[He Zhang](https://sites.google.com/site/hezhangsprinter), [Vishal M. Patel](http://www.rci.rutgers.edu/~vmp93/)

[[Paper Link](https://arxiv.org/abs/1802.07412)] (CVPR'18)

We present a novel density-aware multi-stream densely connected convolutional neural
network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method
enables the network itself to automatically determine the rain-density information and then efficiently remove the
corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with dif-
ferent scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages
features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and
used to train the proposed density-aware network. 

	@inproceedings{derain_zhang_2018,		
	  title={Density-aware Single Image De-raining using a Multi-stream Dense Network},
	  author={Zhang, He and Patel, Vishal M},
	  booktitle={CVPR},
	  year={2018}
	} 

<p align=""center"">
<img src=""sample_results/121_input.jpg"" width=""300px"" height=""200px""/>         <img src=""sample_results/121_our.jpg"" width=""300px"" height=""200px""/>
<img src=""sample_results/38_input.jpg"" width=""300px"" height=""200px""/>         <img src=""sample_results/38_our.jpg"" width=""300px"" height=""200px""/>
</p>



"
2,DID-MDN-README.md, Prerequisites:,"1. Linux
2. Python 2 or 3
3. CPU or NVIDIA GPU + CUDA CuDNN (CUDA 8.0)
 
"
3,DID-MDN-README.md, Installation:,"1. Install PyTorch and dependencies from http://pytorch.org (Ubuntu+Python2.7)
   (conda install pytorch torchvision -c pytorch)

2. Install Torch vision from the source.
   (git clone https://github.com/pytorch/vision
   cd vision
   python setup.py install)

3. Install python package: 
   numpy, scipy, PIL, pdb
   
"
4,DID-MDN-README.md, Demo using pre-trained model,"	python test.py --dataroot ./facades/github --valDataroot ./facades/github --netG ./pre_trained/netG_epoch_9.pth   
Pre-trained model can be downloaded at (put it in the folder 'pre_trained'): https://drive.google.com/drive/folders/1VRUkemynOwWH70bX9FXL4KMWa4s_PSg2?usp=sharing

Pre-trained density-aware model can be downloaded at (Put it in the folder 'classification'): https://drive.google.com/drive/folders/1-G86JTvv7o1iTyfB2YZAQTEHDtSlEUKk?usp=sharing

Pre-trained residule-aware model can be downloaded at (Put it in the folder 'residual_heavy'): https://drive.google.com/drive/folders/1bomrCJ66QVnh-WduLuGQhBC-aSWJxPmI?usp=sharing

"
5,DID-MDN-README.md, Training (Density-aware Deraining network using GT label),"	python derain_train_2018.py  --dataroot ./facades/DID-MDN-training/Rain_Medium/train2018new  --valDataroot ./facades/github --exp ./check --netG ./pre_trained/netG_epoch_9.pth.
	Make sure you download the training sample and put in the right folder

"
6,DID-MDN-README.md, Density-estimation Training (rain-density classifier),"	python train_rain_class.py  --dataroot ./facades/DID-MDN-training/Rain_Medium/train2018new  --exp ./check_class	

"
7,DID-MDN-README.md, Testing,"	python demo.py --dataroot ./your_dataroot --valDataroot ./your_dataroot --netG ./pre_trained/netG_epoch_9.pth   

"
8,DID-MDN-README.md, Reproduce,"To reproduce the quantitative results shown in the paper, please save both generated and target using python demo.py  into the .png format and then test using offline tool such as the PNSR and SSIM measurement in Python or Matlab.   In addition, please use netG.train() for testing since the batch for training is 1. 
 
"
9,DID-MDN-README.md, Dataset,"Training (heavy, medium, light) and testing (TestA and Test B) data can be downloaded at the following link:
https://drive.google.com/file/d/1cMXWICiblTsRl1zjN8FizF5hXOpVOJz4/view?usp=sharing

"
10,DID-MDN-README.md, Acknowledgments,"Great thanks for the insight discussion with [Vishwanath Sindagi](http://www.vishwanathsindagi.com/) and help from [Hang Zhang](http://hangzh.com/)
"
0,cltk-cltk-README.md, The Classical Language Toolkit,"[![PyPi downloads](http://img.shields.io/pypi/v/cltk.svg?style=flat)](https://pypi.python.org/pypi/cltk/) [![Documentation Status](https://readthedocs.org/projects/cltk/badge/?version=latest)](http://docs.cltk.org/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.593336.svg)](https://doi.org/10.5281/zenodo.593336)

[![Build Status](https://travis-ci.org/cltk/cltk.svg?branch=master)](https://travis-ci.org/cltk/cltk) [![codecov.io](http://codecov.io/github/cltk/cltk/coverage.svg?branch=master)](http://codecov.io/github/cltk/cltk?branch=master)

[![Join the chat at https://gitter.im/cltk/cltk](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/cltk/cltk?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)


"
1,cltk-cltk-README.md, About,"The Classical Language Toolkit (CLTK) offers natural language processing (NLP) support for the languages of Ancient, Classical, and Medieval Eurasia. Greek, Latin, Akkadian, and the Germanic languages are currently most complete. The goals of the CLTK are to:
*   compile analysis-friendly corpora;
*   collect and generate linguistic data;
*   act as a free and open platform for generating scientific research.


"
2,cltk-cltk-README.md, Documentation,"The docs are at [docs.cltk.org](http://docs.cltk.org).


"
3,cltk-cltk-README.md, Installation,"CLTK supports Python versions 3.6 and 3.7. The software only runs on POSIX–compliant operating systems (Linux, Mac OS X, FreeBSD, etc.).

``` bash
$ pip install cltk
```

See docs for [complete installation instructions](http://docs.cltk.org/en/latest/installation.html).

The [CLTK organization curates corpora](https://github.com/cltk) which can be downloaded directly or, better, [imported by the toolkit](http://docs.cltk.org/en/latest/importing_corpora.html).


"
4,cltk-cltk-README.md, Tutorials,"For interactive tutorials, in the form of Jupyter Notebooks, see <https://github.com/cltk/tutorials>.


"
5,cltk-cltk-README.md, Contributing,"See the [Quickstart for contributors](https://github.com/cltk/cltk/wiki/Quickstart-for-contributors) for an overview of the process. If you're looking to start with a small contribution, see the [Issue tracker for ""easy"" jobs](https://github.com/cltk/cltk/issues?q=is%3Aopen+is%3Aissue+label%3Aeasy) needing to be done. Bigger projects may be found at [Project ideas](https://github.com/cltk/cltk/wiki/Project-ideas) page. Of course, new ideas are always welcome.


"
6,cltk-cltk-README.md, Citation,"Each major release of the CLTK is given a [DOI](http://en.wikipedia.org/wiki/Digital_object_identifier), a type of unique identity for digital documents. This DOI ought to be included in your citation, as it will allow researchers to reproduce your results should the CLTK's API or codebase change. To find the CLTK's current DOI, observe the blue `DOI` button in the repository's home on GitHub. To the end of your bibliographic entry, append `DOI ` plus the current identifier. You may also add version/release number, located in the `pypi` button at the project's GitHub repository homepage.

Thus, please cite core software as something like:
```
Kyle P. Johnson et al.. (2014-2019). CLTK: The Classical Language Toolkit. DOI 10.5281/zenodo.<current_release_id>
```

A style-neutral BibTeX entry would look like this:
```
@Misc{johnson2014,
author = {Kyle P. Johnson et al.},
title = {CLTK: The Classical Language Toolkit},
howpublished = {\url{https://github.com/cltk/cltk}},
note = {{DOI} 10.5281/zenodo.<current_release_id>},
year = {2014--2019},
}
```


[Many contributors](https://github.com/cltk/cltk/blob/master/contributors.md) have made substantial contributions to the CLTK. For scholarship about particular code, it might be proper to cite these individuals as authors of the work under discussion.


"
7,cltk-cltk-README.md, Gratitude,"We are thankful for the following organizations that have offered support:

* Google Summer of Code (sponsoring two students, 2016, 2017; three students 2018)
* JetBrains (licenses for PyCharm)
* Google Cloud Platform (with credits for the Classical Language Archive and API)


"
8,cltk-cltk-README.md, License,"The CLTK is Copyright (c) 2014-2019 Kyle P. Johnson, under the MIT License. See [LICENSE](https://github.com/cltk/cltk/blob/master/LICENSE) for details.
"
0,segyio-README.md, segyio ,"[![Travis](https://img.shields.io/travis/equinor/segyio/master.svg?label=travis)](https://travis-ci.org/equinor/segyio)
[![Appveyor](https://ci.appveyor.com/api/projects/status/2i5cr8ui2t9qbxk9?svg=true)](https://ci.appveyor.com/project/statoil-travis/segyio)
[![PyPI Updates](https://pyup.io/repos/github/equinor/segyio/shield.svg)](https://pyup.io/repos/github/equinor/segyio/)
[![Python 3](https://pyup.io/repos/github/equinor/segyio/python-3-shield.svg)](https://pyup.io/repos/github/equinor/segyio/)

[readthedocs](https://segyio.readthedocs.io/)

"
1,segyio-README.md, Index ,"* [Introduction](#introduction)
* [Feature summary](#feature-summary)
* [Getting started](#getting-started)
    * [Quick start](#quick-start)
    * [Get segyio](#get-segyio)
    * [Build segyio](#build-segyio)
* [Tutorial](#tutorial)
    * [Basics](#basics)
    * [Modes](#modes)
    * [Mode examples](#mode-examples)
* [Goals](#project-goals)
* [Contributing](#contributing)
* [Examples](#examples)
* [Common issues](#common-issues)
* [History](#history)

"
2,segyio-README.md, Introduction ,"Segyio is a small LGPL licensed C library for easy interaction with SEG-Y and
Seismic Unix formatted seismic data, with language bindings for Python and
Matlab. Segyio is an attempt to create an easy-to-use, embeddable,
community-oriented library for seismic applications. Features are added as they
are needed; suggestions and contributions of all kinds are very welcome.

To catch up on the latest development and features, see the
[changelog](changelog.md). To write future proof code, consult the planned
[breaking changes](breaking-changes.md).

"
3,segyio-README.md, Feature summary ,"  * A low-level C interface with few assumptions; easy to bind to other
    languages
  * Read and write binary and textual headers
  * Read and write traces and trace headers
  * Simple, powerful, and native-feeling Python interface with numpy
    integration
  * Read and write seismic unix files
  * xarray integration with netcdf_segy
  * Some simple applications with unix philosophy

"
4,segyio-README.md, Getting started ,"When segyio is built and installed, you're ready to start programming! Check
out the [tutorial](#tutorial), [examples](#examples), [example
programs](python/examples), and [example
notebooks](https://github.com/equinor/segyio-notebooks). For a technical
reference with examples and small recipes, [read the
docs](https://segyio.readthedocs.io/). API docs are also available with pydoc -
start your favourite Python interpreter and type `help(segyio)`, which should
integrate well with IDLE, pycharm and other Python tools.

"
5,segyio-README.md, Quick start ,"```python
import segyio
import numpy as np
with segyio.open('file.sgy') as f:
    for trace in f.trace:
        filtered = trace[np.where(trace < 1e-2)]
```

See the [examples](#examples) for more.

"
6,segyio-README.md, Get segyio ,"A copy of segyio is available both as pre-built binaries and source code:

* In Debian [unstable](https://packages.debian.org/source/sid/segyio)
    * `apt install python3-segyio`
* Wheels for Python from [PyPI](https://pypi.python.org/pypi/segyio/)
    * `pip install segyio`
* Source code from [github](https://github.com/equinor/segyio)
    * `git clone https://github.com/statoil/segyio`
* Source code in [tarballs](https://github.com/equinor/segyio/releases)

"
7,segyio-README.md, Build segyio ,"To build segyio you need:
 * A C99 compatible C compiler (tested mostly on gcc and clang)
 * A C++ compiler for the Python extension, and C++11 for the tests
 * [CMake](https://cmake.org/) version 2.8.12 or greater
 * [Python](https://www.python.org/) 2.7 or 3.x.
 * [numpy](http://www.numpy.org/) version 1.10 or greater
 * [setuptools](https://pypi.python.org/pypi/setuptools) version 28 or greater
 * [setuptools-scm](https://pypi.python.org/pypi/setuptools_scm)
 * [pytest](https://pypi.org/project/pytest)

 To build the documentation, you also need
 [sphinx](https://pypi.org/project/Sphinx)

To build and install segyio, perform the following actions in your console:

```bash
git clone https://github.com/equinor/segyio
mkdir segyio/build
cd segyio/build
cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON
make
make install
```

`make install` must be done as root for a system install; if you want to
install in your home directory, add `-DCMAKE_INSTALL_PREFIX=~/` or some other
appropriate directory, or `make DESTDIR=~/ install`. Please ensure your
environment picks up on non-standard install locations (PYTHONPATH,
LD_LIBRARY_PATH and PATH).

If you have multiple Python installations, or want to use some alternative
interpreter, you can help cmake find the right one by passing
`-DPYTHON_EXECUTABLE=/opt/python/binary` along with install prefix and build
type.

To build the matlab bindings, invoke CMake with the option `-DBUILD_MEX=ON`. In
some environments the Matlab binaries are in a non-standard location, in which
case you need to help CMake find the matlab binaries by passing
`-DMATLAB_ROOT=/path/to/matlab`.

"
8,segyio-README.md, Developers ,"It's recommended to build in debug mode to get more warnings and to embed debug
symbols in the objects. Substituting `Debug` for `Release` in the
`CMAKE_BUILD_TYPE` is plenty.

Tests are located in the language/tests directories, and it's highly
recommended that new features added are demonstrated for correctness and
contract by adding a test. All tests can be run by invoking `ctest`. Feel free
to use the tests already written as a guide.

After building segyio you can run the tests with `ctest`, executed from the
build directory.

Please note that to run the Python examples you need to let your environment
know where to find the Python library. It can be installed as a user, or on
adding the segyio/build/python library to your pythonpath.

"
9,segyio-README.md, Tutorial ,"All code in this tutorial assumes segyio is imported, and that numpy is
available as np.

```python
import segyio
import numpy as np
```

This tutorial assumes you're familiar with Python and numpy. For a refresh,
check out the [python tutorial](https://docs.python.org/3/tutorial/) and [numpy
quickstart](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)

"
10,segyio-README.md, Basics ,"Opening a file for reading is done with the `segyio.open` function, and
idiomatically used with context managers. Using the `with` statement, files are
properly closed even in the case of exceptions. By default, files are opened
read-only.

```python
with segyio.open(filename) as f:
    ...
```

Open accepts several options (for more a more comprehensive reference, check
the open function's docstring with `help(segyio.open)`. The most important
option is the second (optional) positional argument. To open a file for
writing, do `segyio.open(filename, 'r+')`, from the C `fopen` function.

Files can be opened in *unstructured* mode, either by passing `segyio.open` the
optional arguments `strict=False`, in which case not establishing structure
(inline numbers, crossline numbers etc.) is not an error, and
`ignore_geometry=True`, in which case segyio won't even try to set these
internal attributes.

The segy file object has several public attributes describing this structure:
* `f.ilines`
    Inferred inline numbers
* `f.xlines`
    Inferred crossline numbers
* `f.offsets`
    Inferred offsets numbers
* `f.samples`
    Inferred sample offsets (frequency and recording time delay)
* `f.unstructured`
    True if unstructured, False if structured
* `f.ext_headers`
    The number of extended textual headers

If the file is opened *unstructured*, all the line properties will will be
`None`.

"
11,segyio-README.md, Modes ,"In segyio, data is retrieved and written through so-called *modes*. Modes are
abstract arrays, or addressing schemes, and change what names and indices mean.
All modes are properties on the file handle object, support the `len` function,
and reads and writes are done through `f.mode[]`. Writes are done with
assignment. Modes support array slicing inspired by numpy. The following modes
are available:

* `trace`

    The trace mode offers raw addressing of traces as they are laid out in the
    file. This, along with `header`, is the only mode available for
    unstructured files. Traces are enumerated `0..len(f.trace)`.

    Reading a trace yields a numpy `ndarray`, and reading multiple traces
    yields a generator of `ndarray`s. Generator semantics are used and the same
    object is reused, so if you want to cache or address trace data later, you
    must explicitly copy.

    ```python
    >>> f.trace[10]
    >>> f.trace[-2]
    >>> f.trace[15:45]
    >>> f.trace[:45:3]
    ```

* `header`

    With addressing behaviour similar to `trace`, accessing items yield header
    objects instead of numpy `ndarray`s. Headers are dict-like objects, where
    keys are integers, seismic unix-style keys (in segyio.su module) and segyio
    enums (segyio.TraceField).

    Header values can be updated by assigning a dict-like to it, and keys not
    present on the right-hand-side of the assignment are *unmodified*.

    ```python
    >>> f.header[5] = { segyio.su.tracl: 10 }
    >>> f.header[5].items()
    >>> f.header[5][25, 37] #: read multiple values at once
    ```

* `iline`, `xline`

    These modes will raise an error if the file is unstructured. They consider
    arguments to `[]` as the *keys* of the respective lines. Line numbers are
    always increasing, but can have arbitrary, uneven spacing. The valid names
    can be found in the `ilines` and `xlines` properties.

    As with traces, getting one line yields an `ndarray`, and a slice of lines
    yields a generator of `ndarray`s. When using slices with a step, some
    intermediate items might be skipped if it is not matched by the step, i.e.
    doing `f.line[1:10:3]` on a file with lines `[1,2,3,4,5]` is equivalent of
    looking up `1, 4, 7`, and finding `[1,4]`.

    When working with a 4D pre-stack file, the first offset is implicitly read.
    To access a different or a range of offsets, use comma separated indices or
    ranges, as such: `f.iline[120, 4]`.

* `fast`, `slow`

    These are aliases for `iline` and `xline`, determined by how the traces are
    laid out. For inline sorted files, `fast` would yield `iline`.

* `depth_slice`

    The depth slice is a horizontal, file-wide cut at a depth. The yielded
    values are `ndarray`s and generators-of-arrays.

* `gather`

    The `gather` is the intersection of an inline and crossline, a vertical
    column of the survey, and unless a single offset is specified returns an
    offset x samples `ndarray`. In the presence of ranges, it returns a
    generator of such `ndarray`s.

* `text`

    The `text` mode is an array of the textual headers, where `text[0]` is the
    standard-mandated textual header, and `1..n` are the optional extended
    headers.

    The text headers are returned as 3200-byte string-like blobs (bytes in
    Python 3, str in Python 2), as it is in the file. The `segyio.tools.wrap`
    function can create a line-oriented version of this string.

* `bin`

    The values of the file-wide binary header with a dict-like interface.
    Behaves like the `header` mode, but without the indexing.

"
12,segyio-README.md, Mode examples ,"```python
>>> for line in f.iline[:2430]:
...     print(np.average(line))

>>> for line in f.xline[2:10]:
...     print(line)

>>> for line in f.fast[::2]:
...     print(np.min(line))

>>> for factor, offset in enumerate(f.iline[10, :]):
...     offset *= factor
        print(offset)

>>> f.gather[200, 241, :].shape

>>> text = f.text[0]
>>> type(text)
<type 'bytes'> #: 'str' in Python 2

>>> f.trace[10] = np.zeros(len(f.samples))
```

More examples and recipes can be found in the docstrings `help(segyio)` and the
[examples](#examples) section.

"
13,segyio-README.md, Project goals ,"Segyio does not necessarily attempt to be the end-all of SEG-Y interactions;
rather, we aim to lower the barrier to interacting with SEG-Y files for
embedding, new applications or free-standing programs.

Additionally, the aim is not to support the full standard or all exotic (but
standard compliant) formatted files out there. Some assumptions are made, such
as:

 * All traces in a file are assumed to be of the same size

Currently, segyio supports:
 * Post-stack 3D volumes, sorted with respect to two header words (generally
   INLINE and CROSSLINE)
 * Pre-stack 4D volumes, sorted with respect to three header words (generally
   INLINE, CROSSLINE, and OFFSET)
 * Unstructured data, i.e. a collection of traces
 * Most numerical formats (including IEEE 4- and 8-byte float, IBM float, 2-
   and 4-byte integers)

The writing functionality in segyio is largely meant to *modify* or adapt
files. A file created from scratch is not necessarily a to-spec SEG-Y file, as
we only necessarily write the header fields segyio needs to make sense of the
geometry. It is still highly recommended that SEG-Y files are maintained and
written according to specification, but segyio **does not** enforce this.


"
14,segyio-README.md, SEG-Y Revisions ,"Segyio can handle a lot of files that are SEG-Y-like, i.e. segyio handles files
that don't strictly conform to the SEG-Y standard. Segyio also does not
discriminate between the revisions, but instead tries to use information
available in the file. For an *actual* standard's reference, please see the
publications by SEG:

- [SEG-Y 0 (1975)](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev0.pdf)
- [SEG-Y 1 (2002)](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev1.pdf)
- [SEG-Y 2 (2017)](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev2_0-mar2017.pdf)

"
15,segyio-README.md, Contributing ,"We welcome all kinds of contributions, including code, bug reports, issues,
feature requests, and documentation. The preferred way of submitting a
contribution is to either make an
[issue](https://github.com/equinor/segyio/issues) on github or by forking the
project on github and making a pull request.

"
16,segyio-README.md, xarray integration ,"[Alan Richardson](https://github.com/ar4) has written a great little tool for
using [xarray](http://xarray.pydata.org/en/stable/) with segy files, which he
demos in this
[notebook](https://github.com/ar4/netcdf_segy/blob/master/notebooks/netcdf_segy.ipynb)

"
17,segyio-README.md, Reproducing the test data ,"Small SEG-Y formatted files are included in the repository for test purposes.
The data is non-sensical and made to be predictable, and it is reproducible by
using segyio. The tests file are located in the test-data directory. To
reproduce the data file, build segyio and run the test program `make-file.py`,
`make-ps-file.py`, and `make-rotated-copies.py` as such:

```python
python examples/make-file.py small.sgy 50 1 6 20 25
python examples/make-ps-file.py small-ps.sgy 10 1 5 1 4 1 3
python examples/make-rotated-copies.py small.sgy
```

The small-lsb.sgy file was created by running the flip-endianness program. This
program is included in the segyio source tree, but not a part of the package,
and not intended for distribution and installation, only for reproducing test
files.

The seismic unix file small.su and small-lsb.su were created by the following
commands:

```bash
segyread tape=small.sgy ns=50 remap=tracr,cdp byte=189l,193l conv=1 format=1 \
         > small-lsb.su
suswapbytes < small.su > small-lsb.su
```

If you have have small data files with a free license, feel free to submit it
to the project!

"
19,segyio-README.md, Python ,"Import useful libraries:

```python
import segyio
import numpy as np
from shutil import copyfile
```

Open segy file and inspect it:

```python
filename = 'name_of_your_file.sgy'
with segyio.open(filename) as segyfile:

    #: Memory map file for faster reading (especially if file is big...)
    segyfile.mmap()

    #: Print binary header info
    print(segyfile.bin)
    print(segyfile.bin[segyio.BinField.Traces])

    #: Read headerword inline for trace 10
    print(segyfile.header[10][segyio.TraceField.INLINE_3D])

    #: Print inline and crossline axis
    print(segyfile.xlines)
    print(segyfile.ilines)
```

Read post-stack data cube contained in segy file:

```python
#: Read data along first xline
data = segyfile.xline[segyfile.xlines[1]]

#: Read data along last iline
data = segyfile.iline[segyfile.ilines[-1]]

#: Read data along 100th time slice
data = segyfile.depth_slice[100]

#: Read data cube
data = segyio.tools.cube(filename)
```

Read pre-stack data cube contained in segy file:

```python
filename = 'name_of_your_prestack_file.sgy'
with segyio.open(filename) as segyfile:

    #: Print offsets
    print(segyfile.offset)

    #: Read data along first iline and offset 100:  data [nxl x nt]
    data = segyfile.iline[0, 100]

    #: Read data along first iline and all offsets gath:  data [noff x nxl x nt]
    data = np.asarray([np.copy(x) for x in segyfile.iline[0:1, :]])

    #: Read data along first 5 ilines and all offsets gath:  data [noff nil x nxl x nt]
    data = np.asarray([np.copy(x) for x in segyfile.iline[0:5, :]])

    #: Read data along first xline and all offsets gath:  data [noff x nil x nt]
    data = np.asarray([np.copy(x) for x in segyfile.xline[0:1, :]])
```

Read and understand fairly 'unstructured' data (e.g., data sorted in common shot gathers):

```python
filename = 'name_of_your_prestack_file.sgy'
with segyio.open(filename, ignore_geometry=True) as segyfile:
    segyfile.mmap()

    #: Extract header word for all traces
    sourceX = segyfile.attributes(segyio.TraceField.SourceX)[:]

    #: Scatter plot sources and receivers color-coded on their number
    plt.figure()
    sourceY = segyfile.attributes(segyio.TraceField.SourceY)[:]
    nsum = segyfile.attributes(segyio.TraceField.NSummedTraces)[:]
    plt.scatter(sourceX, sourceY, c=nsum, edgecolor='none')

    groupX = segyfile.attributes(segyio.TraceField.GroupX)[:]
    groupY = segyfile.attributes(segyio.TraceField.GroupY)[:]
    nstack = segyfile.attributes(segyio.TraceField.NStackedTraces)[:]
    plt.scatter(groupX, groupY, c=nstack, edgecolor='none')
```

Write segy file using same header of another file but multiply data by *2

```python
input_file = 'name_of_your_input_file.sgy'
output_file = 'name_of_your_output_file.sgy'

copyfile(input_file, output_file)

with segyio.open(output_file, ""r+"") as src:

    #: multiply data by 2
    for i in src.ilines:
        src.iline[i] = 2 * src.iline[i]
```

[Make segy file from sctrach](python/examples/make-file.py)

"
20,segyio-README.md, MATLAB ,"```
filename='name_of_your_file.sgy'

% Inspect segy
Segy_struct=SegySpec(filename,189,193,1);

% Read headerword inline for each trace
Segy.get_header(filename,'Inline3D')

%Read data along first xline
data= Segy.readCrossLine(Segy_struct,Segy_struct.crossline_indexes(1));

%Read cube
data=Segy.get_cube(Segy_struct);

%Write segy, use same header but multiply data by *2
input_file='input_file.sgy';
output_file='output_file.sgy';
copyfile(input_file,output_file)
data = Segy.get_traces(input_file);
data1 = 2*data;
Segy.put_traces(output_file, data1);
```

"
22,segyio-README.md, ImportError: libsegyio.so.1: cannot open shared object file,"This error shows up when the loader cannot find the core segyio library. If
you've explicitly set the install prefix (with `-DCMAKE_INSTALL_PREFIX`) you
must configure your loader to also look in this prefix, either with a
`ld.conf.d` file or the `LD_LIBRARY_PATH` variable.

If you haven't set `CMAKE_INSTALL_PREFIX`, cmake will by default install to
`/usr/local`, which your loader usually knows about. On Debian based systems,
the library often gets installed to `/usr/local/lib`, which the loader may not
know about. See [issue #239](https://github.com/equinor/segyio/issues/239).

"
23,segyio-README.md, Possible solutions,"* Configure the loader (`sudo ldconfig` often does the trick)
* Install with a different, known prefix, e.g. `-DCMAKE_INSTALL_LIBDIR=lib64`

"
24,segyio-README.md, RuntimeError: unable to find sorting,"This exception is raised when segyio tries to open the in strict mode, under
the assumption that the file is a regular, sorted 3D volume. If the file is
just a collection of traces in arbitrary order, this would fail.

"
25,segyio-README.md, Possible solutions,"Segyio supports files that are just a collection of traces too, but has to be
told that it's ok to do so. Pass `strict = False` or `ignore_geometry = True`
to `segyio.open` to allow or force unstructured mode respectively. Please note
that `f.iline` and similar features are now disabled and will raise errors.

"
26,segyio-README.md, History ,"Segyio was initially written and is maintained by [Equinor
ASA](http://www.equinor.com/) as a free, simple, easy-to-use way of interacting
with seismic data that can be tailored to our needs, and as contribution to the
free software community.
"
0,ipyleaflet-README.md, ipyleaflet,"[![Documentation](http://readthedocs.org/projects/ipyleaflet/badge/?version=latest)](https://ipyleaflet.readthedocs.io/en/latest/?badge=latest)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyter-widgets/ipyleaflet/stable?filepath=examples)
[![Join the Gitter Chat](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jupyter-widgets/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

A Jupyter / Leaflet bridge enabling interactive maps in the Jupyter notebook.

"
1,ipyleaflet-README.md, Usage,"**Selecting a basemap for a leaflet map:**

![Basemap Screencast](basemap.gif)

**Loading a geojson map:**

![GeoJSON Screencast](geojson.gif)

**Making use of leafletjs primitives:**

![Primitives Screencast](primitives.gif)

**Using the splitmap control:**

![Splitmap Screencast](splitmap.gif)

**Displaying velocity data on the top of a map:**

![Velocity Screencast](velocity.gif)

**Choropleth layer:**

![Choropleth Screencast](choropleth.gif)

"
2,ipyleaflet-README.md, Installation,"Using conda:

```
$ conda install -c conda-forge ipyleaflet
```

Using pip:

```
$ pip install ipyleaflet
$ jupyter nbextension enable --py --sys-prefix ipyleaflet  #: can be skipped for
notebook 5.3 and above
```

If you have JupyterLab, you will also need to install the JupyterLab extension:

```
$ jupyter labextension install jupyter-leaflet
```

Some users have found that the ``jupyterlab-manager`` is also required
in jupyterlab if the map does not display.

```
$ jupyter labextension install @jupyter-widgets/jupyterlab-manager
```

"
3,ipyleaflet-README.md, Installation from sources,"For a development installation (requires npm):

```
$ git clone https://github.com/jupyter-widgets/ipyleaflet.git
$ cd ipyleaflet
$ pip install -e .
$ jupyter nbextension install --py --symlink --sys-prefix ipyleaflet
$ jupyter nbextension enable --py --sys-prefix ipyleaflet
$ jupyter labextension install js  #: If you are developing on JupyterLab
```

Note for developers:

- the ``-e`` pip option allows one to modify the Python code in-place. Restart the kernel in order to see the changes.
- the ``--symlink`` argument on Linux or OS X allows one to modify the JavaScript code in-place. This feature is not available with Windows.

    For automatically building the JavaScript code every time there is a change, run the following command from the ``ipyleaflet/js/`` directory:

    ```
    $ npm run watch
    ```

    If you are on JupyterLab you also need to run the following in a separate terminal:

    ```
    $ jupyter lab --watch
    ```

    Every time a JavaScript build has terminated you need to refresh the Notebook page in order to load the JavaScript code again.

"
4,ipyleaflet-README.md, Documentation,"To get started with using `ipyleaflet`, check out the full documentation

https://ipyleaflet.readthedocs.io/

"
5,ipyleaflet-README.md, License,"We use a shared copyright model that enables all contributors to maintain the
copyright on their contributions.

This software is licensed under the BSD-3-Clause license. See the [LICENSE](LICENSE) file for details.

"
6,ipyleaflet-README.md, Related projects,"The `ipyleaflet` repository includes the `jupyter-leaflet` npm package, which
is a front-end component, and the `ipyleaflet` python package which is the
backend for the Python Jupyter kernel.

Similarly, the [`xleaflet`](https://github.com/QuantStack/xleaflet/) project
provides a backend to `jupyter-leaflet` for the ""xeus-cling"" C++ Jupyter
kernel.

![Xleaflet Screencast](xleaflet.gif)
"
0,omfvista-README.md, Grab a few elements of interest and plot em up!,"    vol = project['Block Model']
    assay = project['wolfpass_WP_assay']
    topo = project['Topography']
    dacite = project['Dacite']

Then apply a filtering tool from PyVista_ to the volumetric data:

.. code-block:: python

    thresher = pv.Threshold(vol)

.. figure:: https://github.com/OpenGeoVis/omfvista/raw/master/assets/threshold.gif
   :alt: IPython Thresholding Tool

Then you can put it all in one environment!

.. code-block:: python

    "
1,omfvista-README.md, Grab the active plotting window,    
2,omfvista-README.md,  from the thresher tool,"    p = thresher.plotter
    "
3,omfvista-README.md, Add our datasets,"    p.add_mesh(topo, cmap='gist_earth', opacity=0.5)
    p.add_mesh(assay, color='blue', line_width=3)
    p.add_mesh(dacite, color='yellow', opacity=0.6)
    "
4,omfvista-README.md, Add the bounds axis,"    p.show_bounds()


.. figure:: https://github.com/OpenGeoVis/omfvista/raw/master/assets/interactive.gif
   :alt: Interactive Rendering


And once you like what the render view displays, you can save a screenshot:

.. code-block:: python

    p.screenshot('wolfpass.png')

.. image:: https://github.com/OpenGeoVis/omfvista/raw/master/wolfpass.png
   :alt: Wolf Pass Screenshot
"
0,PVGeo-README.md, Demonstrations of *PVGeo*,"For a quick overview of how  *PVGeo* can be used in a Python environment or
directly within ParaView, checkout the code snippets and videos on the
[**About Examples Page**](https://pvgeo.org/about-examples.html)



"
1,PVGeo-README.md, Connections,"This package provides many VTK-like algorithms designed for geoscientific data
formats and types to perform data integration and analysis.
To ensure our users have powerful and easy to use tools that can visualize the
results of PVGeo algorithms, we are actively involved in the development of
[**PyVista**](https://github.com/pyvista/pyvista): a toolset for easy access to
VTK data objects and 3D visualization in Python.
To learn more about pairing PVGeo with PyVista, please check out the
[**example Jupyter notebooks**](https://github.com/OpenGeoVis/PVGeo-Examples).


"
2,PVGeo-README.md, Getting Started,"To begin using the *PVGeo* Python package, create/activate your Python virtual
environment (we highly recommend using anaconda) and install *PVGeo* through pip:

```bash
pip install PVGeo
```

Now *PVGeo* is ready for use in your standard Python environment (2.7 or >=3.6)
with all dependencies installed! Go ahead and test your install:

```bash
python -c ""import PVGeo; print(PVGeo.__version__)""
```

Note that Windows users must use Python >=3.6 when outside of ParaView.
Further insight can be found in the [**Getting Started Guide**](http://pvgeo.org/overview/getting-started.html).


"
3,PVGeo-README.md, Report Issues and Contribute,"Please feel free to post features you would like to see from this package on the
[**issues page**](https://github.com/OpenGeoVis/PVGeo/issues) as a feature
request.
If you stumble across any bugs or crashes while using code distributed here,
report them in the issues section so we can promptly address it.
For other questions, join the [***PVGeo* community on Slack**](http://slack.pvgeo.org).

Interested in contributing to PVGeo? Please see the [contributing guide](https://pvgeo.org/dev-guide/contributing.html)

"
4,PVGeo-README.md, About the Authors [![Open Source](https://img.shields.io/badge/open--source-yes-brightgreen.svg)](https://opensource.com/resources/what-open-source),"The *PVGeo* code library was created and is managed by [**Bane Sullivan**](http://banesullivan.com),
graduate student in the Hydrological Science and Engineering interdisciplinary
program at the Colorado School of Mines under Whitney Trainor-Guitton.
If you would like to contact us, inquire with [**info@pvgeo.org**](mailto:info@pvgeo.org).

It is important to note the project is open source and that many features in
this repository were made possible by contributors volunteering their time.
Head over to the [**Contributors Page**](https://github.com/OpenGeoVis/PVGeo/graphs/contributors)
to learn more about the developers of *PVGeo*.



"
5,PVGeo-README.md, Linking PVGeo to ParaView,"To use the *PVGeo* library as plugins in ParaView, please see the detailed
explanation [**here**](http://pvgeo.org/overview/getting-started) where you
must create a second isolated Python 2.7 environment that will host PVGeo for
ParaView.
"
0,readgssi-README.md, readgssi,"*Copyleft 🄯 2017-2019*

![Example Radargram](https://github.com/iannesbitt/readgssi/raw/master/examples/main.png)

[![PyPI version](https://img.shields.io/pypi/v/readgssi.svg?colorB=limegreen&label=pypi%20package)](https://badge.fury.io/py/readgssi)
[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.1439119.svg)](https://dx.doi.org/10.5281/zenodo.1439119)
[![License](https://img.shields.io/badge/license-GNU%20Affero%203.0-lightgrey.svg)](https://github.com/iannesbitt/readgssi/blob/master/LICENSE)
[![Build Status](https://travis-ci.org/iannesbitt/readgssi.svg?branch=master)](https://travis-ci.org/iannesbitt/readgssi)
[![Downloads per month](https://img.shields.io/pypi/dm/readgssi.svg)](https://pypi.org/project/readgssi/)

`readgssi` is a tool intended for use as an open-source reader and preprocessing module for subsurface data collected with Geophysical Survey Systems Incorporated (GSSI) ground-penetrating georadar (GPR) devices. It has the capability to read DZT and DZG files with the same pre-extension name and plot the data contained in those files. `readgssi` is also currently able to translate most DZT files to CSV and will be able to translate to other output formats including HDF5  (see [future](#future)). Matlab code donated by [Gabe Lewis](https://earthsciences.dartmouth.edu/people/gabriel-lewis), Dartmouth College Department of Earth Sciences. Python adaptation written with permission by Ian Nesbitt, University of Maine School of Earth and Climate Sciences.

The file read parameters are based on GSSI's DZT file description, similar to the ones available on pages 55-57 of the [SIR-3000 manual](https://support.geophysical.com/gssiSupport/Products/Documents/Control%20Unit%20Manuals/GSSI%20-%20SIR-3000%20Operation%20Manual.pdf). File structure is, unfortunately, prone to change at any time, and although I've been able to test with files from several systems, I have not encountered every iteration of file header yet. If you run into trouble, please [create a github issue](https://github.com/iannesbitt/readgssi/issues).

Questions, feature requests, and bugs: please [open a github issue](https://github.com/iannesbitt/readgssi/issues). Kindly provide the error output, describe what you are attempting to do, and attach the DZT/DZG file(s) causing you trouble.

"
1,readgssi-README.md, requirements,"Strongly recommended to install via [anaconda](https://www.anaconda.com/download):
- [`obspy`](https://obspy.org/)
- [`matplotlib`](https://matplotlib.org/)
- [`numpy`](http://www.numpy.org/)
- [`pandas`](https://pandas.pydata.org/)
- [`h5py`](https://www.h5py.org/)

Install via `pip`:
- [`pynmea2`](https://pypi.org/project/pynmea2/)
- [`geopy`](https://pypi.org/project/geopy/)
- [`pytz`](https://pypi.org/project/pytz/)

"
2,readgssi-README.md, installation,"Once you have [anaconda](https://www.anaconda.com/download) running, installing requirements is pretty easy.

```bash
conda config --add channels conda-forge
conda create -n readgssi python==3.7 pandas h5py pytz obspy
conda activate readgssi
pip install readgssi
```

That should allow you to run the commands below.

"
3,readgssi-README.md, installing from source:,"If you choose to install a specific commit rather than the [latest working release of this software](https://pypi.org/project/readgssi), you may download this package, unzip to your home folder, open a command line, then install in the following way:

```bash
pip install ~/readgssi
```

"
4,readgssi-README.md, usage,"To display the help text:

```bash
$ readgssi -h

usage:
readgssi -i input.DZT [OPTIONS]

optional flags:
     OPTION     |      ARGUMENT       |       FUNCTIONALITY
-o, --output    | file:  /dir/f.ext   |  specify an output file
-f, --format    | string, eg. ""csv""   |  specify output format (csv is the only working format currently)
-p, --plot      | +integer or ""auto""  |  plot will be x inches high (dpi=150), or ""auto"". default: 10
-x, --xscale    | string, eg. ""dist""  |  readgssi will attempt to convert the x-axis to distance, time, or traces based on header values
-z, --zscale    | string, eg. ""time""  |  readgssi will attempt to convert the x-axis to depth, time, or samples based on header values
-n, --noshow    |                     |  suppress matplotlib popup window and simply save a figure (useful for multiple file processing)
-c, --colormap  | string, eg. ""Greys"" |  specify the colormap (https://matplotlib.org/users/colormaps.html#:grayscale-conversion)
-g, --gain      | positive (+)integer |  gain value (higher=greater contrast, default: 1)
-r, --bgr       |                     |  horizontal background removal algorithm (useful to remove ringing)
-R, --reverse   |                     |  reverse (flip radargram horizontally)
-w, --dewow     |                     |  trinomial dewow algorithm
-t, --bandpass  | +int-+int (MHz)     |  butterworth bandpass filter (positive integer range in megahertz; ex. 100-145)
-b, --colorbar  |                     |  add a colorbar to the radar figure
-a, --antfreq   | positive integer    |  specify antenna frequency (read automatically if not given)
-s, --stack     | +integer or ""auto""  |  specify trace stacking value or ""auto"" to autostack to ~2.5:1 x:y axis ratio
-N, --normalize |                     |  reads a .DZG NMEA data if it exists; otherwise tries to read a csv file with lat, lon, and time fields to distance normalize with
-d, --spm       | positive float      |  specify the samples per meter (spm) manually. overrides header value.
-m, --histogram |                     |  produce a histogram of data values
-E, --epsr      | float > 1.0         |  user-defined epsilon sub r (sometimes referred to as ""dielectric""; ignores value in DZT header)
-Z, --zero      | positive integer    |  skip this many samples from the top of the trace downward (useful for removing transceiver delay)

naming scheme for exports:
   CHARACTERS   |      MEANING
    c0          |  Profile from channel 0 (can range from 0 - 3)
    Dn          |  Distance normalization
    Tz233       |  Time zero at 233 samples
    S8          |  Stacked 8 times
    Rv          |  Profile read in reverse (flipped horizontally)
    Bgr         |  Background removal filter
    Dw          |  Dewow filter
    Bp100-145   |  2-corner bandpass filter applied from 100 to 145 MHz
    G30         |  30x contrast gain
```

From a unix command line:
```bash
readgssi -i DZT__001.DZT
```
Simply specifying an input DZT file like in the above command (`-i file`) will display a host of data about the file including:
- name of GSSI control unit
- antenna model
- antenna frequency
- samples per trace
- bits per sample
- traces per second
- L1 dielectric as entered during survey
- sampling depth
- speed of light at given dielectric
- number of traces
- number of seconds

"
6,readgssi-README.md, CSV output,"```bash
readgssi -i DZT__001.DZT -o test.csv -f CSV
```
Translates radar data array to CSV format, if that's your cup of tea. One might use this to export to Matlab. One CSV will be written per channel. The script will rename the output to 'test_100MHz.csv' automatically. No header information is included in the CSV.

```bash
readgssi -i DZT__001.DZT -s 8 -w -r -o test.csv -f CSV
```
Applies 8x stacking, dewow, and background removal filters before exporting to CSV.

"
8,readgssi-README.md, example 1A,"```bash
readgssi -i DZT__001.DZT -p 5 -s auto -c viridis -m
```
The above command will cause `readgssi` to save and show a plot named ""DZT__001_100MHz.png"" with a y-size of 6 inches at 150 dpi (`-p 6`) and the autostacking algorithm will stack the x-axis to some multiple of times shorter than the original data array for optimal viewing on a monitor, approximately 2.5\*y (`-s auto`). The plot will be rendered in the viridis color scheme, which is the default for matplotlib. The `-m` flag will draw a histogram for each data channel.
![Example 1a](https://github.com/iannesbitt/readgssi/raw/master/examples/1a.png)
![Example 1a histogram](https://github.com/iannesbitt/readgssi/raw/master/examples/1a-h.png)

"
9,readgssi-README.md, example 1B,"```bash
readgssi -i DZT__001.DZT -o 1b.png -p 5 -s auto -c viridis -g 50 -m -r -w
```
This will cause `readgssi` to create a plot from the same file, but matplotlib will save the plot as ""1b.png"" (`-o 1b.png`). The script will plot the y-axis size (`-p 5`) and automatically stack the x-axis to (`-s auto`). The script will plot the data with a gain value of 50 (`-g 50`), which will increase the plot contrast by a factor of 50. Next `readgssi` will run the background removal (`-r`) and dewow (`-w`) filters. Finally, the `-m` flag will draw a histogram for each data channel. Note how the histogram changes when filters are applied.
![Example 1b](https://github.com/iannesbitt/readgssi/raw/master/examples/1b.png)
![Example 1b histogram](https://github.com/iannesbitt/readgssi/raw/master/examples/1b-h.png)

"
10,readgssi-README.md, example 1C: gain can be tricky depending on your colormap,"```bash
readgssi -i DZT__001.DZT -o 1c.png -p 5 -s auto -r -w -c seismic
```
Here, background removal and dewow filters are applied, but no gain adjustments are made (equivalent to `-g 1`). The script uses matplotlib's ""seismic"" colormap (`-c seismic`) which is specifically designed for this type of waterfall array plotting. Even without gain, you will often be able to easily see very slight signal perturbations. It is not colorblind-friendly for either of the two most common types of human colorblindness, however, which is why it is not the default colormap.
![Example 1c](https://github.com/iannesbitt/readgssi/raw/master/examples/1c.png)

"
11,readgssi-README.md, example 2A: no background removal,"```bash
readgssi -i DZT__002.DZT -o 2a.png -p 10 -s 3 -n
```
Here `readgssi` will create a plot of size 10 and stack 3x (`-p 10 -s 3`). Matplotlib will use the default ""Greys"" colormap and save a PNG of the figure, but the script will suppress the matplotlib window (`-n`, useful for processing an entire directory full of DZTs at once).
![Example 2a](https://github.com/iannesbitt/readgssi/raw/master/examples/2a.png)

"
12,readgssi-README.md, example 2B: horizontal mean BGR algorithm applied,"```bash
readgssi -i DZT__002.DZT -o 2b.png -p 10 -s 3 -n -r
```
The script does the same thing, except it applies horizontal mean background removal `-r`. Note the difference in ringing artifacts between examples 2a and 2b.
![Example 2b](https://github.com/iannesbitt/readgssi/raw/master/examples/2b.png)


"
13,readgssi-README.md, contributors,"- Ian Nesbitt ([@iannesbitt](https://github.com/iannesbitt), author)
- François-Xavier Simon ([@fxsimon](https://github.com/fxsimon))
- Thomas Paulin ([@thomaspaulin](https://github.com/thomaspaulin))

"
14,readgssi-README.md, citation suggestion:,"Ian M. Nesbitt, François-Xavier Simon, Thomas Paulin, 2018. readgssi - an open-source tool to read and plot GSSI ground-penetrating radar data. [doi:10.5281/zenodo.1439119](https://dx.doi.org/10.5281/zenodo.1439119)

"
15,readgssi-README.md, known bugs:,"- color bar shows up too large on some plots (matplotlib bug)

"
16,readgssi-README.md, future,"- explicit documentation
- automatic script testing for smoother dev
- create a class for surveyline objects, similar to [`obspy.core.trace.Trace`](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.html)
- GPS transcription from CSV with fields like `mark name, lon, lat, elev, time`
- Use GPS altitude to adjust z position across profile
- GUI-based geologic/dielectric layer picking
  - layer velocity calculation (using minimum of clustered hyperbola tail angle measurements, or manual input)
  - velocity-based depth adjustments
  - ability to incorporate ground truth measurements
- velocity gradient/angle of incidence-based array migration
- translation to common geophysical formats (HDF5, SEGY, etc.)
- integration with [`vista`](https://docs.pyvista.org) for 3D visualization of location-aware arrays
"
0,RESCAN-README.md, RESCAN: Recurrent Squeeze-and-Excitation Context Aggregation Net,"Xia Li, Jianlong Wu, [Zhouchen Lin][2], [Hong Liu][3], [Hongbin Zha][4]<br>

Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University<br>
Key Laboratory of Machine Perception (MOE), School of EECS, Peking University<br>
Cooperative Medianet Innovation Center, Shanghai Jiao Tong University<br>
{[ethanlee][5], [jlwu1992][6], [zlin][7], [hongliu][8]}@pku.edu.cn, zha@cis.pku.edu.cn

Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics.

Paper Link: http://openaccess.thecvf.com/content_ECCV_2018/papers/Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper.pdf

"
1,RESCAN-README.md, Prerequisite,"- Python>=3.6
- Pytorch>=4.1.0
- Opencv>=3.1.0
- tensorboardX

"
2,RESCAN-README.md, Project Structure,"- config: contains all codes
    - cal_ssim.py
    - clean.sh
    - dataset.py
    - main.py
    - model.py
    - settings.py
    - show.py
    - tensorboard.sh
- explore.sh
- logdir: holds patches generated in training process
- models: holds checkpoints
- showdir: holds images predicted by the model

"
3,RESCAN-README.md, Best Practices,"Hold every experiment in an independent folder, and assign a long name to it.
We recommend list the important parameters in the folder name, for example: RESCAN.ConvRNN.Full.d_7.c_24(d: depth, c: channel).

"
4,RESCAN-README.md, Default Dataset settings,"Rain100H: [http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html][9]<br>
Rain800: [https://drive.google.com/drive/folders/0Bw2e6Q0nQQvGbi1xV1Yxd09rY2s][10]

We concatenate the two images(B and O) together as default inputs. If you want to change this setting, just modify config/dataset.py.
Moreover, there should be three folders 'train', 'val', 'test' in the dataset folder.
After download the datasets, don't forget to transform the format!

Update: Rain100H has updated its testing set, from origin 100 images(test100) to 200(test200) images. We update the performance of RESCAN + GRU as follow:

|         | PSNR  | SSIM  |
| :------:| :---: | :---: |
| test100 | 26.45 | 0.8458 |
| test200 | 25.92 | 0.8411 |

"
5,RESCAN-README.md," Train, Test and Show","    python train.py
    python eval.py
    python show.py

"
6,RESCAN-README.md, Scripts,"- explore.sh: Show the predicted images in browser
- config/tensorboard.sh: Open the tensorboard server
- config/clean.sh: Clear all the training records in the folder

"
7,RESCAN-README.md, Cite,"If you use our code, please refer this repo.
If you publish your paper that refer to our paper, please cite:

    @inproceedings{li2018recurrent,  
        title={Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining},  
        author={Li, Xia and Wu, Jianlong and Lin, Zhouchen and Liu, Hong and Zha, Hongbin},  
        booktitle={European Conference on Computer Vision},  
        pages={262--277},  
        year={2018},  
        organization={Springer}  
    }


  [2]: http://cis.pku.edu.cn/faculty/vision/zlin/zlin.htm
  [3]: http://robotics.pkusz.edu.cn/team/leader/
  [4]: http://cis.pku.edu.cn/vision/Visual&Robot/people/zha/
  [5]: ethanlee@pku.edu.cn
  [6]: jlwu1992@pku.edu.cn
  [7]: zlin@pku.edu.cn
  [8]: hongliu@pku.edu.cn
  [9]: http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html
  [10]: https://drive.google.com/drive/folders/0Bw2e6Q0nQQvGbi1xV1Yxd09rY2s
  
"
8,RESCAN-README.md, Recent Works,"- [SPANet](https://github.com/stevewongv/SPANet) from Tianyu Wang
- [HeavyRainRemoval](https://github.com/liruoteng/HeavyRainRemoval) from Ruoteng Li
"
0,Detectron-README.md, Detectron,"Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.

At FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440), [DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434), and [Group Normalization](https://arxiv.org/abs/1803.08494).

<div align=""center"">
  <img src=""demo/output/33823288584_1d21cf0a26_k_example_output.jpg"" width=""700px"" />
  <p>Example Mask R-CNN output.</p>
</div>

"
1,Detectron-README.md, Introduction,"The goal of Detectron is to provide a high-quality, high-performance
codebase for object detection *research*. It is designed to be flexible in order
to support rapid implementation and evaluation of novel research. Detectron
includes implementations of the following object detection algorithms:

- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*
- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*
- [Faster R-CNN](https://arxiv.org/abs/1506.01497)
- [RPN](https://arxiv.org/abs/1506.01497)
- [Fast R-CNN](https://arxiv.org/abs/1504.08083)
- [R-FCN](https://arxiv.org/abs/1605.06409)

using the following backbone network architectures:

- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)
- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)
- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)
- [VGG16](https://arxiv.org/abs/1409.1556)

Additional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below.

"
2,Detectron-README.md, Update,"- 4/2018: Support Group Normalization - see [`GN/README.md`](./projects/GN/README.md)

"
3,Detectron-README.md, License,"Detectron is released under the [Apache 2.0 license](https://github.com/facebookresearch/detectron/blob/master/LICENSE). See the [NOTICE](https://github.com/facebookresearch/detectron/blob/master/NOTICE) file for additional details.

"
4,Detectron-README.md, Citing Detectron,"If you use Detectron in your research or wish to refer to the baseline results published in the [Model Zoo](MODEL_ZOO.md), please use the following BibTeX entry.

```
@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}
```

"
5,Detectron-README.md, Model Zoo and Baselines,"We provide a large set of baseline results and trained models available for download in the [Detectron Model Zoo](MODEL_ZOO.md).

"
6,Detectron-README.md, Installation,"Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).

"
7,Detectron-README.md, Quick Start: Using Detectron,"After installation, please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.

"
8,Detectron-README.md, Getting Help,"To start, please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.

If bugs are found, **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.

"
9,Detectron-README.md, References,"- [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440).
  Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He.
  Tech report, arXiv, Dec. 2017.
- [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370).
  Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick.
  Tech report, arXiv, Nov. 2017.
- [Non-Local Neural Networks](https://arxiv.org/abs/1711.07971).
  Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
  Tech report, arXiv, Nov. 2017.
- [Mask R-CNN](https://arxiv.org/abs/1703.06870).
  Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
  IEEE International Conference on Computer Vision (ICCV), 2017.
- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002).
  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
  IEEE International Conference on Computer Vision (ICCV), 2017.
- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).
  Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
  Tech report, arXiv, June 2017.
- [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333).
  Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He.
  Tech report, arXiv, Apr. 2017.
- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144).
  Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431).
  Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
- [R-FCN: Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409).
  Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.
  Conference on Neural Information Processing Systems (NIPS), 2016.
- [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).
  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)
  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
  Conference on Neural Information Processing Systems (NIPS), 2015.
- [Fast R-CNN](http://arxiv.org/abs/1504.08083).
  Ross Girshick.
  IEEE International Conference on Computer Vision (ICCV), 2015.
"
0,facebookresearch-pyrobot-README.md, What can you do with PyRobot?,"<p align=""center"">
    <img src=""https://thumbs.gfycat.com/FickleSpeedyChimneyswift-size_restricted.gif"", height=""180"">
    <img src=""https://thumbs.gfycat.com/FinishedWeirdCockerspaniel-size_restricted.gif"", height=""180"">
    <img src=""https://thumbs.gfycat.com/WeightyLeadingGrub-size_restricted.gif"", height=""180"">
</p>

"
2,facebookresearch-pyrobot-README.md, Installing both PyRobot and LoCoBot dependencies,"* Install **Ubuntu 16.04**

* Download the installation script
```bash
sudo apt update
sudo apt-get install curl
curl 'https://raw.githubusercontent.com/facebookresearch/pyrobot/master/robots/LoCoBot/install/locobot_install_all.sh' > locobot_install_all.sh
```

* Run the script to install everything (ROS, realsense driver, etc.). **Please connect the nuc machine to a realsense camera before running the following commands**.
```bash
chmod +x locobot_install_all.sh 
./locobot_install_all.sh
```

"
3,facebookresearch-pyrobot-README.md, Installing just PyRobot,"* Install **Ubuntu 16.04** 

* Install [ROS kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu)

* Install KDL

```bash
sudo apt-get -y install ros-kinetic-orocos-kdl ros-kinetic-kdl-parser-py ros-kinetic-python-orocos-kdl ros-kinetic-trac-ik
```

* Install Python virtual environment

```bash
sudo apt-get -y install python-virtualenv
virtualenv_name=""pyenv_pyrobot""
VIRTUALENV_FOLDER=~/${virtualenv_name}
virtualenv --system-site-packages -p python2.7 $VIRTUALENV_FOLDER
```

* Install PyRobot 

```bash
cd ~
mkdir -p low_cost_ws/src
cd ~/low_cost_ws/src
source ~/${virtualenv_name}/bin/activate
git clone --recurse-submodules https://github.com/facebookresearch/pyrobot.git
cd pyrobot/
pip install .
```

**Warning**: As realsense keeps updating, compatibility issues might occur if you accidentally update 
realsense-related packages from `Software Updater` in ubuntu. Therefore, we recommend you not to update
any libraries related to realsense. Check the list of updates carefully when ubuntu prompts software udpates.

"
4,facebookresearch-pyrobot-README.md, Getting Started,"Please refer to [pyrobot.org](https://pyrobot.org/) and [locobot.org](http://locobot.org)

"
5,facebookresearch-pyrobot-README.md, The Team,"[Adithya Murali](http://adithyamurali.com/), [Tao Chen](https://taochenshh.github.io), [Dhiraj Gandhi](http://www.cs.cmu.edu/~dgandhi/), Kalyan Vasudev, [Lerrel Pinto](http://www.cs.cmu.edu/~lerrelp/), [Saurabh Gupta](http://saurabhg.web.illinois.edu) and [Abhinav Gupta](http://www.cs.cmu.edu/~abhinavg/). We would also like to thank everyone who has helped PyRobot in any way.

"
6,facebookresearch-pyrobot-README.md, Future features,"We are planning several features, namely:
* Interfacing with other simulators like [AI Habitat](https://aihabitat.org)
* Gravity compensation
* PyRobot interface for [UR5](https://www.universal-robots.com)

"
7,facebookresearch-pyrobot-README.md, Citation,"```
@article{pyrobot2019,
  title={PyRobot: An Open-source Robotics Framework for Research and Benchmarking},
  author={Adithyavairavan Murali and Tao Chen and Kalyan Vasudev Alwala and Dhiraj Gandhi and Lerrel Pinto and Saurabh Gupta and Abhinav Gupta},
  journal={arXiv preprint arXiv:1906.08236},
  year={2019}
}
```
"
8,facebookresearch-pyrobot-README.md, License,"PyRobot is under MIT license, as found in the LICENSE file.
"
0,pymeshfix-README.md, Test of pymeshfix without VTK module,"    examples.native()

    "
1,pymeshfix-README.md, Performs same mesh repair while leveraging VTK's plotting/mesh loading,"    examples.with_vtk()


Easy Example
------------
This example uses the Cython wrapper directly. No bells or whistles here:

.. code:: python

    from pymeshfix import _meshfix

    "
2,pymeshfix-README.md, Read mesh from infile and output cleaned mesh to outfile,"    _meshfix.clean_from_file(infile, outfile)


This example assumes the user has vertex and faces arrays in Python.

.. code:: python

    from pymeshfix import _meshfix

    "
3,pymeshfix-README.md, Generate vertex and face arrays of cleaned mesh,    
4,pymeshfix-README.md, where v and f are numpy arrays or python lists,"    vclean, fclean = _meshfix.clean_from_arrays(v, f)


Complete Examples with and without VTK
--------------------------------------

One of the main reasons to bring MeshFix to Python is to allow the library to
communicate to other python programs without having to use the hard drive.
Therefore, this example assumes that you have a mesh within memory and wish to
repair it using MeshFix.

.. code:: python

    import pymeshfix

    "
5,pymeshfix-README.md, Create object from vertex and face arrays,"    meshfix = pymeshfix.MeshFix(v, f)

    "
6,pymeshfix-README.md, Plot input,"    meshfix.plot()

    "
7,pymeshfix-README.md, Repair input mesh,"    meshfix.repair()

    "
8,pymeshfix-README.md, Access the repaired mesh with vtk,"    mesh = meshfix.mesh

    "
9,pymeshfix-README.md," Or, access the resulting arrays directly from the object",    meshfix.v 
10,pymeshfix-README.md, numpy np.float array,    meshfix.f 
11,pymeshfix-README.md, numpy np.int32 array,    
12,pymeshfix-README.md, View the repaired mesh (requires vtkInterface),"    meshfix.plot()

    "
13,pymeshfix-README.md, Save the mesh,"    meshfix.write('out.ply')

Alternatively, the user could use the Cython wrapper of MeshFix directly if
vtk is unavailable or they wish to have more control over the cleaning
algorithm.

.. code:: python

    from pymeshfix import _meshfix

    "
14,pymeshfix-README.md, Create TMesh object,"    tin = _meshfix.PyTMesh()

    tin.LoadFile(infile)
    "
15,pymeshfix-README.md," tin.load_array(v, f)  or read arrays from memory",    
16,pymeshfix-README.md, Attempt to join nearby components,    
17,pymeshfix-README.md, tin.join_closest_components(),    
18,pymeshfix-README.md, Fill holes,"    tin.fill_small_boundaries()
    print('There are {:d} boundaries'.format(tin.boundaries())

    "
19,pymeshfix-README.md, Clean (removes self intersections),"    tin.clean(max_iters=10, inner_loops=3)

    "
20,pymeshfix-README.md, Check mesh for holes again,"    print('There are {:d} boundaries'.format(tin.boundaries())

    "
21,pymeshfix-README.md, Clean again if necessary...,    
22,pymeshfix-README.md, Output mesh,"    tin.save_file(outfile)

     "
23,pymeshfix-README.md, or return numpy arrays,"    vclean, fclean = tin.return_arrays()


Algorithm and Citation Policy
-----------------------------

To better understand how the algorithm works, please refer to the following
paper:

    M. Attene. A lightweight approach to repairing digitized polygon meshes.
    The Visual Computer, 2010. (c) Springer. DOI: 10.1007/s00371-010-0416-3

This software is based on ideas published therein. If you use MeshFix for
research purposes you should cite the above paper in your published results.
MeshFix cannot be used for commercial purposes without a proper licensing
contract.


Copyright
---------

MeshFix is Copyright(C) 2010: IMATI-GE / CNR

All rights reserved.

This program is dual-licensed as follows:

(1) You may use MeshFix as free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by the Free
Software Foundation; either version 3 of the License, or (at your option) any
later version.

In this case the program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License
(http://www.gnu.org/licenses/gpl.txt) for more details.

(2) You may use MeshFix as part of a commercial software. In this case a proper
agreement must be reached with the Authors and with IMATI-GE/CNR based on a
proper licensing contract.
"
0,facebookresearch-DensePose-README.md, DensePose: ,"**Dense Human Pose Estimation In The Wild**

_Rıza Alp Güler, Natalia Neverova, Iasonas Kokkinos_

[[`densepose.org`](https://densepose.org)] [[`arXiv`](https://arxiv.org/abs/1802.00434)] [[`BibTeX`](#CitingDensePose)]

Dense human pose estimation aims at mapping all human pixels of an RGB image to the 3D surface of the human body. 
DensePose-RCNN is implemented in the [Detectron](https://github.com/facebookresearch/Detectron) framework and is powered by [Caffe2](https://github.com/caffe2/caffe2).

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1qfSOkpueo1kVZbXOuQJJhyagKjMgepsz"" width=""700px"" />
</div>


In this repository, we provide the code to train and evaluate DensePose-RCNN. We also provide notebooks to visualize the collected DensePose-COCO dataset and show the correspondences to the SMPL model.

"
1,facebookresearch-DensePose-README.md, Installation,"Please find installation instructions for Caffe2 and DensePose in [`INSTALL.md`](INSTALL.md), a document based on the [Detectron](https://github.com/facebookresearch/Detectron) installation instructions.

"
2,facebookresearch-DensePose-README.md, Inference-Training-Testing,"After installation, please see [`GETTING_STARTED.md`](GETTING_STARTED.md)  for examples of inference and training and testing.

"
4,facebookresearch-DensePose-README.md, Visualization of DensePose-COCO annotations:,"See [`notebooks/DensePose-COCO-Visualize.ipynb`](notebooks/DensePose-COCO-Visualize.ipynb) to visualize the DensePose-COCO annotations on the images:

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1uYRJkIA24KkJU2i4sMwrKa61P0xtZzHk"" width=""800px"" />
</div>

---

"
5,facebookresearch-DensePose-README.md, DensePose-COCO in 3D:,"See [`notebooks/DensePose-COCO-on-SMPL.ipynb`](notebooks/DensePose-COCO-on-SMPL.ipynb) to localize the DensePose-COCO annotations on the 3D template ([`SMPL`](http://smpl.is.tue.mpg.de)) model:

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1m32oyMuE7AZd3EOf9k8zHpr75C8bHlYj"" width=""500px"" />
</div>

---
"
6,facebookresearch-DensePose-README.md, Visualize DensePose-RCNN Results:,"See [`notebooks/DensePose-RCNN-Visualize-Results.ipynb`](notebooks/DensePose-RCNN-Visualize-Results.ipynb) to visualize the inferred DensePose-RCNN Results.

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1k4HtoXpbDV9MhuyhaVcxDrXnyP_NX896"" width=""900px"" />
</div>

---
"
7,facebookresearch-DensePose-README.md, DensePose-RCNN Texture Transfer:,"See [`notebooks/DensePose-RCNN-Texture-Transfer.ipynb`](notebooks/DensePose-RCNN-Texture-Transfer.ipynb) to localize the DensePose-COCO annotations on the 3D template ([`SMPL`](http://smpl.is.tue.mpg.de)) model:

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1r-w1oDkDHYnc1vYMbpXcYBVD1-V3B4Le"" width=""900px"" />
</div>

"
8,facebookresearch-DensePose-README.md, License,"This source code is licensed under the license found in the [`LICENSE`](LICENSE) file in the root directory of this source tree.

"
9,facebookresearch-DensePose-README.md," <a name=""CitingDensePose""></a>Citing DensePose","If you use Densepose, please use the following BibTeX entry.

```
  @InProceedings{Guler2018DensePose,
  title={DensePose: Dense Human Pose Estimation In The Wild},
  author={R\{i}za Alp G\""uler, Natalia Neverova, Iasonas Kokkinos},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
  }
```


"
0,DaSiamRPN-README.md, DaSiamRPN,"This repository includes PyTorch code for reproducing the results on VOT2018.

[**Distractor-aware Siamese Networks for Visual Object Tracking**](https://arxiv.org/pdf/1808.06048.pdf)  

Zheng Zhu<sup>\*</sup>, Qiang Wang<sup>\*</sup>, Bo Li<sup>\*</sup>, Wei Wu, Junjie Yan, and Weiming Hu 

*European Conference on Computer Vision (ECCV), 2018*



"
1,DaSiamRPN-README.md, Introduction,"**SiamRPN** formulates the task of visual tracking as a task of localization and identification simultaneously, initially described in an [CVPR2018 spotlight paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf). (Slides at [CVPR 2018 Spotlight](https://drive.google.com/open?id=1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq))

**DaSiamRPN** improves the performances of SiamRPN by (1) introducing an effective sampling strategy to control the imbalanced sample distribution, (2) designing a novel distractor-aware module to perform incremental learning, (3) making a long-term tracking extension. [ECCV2018](https://arxiv.org/pdf/1808.06048.pdf). (Slides at [VOT-18 Real-time challenge winners talk](https://drive.google.com/open?id=1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr))

<div align=""center"">
  <img src=""votresult.png"" width=""700px"" />
</div>

"
2,DaSiamRPN-README.md, Prerequisites,"CPU: Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
GPU: NVIDIA GTX1060

- python2.7
- pytorch == 0.3.1
- numpy
- opencv


"
3,DaSiamRPN-README.md, Pretrained model for SiamRPN,"In our tracker, we use an AlexNet variant as our backbone, which is end-to-end trained for visual tracking.
The pretrained model can be downloaded from google drive: [SiamRPNBIG.model](https://drive.google.com/file/d/1-vNVZxfbIplXHrqMHiJJYWXYWsOIvGsf/view?usp=sharing).
Then, you should copy the pretrained model file `SiamRPNBIG.model` to the subfolder './code', so that the tracker can find and load the pretrained_model.


"
4,DaSiamRPN-README.md, Detailed steps to install the prerequisites,"- install pytorch, numpy, opencv following the instructions in the `run_install.sh`. Please do **not** use conda to install.
- you can alternatively modify `/PATH/TO/CODE/FOLDER/` in `tracker_SiamRPN.m` 
  If the tracker is ready, you will see the tracking results. (EAO: 0.3827)


"
5,DaSiamRPN-README.md, Results,"All results can be downloaded from [Google Drive](https://drive.google.com/drive/folders/1HJOvl_irX3KFbtfj88_FVLtukMI1GTCR?usp=sharing).

| | <sub>VOT2015</br>A / R / EAO</sub> | <sub>VOT2016</br>A / R / EAO</sub> | <sub>VOT2017 & VOT2018</br>A / R / EAO</sub> | <sub>OTB2015</br>OP / DP</sub> | <sub>UAV123</br>AUC / DP</sub> | <sub>UAV20L</br>AUC / DP</sub> |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| <sub> **SiamRPN** </br> CVPR2017 </sub> | <sub>0.58 / 1.13 / 0.349<sub> | <sub>0.56 / 0.26 / 0.344<sub> | <sub>0.49 / 0.46 / 0.244<sub> | <sub>81.9 / 85.0<sub> | <sub>0.527 / 0.748<sub> | <sub>0.454 / 0.617<sub> |
| <sub> **DaSiamRPN** </br> ECCV2018 </sub> | <sub>**0.63** / **0.66** / **0.446**<sub> | <sub>**0.61** / **0.22** / **0.411**<sub> | <sub>0.56 / 0.34 / 0.326<sub> | <sub>**86.5** / **88.0**<sub> | <sub>**0.586** / **0.796**<sub> | <sub>**0.617** / **0.838**<sub> |
| <sub> **DaSiamRPN** </br> VOT2018 </sub> | <sub>-<sub> | <sub>-<sub>  | <sub>**0.59** / **0.28** / **0.383**<sub> | <sub>-<sub> | <sub>-<sub> | <sub>-<sub> |


"
6,DaSiamRPN-README.md, Demo and Test on OTB2015,"<div align=""center"">
  <img src=""code/data/bag.gif"" width=""400px"" />
</div>

- To reproduce the reuslts on paper, the pretrained model can be downloaded from [Google Drive](https://drive.google.com/open?id=1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H): `SiamRPNOTB.model`. <br />
:zap: :zap: This model is the **fastest** (~200fps) Siamese Tracker with AUC of 0.655 on OTB2015. :zap: :zap: 

- You must download OTB2015 dataset (download [script](code/data/get_otb_data.sh)) at first.

A simple test example.

```
cd code
python demo.py
```

If you want to test the performance on OTB2015, please using the follwing command.

```
cd code
python test_otb.py
python eval_otb.py OTB2015 ""Siam*"" 0 1
```


"
7,DaSiamRPN-README.md, License,"Licensed under an MIT license.


"
8,DaSiamRPN-README.md, Citing DaSiamRPN,"If you find **DaSiamRPN** and **SiamRPN** useful in your research, please consider citing:

```
@inproceedings{Zhu_2018_ECCV,
  title={Distractor-aware Siamese Networks for Visual Object Tracking},
  author={Zhu, Zheng and Wang, Qiang and Bo, Li and Wu, Wei and Yan, Junjie and Hu, Weiming},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@InProceedings{Li_2018_CVPR,
  title = {High Performance Visual Tracking With Siamese Region Proposal Network},
  author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2018}
}
```
"
0,hmr-README.md, End-to-end Recovery of Human Shape and Pose,"Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Jitendra Malik
CVPR 2018

[Project Page](https://akanazawa.github.io/hmr/)
![Teaser Image](https://akanazawa.github.io/hmr/resources/images/teaser.png)

"
1,hmr-README.md, Requirements,"- Python 2.7
- [TensorFlow](https://www.tensorflow.org/) tested on version 1.3, demo alone runs with TF 1.12

"
3,hmr-README.md, Linux Setup with virtualenv,"```
virtualenv venv_hmr
source venv_hmr/bin/activate
pip install -U pip
deactivate
source venv_hmr/bin/activate
pip install -r requirements.txt
```
"
4,hmr-README.md, Install TensorFlow,"With GPU:
```
pip install tensorflow-gpu==1.3.0
```
Without GPU:
```
pip install tensorflow==1.3.0
```

"
5,hmr-README.md, Windows Setup with python 3 and Anaconda,"This is only partialy tested.
```
conda env create -f hmr.yml
```
"
6,hmr-README.md, if you need to get chumpy ,"https://github.com/mattloper/chumpy/tree/db6eaf8c93eb5ae571eb054575fb6ecec62fd86d


"
7,hmr-README.md, Demo,"1. Download the pre-trained models
```
wget https://people.eecs.berkeley.edu/~kanazawa/cachedir/hmr/models.tar.gz && tar -xf models.tar.gz
```

2. Run the demo
```
python -m demo --img_path data/coco1.png
python -m demo --img_path data/im1954.jpg
```

Images should be tightly cropped, where the height of the person is roughly 150px.
On images that are not tightly cropped, you can run
[openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) and supply
its output json (run it with `--write_json` option).
When json_path is specified, the demo will compute the right scale and bbox center to run HMR:
```
python -m demo --img_path data/random.jpg --json_path data/random_keypoints.json
```
(The demo only runs on the most confident bounding box, see `src/util/openpose.py:get_bbox`)

"
8,hmr-README.md, Webcam Demo (thanks @JulesDoe!),"1. Download pre-trained models like above.
2. Run webcam Demo
2. Run the demo
```
python -m demo --img_path data/coco1.png
python -m demo --img_path data/im1954.jpg
```

"
9,hmr-README.md, Training code/data,"Please see the [doc/train.md](https://github.com/akanazawa/hmr/blob/master/doc/train.md)!

"
10,hmr-README.md, Citation,"If you use this code for your research, please consider citing:
```
@inProceedings{kanazawaHMR18,
  title={End-to-end Recovery of Human Shape and Pose},
  author = {Angjoo Kanazawa
  and Michael J. Black
  and David W. Jacobs
  and Jitendra Malik},
  booktitle={Computer Vision and Pattern Regognition (CVPR)},
  year={2018}
}
```

"
11,hmr-README.md, Opensource contributions,"[Dawars](https://github.com/Dawars) has created a docker image for this project: https://hub.docker.com/r/dawars/hmr/

[MandyMo](https://github.com/MandyMo) has implemented a pytorch version of the repo: https://github.com/MandyMo/pytorch_HMR.git

[Dene33](https://github.com/Dene33) has made a .ipynb for Google Colab that takes video as input and returns .bvh animation!
https://github.com/Dene33/video_to_bvh 

<img alt=""bvh"" src=""https://i.imgur.com/QxML83b.gif"" /><img alt="""" src=""https://i.imgur.com/vfge7DS.gif"" />
<img alt=""bvh2"" src=https://i.imgur.com/UvBM1gv.gif />

I have not tested them, but the contributions are super cool! Thank you!!


"
0,GPRPy-README.md, GPRPy,"Open-source Ground Penetrating Radar processing and visualization software.

![Profile GUI](profileGUI.png)

![CMP/WARR GUI](CWGUI.png)


"
1,GPRPy-README.md, Simplemost installation,"**In the following instructions, if you use Windows, use the comands `python` and `pip`. If you use Mac or Linux, use the commands `python3` and `pip3` instead.**

1) Download the GPRPy software from 
   [https://github.com/NSGeophysics/GPRPy/archive/master.zip](https://github.com/NSGeophysics/GPRPy/archive/master.zip). <br/>
   Save the file somewhere on your computer and extract the zip folder. <br/>
   As an **alternative**, you can install git from [https://git-scm.com/](https://git-scm.com/), then run in a command prompt:<br/>
   `git clone https://github.com/NSGeophysics/GPRPy.git`<br/>
   The advantage of the latter is that you can easily update your software by running from the GPRPy folder in a command prompt:<br/>
   `git pull origin master`

2) Install Python 3.7 for example from [https://conda.io/miniconda.html](https://conda.io/miniconda.html)

3) Once the installation finished, open a command prompt that can run Python <br/>
   On Windows: click on Start, then enter ""Anaconda Prompt"", without the quotation marks into the ""Search programs and files"" field. On Mac or Linux, open the regular terminal.

4) In the command prompt, change to the directory  where you downloaded the GPRPy files.
   This is usually through a command like for example<br/>
   `cd Desktop\GPRPy`<br/>
   if you downloaded GPRPy directly onto your desktop. Then type the following and press enter afterward:<br/>
   `python installMigration.py`<br/>
   Then type the following and press enter afterward:<br/>
   `pip install .`<br/>
   **don't forget the period ""."" at the end of the `pip install` command**


"
2,GPRPy-README.md, Running the software,"After installation, you can run the script from the Anaconda Prompt (or your Python-enabled prompt) by running either<br/>
`gprpy`<br/>
or<br/>
`python -m gprpy`

The first time you run GPRPy it could take a while to initialize. GPRPy will ask you if you want to run the profile [p] or WARR / CMP [c] user interface. Type<br/>
`p`<br/>
and then enter for profile, or<br/>
`c`<br/>
and then enter for CMP / WARR.

You can also directly select one by running either<br/>
`gprpy p`<br/>
or<br/>
`gprpy c`<br/>
or<br/>
`python -m gprpy p`<br/>
or<br/>
`python -m gprpy c`


"
3,GPRPy-README.md, Running automatically generated scripts,"To run automatically generated scripts, open the command prompt that can run python (for example Anaconda Prompt), switch to the folder with the automatically generated script and run<br/>
`python myscriptname.py`<br/>
where myscriptname.py is the name of your automatically generated script.  


"
4,GPRPy-README.md, In case of trouble,"If you have several versions of python installed, for example on a Mac or Linux system, replace, in the commands shown earlier,
`python` with `python3`<br/>
and<br/>
`pip` with `pip3`

If you have any troubles getting the software running, please send me an email or open an issue on GitHub and I will help you getting it running.


"
5,GPRPy-README.md, Uninstalling GPRPy,"To uninstall GPRPy, simply run, in the (Anaconda) command prompt<br/>
`pip uninstall gprpy`

"
6,GPRPy-README.md, News,"Follow [@GPRPySoftware](https://twitter.com/GPRPySoftware) on twitter to hear about news and updates.
Recent tweets:

<blockquote class=""twitter-tweet"" data-lang=""en""><p lang=""en"" dir=""ltr"">Fixed small issue that led to multiples when picking points in profile. Thanks Marcus Pacheco for pointing it out! If you use picking in profile mode, please update to version 1.0.3 (uninstall the old version before).</p>&mdash; GPRPy (@GPRPySoftware) <a href=""https://twitter.com/GPRPySoftware/status/1139243564469313536?ref_src=twsrc%5Etfw"">June 13, 2019</a></blockquote>

<blockquote class=""twitter-tweet"" data-lang=""en""><p lang=""en"" dir=""ltr"">I will post updates, changes, and GPRPy news here.</p>&mdash; GPRPy (@GPRPySoftware) <a href=""https://twitter.com/GPRPySoftware/status/1089246592786485251?ref_src=twsrc%5Etfw"">January 26, 2019</a></blockquote>

<blockquote class=""twitter-tweet"" data-lang=""en""><p lang=""en"" dir=""ltr"">GPRPy is a free ground penetrating radar processing and visualization software developed at the University of Alabama. You can download it and install it following the instructions here: <a href=""https://nsgeophysics.github.io/GPRPy/"">nsgeophysics.github.io/GPRPy/</a></p>&mdash; GPRPy (@GPRPySoftware) <a href=""https://twitter.com/GPRPySoftware/status/1088806792191197188?ref_src=twsrc%5Etfw"">January 25, 2019</a></blockquote>


"
0,d3-README.md, D3: Data-Driven Documents,"<a href=""https://d3js.org""><img src=""https://d3js.org/logo.svg"" align=""left"" hspace=""10"" vspace=""6""></a>

**D3** (or **D3.js**) is a JavaScript library for visualizing data using web standards. D3 helps you bring data to life using SVG, Canvas and HTML. D3 combines powerful visualization and interaction techniques with a data-driven approach to DOM manipulation, giving you the full capabilities of modern browsers and the freedom to design the right visual interface for your data.

"
1,d3-README.md, Resources,"* [API Reference](https://github.com/d3/d3/blob/master/API.md)
* [Release Notes](https://github.com/d3/d3/releases)
* [Gallery](https://github.com/d3/d3/wiki/Gallery)
* [Examples](https://bl.ocks.org/mbostock)
* [Wiki](https://github.com/d3/d3/wiki)

"
2,d3-README.md, Installing,"If you use npm, `npm install d3`. Otherwise, download the [latest release](https://github.com/d3/d3/releases/latest). The released bundle supports anonymous AMD, CommonJS, and vanilla environments. You can load directly from [d3js.org](https://d3js.org), [CDNJS](https://cdnjs.com/libraries/d3), or [unpkg](https://unpkg.com/d3/). For example:

```html
<script src=""https://d3js.org/d3.v5.js""></script>
```

For the minified version:

```html
<script src=""https://d3js.org/d3.v5.min.js""></script>
```

You can also use the standalone D3 microlibraries. For example, [d3-selection](https://github.com/d3/d3-selection):

```html
<script src=""https://d3js.org/d3-selection.v1.js""></script>
```

D3 is written using [ES2015 modules](http://www.2ality.com/2014/09/es6-modules-final.html). Create a [custom bundle using Rollup](https://bl.ocks.org/mbostock/bb09af4c39c79cffcde4), Webpack, or your preferred bundler. To import D3 into an ES2015 application, either import specific symbols from specific D3 modules:

```js
import {scaleLinear} from ""d3-scale"";
```

Or import everything into a namespace (here, `d3`):

```js
import * as d3 from ""d3"";
```

In Node:

```js
var d3 = require(""d3"");
```

You can also require individual modules and combine them into a `d3` object using [Object.assign](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign):

```js
var d3 = Object.assign({}, require(""d3-format""), require(""d3-geo""), require(""d3-geo-projection""));
```
"
0,iter-reason-README.md, Iterative Visual Reasoning Beyond Convolutions,"By Xinlei Chen, Li-Jia Li, Li Fei-Fei and Abhinav Gupta. 

"
1,iter-reason-README.md, Disclaimer,"  - This is the authors' implementation of the system described in the paper, not an official Google product.
  - Right now:
    - The available reasoning module is based on convolutions and spatial memory.
    - For simplicity, the released code uses the tensorflow default `crop_and_resize` operation, rather than the customized one reported in the paper (I find the default one is actually better by ~1%).

"
2,iter-reason-README.md, Prerequisites,"1. Tensorflow, tested with version 1.6 with Ubuntu 16.04, installed with:
  ```Shell
  pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl
  ```

2. Other packages needed can be installed with `pip`:
  ```Shell
  pip install Cython easydict matplotlib opencv-python Pillow pyyaml scipy
  ```

3. For running COCO, the API can be installed globally:
  ```Shell
  #: any path is okay
  mkdir ~/install && cd ~/install
  git clone https://github.com/cocodataset/cocoapi.git cocoapi
  cd cocoapi/PythonAPI
  python setup.py install --user
  ```

"
3,iter-reason-README.md, Setup and Running,"1. Clone the repository.
  ```Shell
  git clone https://github.com/endernewton/iter-reason.git
  cd iter-reason
  ```

2. Set up data, here we use [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) as an example.
  ```Shell
  mkdir -p data/ADE
  cd data/ADE
  wget -v http://groups.csail.mit.edu/vision/datasets/ADE20K/ADE20K_2016_07_26.zip
  tar -xzvf ADE20K_2016_07_26.zip
  mv ADE20K_2016_07_26/* ./
  rmdir ADE20K_2016_07_26
  #: then get the train/val/test split
  wget -v http://xinleic.xyz/data/ADE_split.tar.gz
  tar -xzvf ADE_split.tar.gz
  rm -vf ADE_split.tar.gz
  cd ../..
  ```

3. Set up pre-trained ImageNet models. This is similarly done in [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn). Here by default we use ResNet-50 as the backbone:
  ```Shell
   mkdir -p data/imagenet_weights
   cd data/imagenet_weights
   wget -v http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz
   tar -xzvf resnet_v1_50_2016_08_28.tar.gz
   mv resnet_v1_50.ckpt res50.ckpt
   cd ../..
   ```

4. Compile the library (for computing bounding box overlaps).
  ```Shell
  cd lib
  make
  cd ..
  ```

5. Now you are ready to run! For example, to train and test the baseline:
  ```Shell
  ./experiments/scripts/train.sh [GPU_ID] [DATASET] [NET] [STEPS] [ITER] 
  #: GPU_ID is the GPU you want to test on
  #: DATASET in {ade, coco, vg} is the dataset to train/test on, defined in the script
  #: NET in {res50, res101} is the backbone networks to choose from
  #: STEPS (x10K) is the number of iterations before it reduces learning rate, can support multiple steps separated by character 'a'
  #: ITER (x10K) is the total number of iterations to run
  #: Examples:
  #: train on ADE20K for 320K iterations, reducing learning rate at 280K.
  ./experiments/scripts/train.sh 0 ade 28 32
  #: train on COCO for 720K iterations, reducing at 320K and 560K.
  ./experiments/scripts/train.sh 1 coco 32a56 72
  ```

6. To train and test the reasoning modules (based on ResNet-50):
  ```Shell
  ./experiments/scripts/train_memory.sh [GPU_ID] [DATASET] [MEM] [STEPS] [ITER] 
  #: MEM in {local} is the type of reasoning modules to use 
  #: Examples:
  #: train on ADE20K on the local spatial memory.
  ./experiments/scripts/train_memory.sh 0 ade local 28 32
  ```

7. Once the training is done, you can test the models separately with `test.sh` and `test_memory.sh`, we also provided a separate set of scripts to test on larger image inputs.

8. You can use tensorboard to visualize and track the progress, for example:
  ```Shell
  tensorboard --logdir=tensorboard/res50/ade_train_5/ --port=7002 &
  ```

"
4,iter-reason-README.md, References,"```
@inproceedings{chen18iterative,
    author = {Xinlei Chen and Li-Jia Li and Li Fei-Fei and Abhinav Gupta},
    title = {Iterative Visual Reasoning Beyond Convolutions},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    Year = {2018}
}
```

The idea of spatial memory was developed in:
```
@inproceedings{chen2017spatial,
    author = {Xinlei Chen and Abhinav Gupta},
    title = {Spatial Memory for Context Reasoning in Object Detection},
    booktitle = {Proceedings of the International Conference on Computer Vision},
    Year = {2017}
}
```"
0,tetgen-README.md, get cell centroids,"    cells = grid.cells.reshape(-1, 5)[:, 1:]
    cell_center = grid.points[cells].mean(1)

    "
1,tetgen-README.md, extract cells below the 0 xy plane,"    mask = cell_center[:, 2] < 0
    cell_ind = mask.nonzero()[0]
    subgrid = grid.extract_cells(cell_ind)

    "
2,tetgen-README.md, advanced plotting,"    plotter = pv.Plotter()
    plotter.add_mesh(subgrid, 'lightgrey', lighting=True, show_edges=True)
    plotter.add_mesh(sphere, 'r', 'wireframe')
    plotter.add_legend([[' Input Mesh ', 'r'],
                        [' Tesselated Mesh ', 'black']])
    plotter.show()

.. image:: https://github.com/pyvista/tetgen/raw/master/docs/images/sphere_subgrid.png

Cell quality scalars can be obtained and plotted with:

.. code:: python

    cell_qual = subgrid.quality

    "
3,tetgen-README.md, plot quality,"    subgrid.plot(scalars=cell_qual, stitle='Quality', cmap='bwr', clim=[0,1],
                 flip_scalars=True, show_edges=True,)

.. image:: https://github.com/pyvista/tetgen/raw/master/docs/images/sphere_qual.png


Acknowledgments
---------------
Software was originally created by Hang Si based on work published in
`TetGen, a Delaunay-Based Quality Tetrahedral Mesh Generator <https://dl.acm.org/citation.cfm?doid=2629697>`__.
"
0,GAN_stability-README.md, GAN stability,"This repository contains the experiments in the supplementary material for the paper [Which Training Methods for GANs do actually Converge?](https://avg.is.tuebingen.mpg.de/publications/meschedericml2018).

To cite this work, please use
```
@INPROCEEDINGS{Mescheder2018ICML,
  author = {Lars Mescheder and Sebastian Nowozin and Andreas Geiger},
  title = {Which Training Methods for GANs do actually Converge?},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2018}
}
```
You can find further details on [our project page](https://avg.is.tuebingen.mpg.de/research_projects/convergence-and-stability-of-gan-training).

"
1,GAN_stability-README.md, Usage,"First download your data and put it into the `./data` folder.

To train a new model, first create a config script similar to the ones provided in the `./configs` folder.  You can then train you model using
```
python train.py PATH_TO_CONFIG
```

To compute the inception score for your model and generate samples, use
```
python test.py PATH_TO_CONIFG
```

Finally, you can create nice latent space interpolations using
```
python interpolate.py PATH_TO_CONFIG
```
or
```
python interpolate_class.py PATH_TO_CONFIG
```

"
2,GAN_stability-README.md, Notes,"* For the results presented in the paper, we did not use a moving average over the weights. However, using a moving average helps to reduce noise and we therefore recommend its usage. Indeed, we found that using a moving average leads to much better inception scores on Imagenet.
* Batch normalization is currently *not* supported when using an exponential running average, as the running average is only computed over the parameters of the models and not the other buffers of the model.

"
4,GAN_stability-README.md, celebA-HQ,"![celebA-HQ](results/celebA-HQ.jpg)

"
5,GAN_stability-README.md, Imagenet,"![Imagenet 0](results/imagenet_00.jpg)
![Imagenet 1](results/imagenet_01.jpg)
![Imagenet 2](results/imagenet_02.jpg)
![Imagenet 3](results/imagenet_03.jpg)
![Imagenet 4](results/imagenet_04.jpg)
"
0,pylops-README.md, Objective,"This Python library is inspired by the MATLAB [Spot – A Linear-Operator Toolbox](http://www.cs.ubc.ca/labs/scl/spot/) project.

Linear operators and inverse problems are at the core of many of the most used algorithms
in signal processing, image processing, and remote sensing. When dealing with small-scale problems,
the Python numerical scientific libraries [numpy](http://www.numpy.org)
and [scipy](https://www.scipy.org/scipylib/index.html) allow to perform many
of the underlying matrix operations (e.g., computation of matrix-vector products and manipulation of matrices)
in a simple and compact way.

Many useful operators, however, do not lend themselves to an explicit matrix
representation when used to solve large-scale problems. PyLops operators, on the other hand, still represent a matrix
and can be treated in a similar way, but do not rely on the explicit creation of a dense (or sparse) matrix itself. Conversely,
the forward and adjoint operators are represented by small pieces of codes that mimic the effect of the matrix
on a vector or another matrix.

Luckily, many iterative methods (e.g. cg, lsqr) do not need to know the individual entries of a matrix to solve a linear system.
Such solvers only require the computation of forward and adjoint matrix-vector products as done for any of the PyLops operators.

Here is a simple example showing how a dense first-order first derivative operator can be created,
applied and inverted using numpy/scipy commands:
```python
import numpy as np
from scipy.linalg import lstsq

nx = 7
x = np.arange(nx) - (nx-1)/2

D = np.diag(0.5*np.ones(nx-1), k=1) - \
    np.diag(0.5*np.ones(nx-1), k=-1)
D[0] = D[-1] = 0 #: take away edge effects

#: y = Dx
y = np.dot(D,x)
#: x = D'y
xadj = np.dot(D.T,y)
#: xinv = D^-1 y
xinv = lstsq(D, y)[0]
```
and similarly using PyLops commands:
```python
from pylops import FirstDerivative

Dlop = FirstDerivative(nx, dtype='float64')

#: y = Dx
y = Dlop*x
#: x = D'y
xadj = Dlop.H*y
#: xinv = D^-1 y
xinv = Dlop / y
```

Note how this second approach does not require creating a dense matrix, reducing both the memory load and the computational cost of
applying a derivative to an input vector x. Moreover, the code becomes even more compact and espressive than in the previous case
letting the user focus on the formulation of equations of the forward problem to be solved by inversion.


"
1,pylops-README.md, Project structure,"This repository is organized as follows:
* **pylops**:       python library containing various linear operators and auxiliary routines
* **pytests**:    set of pytests
* **testdata**:   sample datasets used in pytests and documentation
* **docs**:       sphinx documentation
* **examples**:   set of python script examples for each linear operator to be embedded in documentation using sphinx-gallery
* **tutorials**:  set of python script tutorials to be embedded in documentation using sphinx-gallery

"
2,pylops-README.md, Getting started,"You need **Python 3.6.4 or greater**.

"
3,pylops-README.md, From PyPi,"If you want to use PyLops within your codes,
install it in your Python environment by typing the following command in your terminal:

```
pip install pylops
```

Open a python terminal and type:

```
import pylops
```

If you do not see any error, you should be good to go, enjoy!

"
4,pylops-README.md, From Conda-forge,"Alternatively, you can install PyLops using the conda-forge distribution by typing the following command in your terminal:

```
conda install -c conda-forge pylops
```

"
5,pylops-README.md, From Docker,"If you simply want to try PyLops but do not have Python in your
local machine, you can use our [Docker](https://www.docker.com) image. After installing Docker in your computer,
type the following command in your terminal (note that this will take some time the first time
you type it as you will download and install the docker image):

```
docker run -it -v /path/to/local/folder:/home/jupyter/notebook -p 8888:8888 mrava87/pylops:notebook
```

This will give you an address that you can put in your browser and will open a jupyter-notebook enviroment with PyLops
and other basic Python libraries installed. Here `/path/to/local/folder` is the absolute path of a local folder
on your computer where you will create a notebook (or containing notebooks that you want to continue working on). Note that
anything you do to the notebook(s) will be saved in your local folder.

A larger image with Conda distribution is also available. Simply use `conda_notebook` instead of `notebook` in the
previous command.

"
6,pylops-README.md, Contributing,"*Feel like contributing to the project? Adding new operators or tutorial?*

We advise using the [Anaconda Python distribution](https://www.anaconda.com/download)
to ensure that all the dependencies are installed via the `Conda` package manager. Follow
the following instructions and read carefully the [CONTRIBUTING](CONTRIBUTING.md) file before getting started.

"
7,pylops-README.md, 1. Fork and clone the repository,"Execute the following command in your terminal:

```
git clone https://github.com/your_name_here/pylops.git
```

"
8,pylops-README.md, 2. Install PyLops in a new Conda environment,"To ensure that further development of PyLops is performed within the same environment (i.e., same dependencies) as
that defined by ``requirements-dev.txt`` or ``environment-dev.yml`` files, we suggest to work off a new Conda enviroment.

The first time you clone the repository run the following command:
```
make dev-install_conda
```
To ensure that everything has been setup correctly, run tests:
```
make tests
```
Make sure no tests fail, this guarantees that the installation has been successfull.

Remember to always activate the conda environment every time you open a new terminal by typing:
```
source activate pylops
```

"
9,pylops-README.md, Documentation,"The official documentation of PyLops is available [here](https://pylops.readthedocs.io/).

Visit this page to get started learning about different operators and their applications as well as how to
create new operators yourself and make it to the ``Contributors`` list.

Moreover, if you have installed PyLops using the *developer environment* you can also build the documentation locally by
typing the following command:
```
make doc
```
Once the documentation is created, you can make any change to the source code and rebuild the documentation by
simply typing
```
make docupdate
```
Note that if a new example or tutorial is created (and if any change is made to a previously available example or tutorial)
you are required to rebuild the entire documentation before your changes will be visible.


"
10,pylops-README.md, History,"PyLops was initially written and it is currently maintained by [Equinor](https://www.equinor.com).
It is a flexible and scalable python library for large-scale optimization with linear
operators that can be tailored to our needs, and as contribution to the free software community.


"
11,pylops-README.md, Contributors,"* Matteo Ravasi, mrava87
* Carlos da Costa, cako
* Dieter Werthmüller, prisae
* Tristan van Leeuwen, TristanvanLeeuwen
"
0,generator-arcgis-js-app-README.md, generator-arcgis-js-app,"> [Yeoman](http://yeoman.io) generator

This is a yeoman generator for [ArcGIS API for JavaScript applications](https://developers.arcgis.com/javascript/).

"
2,generator-arcgis-js-app-README.md, What is Yeoman?,"Trick question. It's not a thing. It's this guy:

![](http://i.imgur.com/JHaAlBJ.png)

Basically, he wears a top hat, lives in your computer, and waits for you to tell him what kind of application you wish to create.

Not every new computer comes with a Yeoman pre-installed. He lives in the [npm](https://npmjs.org) package repository. You only have to ask for him once, then he packs up and moves into your hard drive. *Make sure you clean up, he likes new and shiny things.*

```bash
npm install -g yo
npm install -g bower
```

Bower is a required dependency of using the packages in the generated app.

"
3,generator-arcgis-js-app-README.md, Yeoman Generators,"Yeoman travels light. He didn't pack any generators when he moved in. You can think of a generator like a plug-in. You get to choose what type of application you wish to create, such as a Backbone application or even a Chrome extension.

To install generator-arcgis-js-app from npm, run:

```bash
npm install -g generator-arcgis-js-app
```

Finally, initiate the generator inside application folder:

```bash
yo arcgis-js-app
```

or

```bash
yo arcgis-js-app application-name
```

You will be asked:
* Application name if not provided
* Description of application
* ArcGIS API Version (3.x or 4.x)
* Use Stylus or Sass
* Email to be used in package information

Will create component and tests. Updates `intern.js` with test suite.


"
4,generator-arcgis-js-app-README.md, What is used?,"* New use 3.x or 4.x of the [ArcGIS API for JavaScript](https://developers.arcgis.com/javascript/)
* Output application uses [GruntJS](http://gruntjs.com/) for running tasks
* All code is written in [ES6/ES2015](https://github.com/lukehoban/es6features) and transpiled with [babel](https://babeljs.io/)
* Uses [eslint](https://github.com/eslint/eslint) to lint code
* Uses [stylus](https://learnboost.github.io/stylus/) or [sass](http://sass-lang.com/) as a css preprocessor
* Uses [livereload](http://livereload.com/) for easier development workflow

"
5,generator-arcgis-js-app-README.md, Usage,"`grunt` - default task, will output code to a `dist` folder with sourcemaps.

`grunt dev` - will start a local server on at `http://localhost:8282/` and watch for changes. Uses livereload to refresh browser with each update.

`http://localhost:8282/dist/` - application

`http://localhost:8282/node_modules/intern/client.html?config=tests/intern` - test suites

`grunt build` - build the application and output to a `release` folder.

`grunt e2e` - runs all tests using local [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/).


"
6,generator-arcgis-js-app-README.md, Still a beta,"* Considering implementing a [widgetloader](https://github.com/odoe/esri-widgetloader)
* Needs ability to inject code into [Application.js](generators/app/templates/src/app/templates/Appication.js)
* Guide on application architecture

"
7,generator-arcgis-js-app-README.md, Notes,"Uses [theintern.io](https://theintern.github.io/) for testing.

It is recommended that you use NPM 3.x to install dependencies, as this [will reduce the time it takes for Babel to transpile ES2015 code](https://phabricator.babeljs.io/T6756#67810).

"
8,generator-arcgis-js-app-README.md, Getting To Know Yeoman,"Yeoman has a heart of gold. He's a person with feelings and opinions, but he's very easy to work with. If you think he's too opinionated, he can be easily convinced.

If you'd like to get to know Yeoman better and meet some of his friends, [Grunt](http://gruntjs.com) and [Bower](http://bower.io), check out the complete [Getting Started Guide](https://github.com/yeoman/yeoman/wiki/Getting-Started).


"
9,generator-arcgis-js-app-README.md, License,"MIT
"
0,puppeteer-README.md, Puppeteer,"<!-- [START badges] -->
[![Linux Build Status](https://img.shields.io/travis/com/GoogleChrome/puppeteer/master.svg)](https://travis-ci.com/GoogleChrome/puppeteer) [![Windows Build Status](https://img.shields.io/appveyor/ci/aslushnikov/puppeteer/master.svg?logo=appveyor)](https://ci.appveyor.com/project/aslushnikov/puppeteer/branch/master) [![Build Status](https://api.cirrus-ci.com/github/GoogleChrome/puppeteer.svg)](https://cirrus-ci.com/github/GoogleChrome/puppeteer) [![NPM puppeteer package](https://img.shields.io/npm/v/puppeteer.svg)](https://npmjs.org/package/puppeteer)
<!-- [END badges] -->

<img src=""https://user-images.githubusercontent.com/10379601/29446482-04f7036a-841f-11e7-9872-91d1fc2ea683.png"" height=""200"" align=""right"">

"
1,puppeteer-README.md, [API](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md) | [FAQ](faq) | [Contributing](https://github.com/GoogleChrome/puppeteer/blob/master/CONTRIBUTING.md) | [Troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md),"> Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). Puppeteer runs [headless](https://developers.google.com/web/updates/2017/04/headless-chrome) by default, but can be configured to run full (non-headless) Chrome or Chromium.

<!-- [START usecases] -->
"
2,puppeteer-README.md, What can I do?,"Most things that you can do manually in the browser can be done using Puppeteer! Here are a few examples to get you started:

* Generate screenshots and PDFs of pages.
* Crawl a SPA (Single-Page Application) and generate pre-rendered content (i.e. ""SSR"" (Server-Side Rendering)).
* Automate form submission, UI testing, keyboard input, etc.
* Create an up-to-date, automated testing environment. Run your tests directly in the latest version of Chrome using the latest JavaScript and browser features.
* Capture a [timeline trace](https://developers.google.com/web/tools/chrome-devtools/evaluate-performance/reference) of your site to help diagnose performance issues.
* Test Chrome Extensions.
<!-- [END usecases] -->

Give it a spin: https://try-puppeteer.appspot.com/

<!-- [START getstarted] -->
"
4,puppeteer-README.md, Installation,"To use Puppeteer in your project, run:

```bash
npm i puppeteer
#: or ""yarn add puppeteer""
```

Note: When you install Puppeteer, it downloads a recent version of Chromium (~170MB Mac, ~282MB Linux, ~280MB Win) that is guaranteed to work with the API. To skip the download, see [Environment variables](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#environment-variables).


"
5,puppeteer-README.md, puppeteer-core,"Since version 1.7.0 we publish the [`puppeteer-core`](https://www.npmjs.com/package/puppeteer-core) package,
a version of Puppeteer that doesn't download Chromium by default.

```bash
npm i puppeteer-core
#: or ""yarn add puppeteer-core""
```

`puppeteer-core` is intended to be a lightweight version of Puppeteer for launching an existing browser installation or for connecting to a remote one. Be sure that the version of puppeteer-core you install is compatible with the
browser you intend to connect to.

See [puppeteer vs puppeteer-core](https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md#puppeteer-vs-puppeteer-core).

"
6,puppeteer-README.md, Usage,"Note: Puppeteer requires at least Node v6.4.0, but the examples below use async/await which is only supported in Node v7.6.0 or greater.

Puppeteer will be familiar to people using other browser testing frameworks. You create an instance
of `Browser`, open pages, and then manipulate them with [Puppeteer's API](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#).

**Example** - navigating to https://example.com and saving a screenshot as *example.png*:

Save file as **example.js**

```js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');
  await page.screenshot({path: 'example.png'});

  await browser.close();
})();
```

Execute script on the command line

```bash
node example.js
```

Puppeteer sets an initial page size to 800px x 600px, which defines the screenshot size. The page size can be customized  with [`Page.setViewport()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagesetviewportviewport).

**Example** - create a PDF.

Save file as **hn.js**

```js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://news.ycombinator.com', {waitUntil: 'networkidle2'});
  await page.pdf({path: 'hn.pdf', format: 'A4'});

  await browser.close();
})();
```

Execute script on the command line

```bash
node hn.js
```

See [`Page.pdf()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagepdfoptions) for more information about creating pdfs.

**Example** - evaluate script in the context of the page

Save file as **get-dimensions.js**

```js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');

  // Get the ""viewport"" of the page, as reported by the page.
  const dimensions = await page.evaluate(() => {
    return {
      width: document.documentElement.clientWidth,
      height: document.documentElement.clientHeight,
      deviceScaleFactor: window.devicePixelRatio
    };
  });

  console.log('Dimensions:', dimensions);

  await browser.close();
})();
```

Execute script on the command line

```bash
node get-dimensions.js
```

See [`Page.evaluate()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pageevaluatepagefunction-args) for more information on `evaluate` and related methods like `evaluateOnNewDocument` and `exposeFunction`.

<!-- [END getstarted] -->

<!-- [START runtimesettings] -->
"
7,puppeteer-README.md, Default runtime settings,"**1. Uses Headless mode**

Puppeteer launches Chromium in [headless mode](https://developers.google.com/web/updates/2017/04/headless-chrome). To launch a full version of Chromium, set the ['headless' option](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#puppeteerlaunchoptions) when launching a browser:

```js
const browser = await puppeteer.launch({headless: false}); // default is true
```

**2. Runs a bundled version of Chromium**

By default, Puppeteer downloads and uses a specific version of Chromium so its API
is guaranteed to work out of the box. To use Puppeteer with a different version of Chrome or Chromium,
pass in the executable's path when creating a `Browser` instance:

```js
const browser = await puppeteer.launch({executablePath: '/path/to/Chrome'});
```

See [`Puppeteer.launch()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#puppeteerlaunchoptions) for more information.

See [`this article`](https://www.howtogeek.com/202825/what%E2%80%99s-the-difference-between-chromium-and-chrome/) for a description of the differences between Chromium and Chrome. [`This article`](https://chromium.googlesource.com/chromium/src/+/master/docs/chromium_browser_vs_google_chrome.md) describes some differences for Linux users.

**3. Creates a fresh user profile**

Puppeteer creates its own Chromium user profile which it **cleans up on every run**.

<!-- [END runtimesettings] -->

"
8,puppeteer-README.md, Resources,"- [API Documentation](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md)
- [Examples](https://github.com/GoogleChrome/puppeteer/tree/master/examples/)
- [Community list of Puppeteer resources](https://github.com/transitive-bullshit/awesome-puppeteer)


<!-- [START debugging] -->

"
9,puppeteer-README.md, Debugging tips,"1. Turn off headless mode - sometimes it's useful to see what the browser is
   displaying. Instead of launching in headless mode, launch a full version of
   the browser using  `headless: false`:

        const browser = await puppeteer.launch({headless: false});

2. Slow it down - the `slowMo` option slows down Puppeteer operations by the
   specified amount of milliseconds. It's another way to help see what's going on.

        const browser = await puppeteer.launch({
          headless: false,
          slowMo: 250 // slow down by 250ms
        });

3. Capture console output - You can listen for the `console` event.
   This is also handy when debugging code in `page.evaluate()`:

        page.on('console', msg => console.log('PAGE LOG:', msg.text()));

        await page.evaluate(() => console.log(`url is ${location.href}`));

4. Use debugger in application code browser

    There are two execution context: node.js that is running test code, and the browser
    running application code being tested. This lets you debug code in the
    application code browser; ie code inside `evaluate()`.

    - Use `{devtools: true}` when launching Puppeteer:

        `const browser = await puppeteer.launch({devtools: true});`

    - Change default test timeout:

        jest: `jest.setTimeout(100000);`

        jasmine: `jasmine.DEFAULT_TIMEOUT_INTERVAL = 100000;`

        mocha: `this.timeout(100000);` (don't forget to change test to use [function and not '=>'](https://stackoverflow.com/a/23492442))

    - Add an evaluate statement with `debugger` inside / add  `debugger` to an existing evaluate statement:

      `await page.evaluate(() => {debugger;});`

       The test will now stop executing in the above evaluate statement, and chromium will stop in debug mode.

5. Use debugger in node.js

    This will let you debug test code. For example, you can step over `await page.click()` in the node.js script and see the click happen in the application code browser.

    Note that you won't be able to run `await page.click()` in
    DevTools console due to this [Chromium bug](https://bugs.chromium.org/p/chromium/issues/detail?id=833928). So if
    you want to try something out, you have to add it to your test file.

    - Add `debugger;` to your test, eg:
      ```
      debugger;
      await page.click('a[target=_blank]');
      ```
    - Set `headless` to `false`
    - Run `node --inspect-brk`, eg `node --inspect-brk node_modules/.bin/jest tests`
    - In Chrome open `chrome://inspect/#devices` and click `inspect`
    - In the newly opened test browser, type `F8` to resume test execution
    - Now your `debugger` will be hit and you can debug in the test browser


6. Enable verbose logging - internal DevTools protocol traffic
   will be logged via the [`debug`](https://github.com/visionmedia/debug) module under the `puppeteer` namespace.

        "
10,puppeteer-README.md, Basic verbose logging,"        env DEBUG=""puppeteer:*"" node script.js

        "
11,puppeteer-README.md, Protocol traffic can be rather noisy. This example filters out all Network domain messages,"        env DEBUG=""puppeteer:*"" env DEBUG_COLORS=true node script.js 2>&1 | grep -v '""Network'

7. Debug your Puppeteer (node) code easily, using [ndb](https://github.com/GoogleChromeLabs/ndb)

  - `npm install -g ndb` (or even better, use [npx](https://github.com/zkat/npx)!)

  - add a `debugger` to your Puppeteer (node) code

  - add `ndb` (or `npx ndb`) before your test command. For example:

    `ndb jest` or `ndb mocha` (or `npx ndb jest` / `npx ndb mocha`)

  - debug your test inside chromium like a boss!


<!-- [END debugging] -->

"
12,puppeteer-README.md, Contributing to Puppeteer,"Check out [contributing guide](https://github.com/GoogleChrome/puppeteer/blob/master/CONTRIBUTING.md) to get an overview of Puppeteer development.

<!-- [START faq] -->

"
14,puppeteer-README.md, Q: Who maintains Puppeteer?,"The Chrome DevTools team maintains the library, but we'd love your help and expertise on the project!
See [Contributing](https://github.com/GoogleChrome/puppeteer/blob/master/CONTRIBUTING.md).

"
15,puppeteer-README.md, Q: What are Puppeteer’s goals and principles?,"The goals of the project are:

- Provide a slim, canonical library that highlights the capabilities of the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/).
- Provide a reference implementation for similar testing libraries. Eventually, these other frameworks could adopt Puppeteer as their foundational layer.
- Grow the adoption of headless/automated browser testing.
- Help dogfood new DevTools Protocol features...and catch bugs!
- Learn more about the pain points of automated browser testing and help fill those gaps.

We adapt [Chromium principles](https://www.chromium.org/developers/core-principles) to help us drive product decisions:
- **Speed**: Puppeteer has almost zero performance overhead over an automated page.
- **Security**: Puppeteer operates off-process with respect to Chromium, making it safe to automate potentially malicious pages.
- **Stability**: Puppeteer should not be flaky and should not leak memory.
- **Simplicity**: Puppeteer provides a high-level API that’s easy to use, understand, and debug.

"
16,puppeteer-README.md, Q: Is Puppeteer replacing Selenium/WebDriver?,"**No**. Both projects are valuable for very different reasons:
- Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers.
- Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.

That said, you **can** use Puppeteer to run tests against Chromium, e.g. using the community-driven [jest-puppeteer](https://github.com/smooth-code/jest-puppeteer). While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver:

- Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it [very easy to start with](https://github.com/GoogleChrome/puppeteer/#getting-started). At the end of the day, it’s better to have a few tests running chromium-only, than no tests at all.
- Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There’s no need for evil “sleep(1000)” calls in puppeteer scripts.
- Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution.
- Puppeteer shines when it comes to debugging: flip the “headless” bit to false, add “slowMo”, and you’ll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.

"
17,puppeteer-README.md, Q: Why doesn’t Puppeteer v.XXX work with Chromium v.YYY?,"We see Puppeteer as an **indivisible entity** with Chromium. Each version of Puppeteer bundles a specific version of Chromium – **the only** version it is guaranteed to work with.

This is not an artificial constraint: A lot of work on Puppeteer is actually taking place in the Chromium repository. Here’s a typical story:
- A Puppeteer bug is reported: https://github.com/GoogleChrome/puppeteer/issues/2709
- It turned out this is an issue with the DevTools protocol, so we’re fixing it in Chromium: https://chromium-review.googlesource.com/c/chromium/src/+/1102154
- Once the upstream fix is landed, we roll updated Chromium into Puppeteer: https://github.com/GoogleChrome/puppeteer/pull/2769

However, oftentimes it is desirable to use Puppeteer with the official Google Chrome rather than Chromium. For this to work, you should install a `puppeteer-core` version that corresponds to the Chrome version.

For example, in order to drive Chrome 71 with puppeteer-core, use `chrome-71` npm tag:
```bash
npm install puppeteer-core@chrome-71
```

"
18,puppeteer-README.md, Q: Which Chromium version does Puppeteer use?,"Look for `chromium_revision` in [package.json](https://github.com/GoogleChrome/puppeteer/blob/master/package.json).

"
19,puppeteer-README.md, Q: What’s considered a “Navigation”?,"From Puppeteer’s standpoint, **“navigation” is anything that changes a page’s URL**.
Aside from regular navigation where the browser hits the network to fetch a new document from the web server, this includes [anchor navigations](https://www.w3.org/TR/html5/single-page.html#scroll-to-fragid) and [History API](https://developer.mozilla.org/en-US/docs/Web/API/History_API) usage.

With this definition of “navigation,” **Puppeteer works seamlessly with single-page applications.**

"
20,puppeteer-README.md," Q: What’s the difference between a “trusted"" and ""untrusted"" input event?","In browsers, input events could be divided into two big groups: trusted vs. untrusted.

- **Trusted events**: events generated by users interacting with the page, e.g. using a mouse or keyboard.
- **Untrusted event**: events generated by Web APIs, e.g. `document.createEvent` or `element.click()` methods.

Websites can distinguish between these two groups:
- using an [`Event.isTrusted`](https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted) event flag
- sniffing for accompanying events. For example, every trusted `'click'` event is preceded by `'mousedown'` and `'mouseup'` events.

For automation purposes it’s important to generate trusted events. **All input events generated with Puppeteer are trusted and fire proper accompanying events.** If, for some reason, one needs an untrusted event, it’s always possible to hop into a page context with `page.evaluate` and generate a fake event:

```js
await page.evaluate(() => {
  document.querySelector('button[type=submit]').click();
});
```

"
21,puppeteer-README.md, Q: What features does Puppeteer not support?,"You may find that Puppeteer does not behave as expected when controlling pages that incorporate audio and video. (For example, [video playback/screenshots is likely to fail](https://github.com/GoogleChrome/puppeteer/issues/291).) There are two reasons for this:

* Puppeteer is bundled with Chromium--not Chrome--and so by default, it inherits all of [Chromium's media-related limitations](https://www.chromium.org/audio-video). This means that Puppeteer does not support licensed formats such as AAC or H.264. (However, it is possible to force Puppeteer to use a separately-installed version Chrome instead of Chromium via the [`executablePath` option to `puppeteer.launch`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#puppeteerlaunchoptions). You should only use this configuration if you need an official release of Chrome that supports these media formats.)
* Since Puppeteer (in all configurations) controls a desktop version of Chromium/Chrome, features that are only supported by the mobile version of Chrome are not supported. This means that Puppeteer [does not support HTTP Live Streaming (HLS)](https://caniuse.com/#feat=http-live-streaming).

"
22,puppeteer-README.md, Q: I am having trouble installing / running Puppeteer in my test environment. Where should I look for help?,"We have a [troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.

"
23,puppeteer-README.md, Q: How do I try/test a prerelease version of Puppeteer?,"You can check out this repo or install the latest prerelease from npm:

```bash
npm i --save puppeteer@next
```

Please note that prerelease may be unstable and contain bugs.

"
24,puppeteer-README.md, Q: I have more questions! Where do I ask?,"There are many ways to get help on Puppeteer:
- [bugtracker](https://github.com/GoogleChrome/puppeteer/issues)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/puppeteer)
- [slack channel](https://join.slack.com/t/puppeteer/shared_invite/enQtMzU4MjIyMDA5NTM4LTM1OTdkNDhlM2Y4ZGUzZDdjYjM5ZWZlZGFiZjc4MTkyYTVlYzIzYjU5NDIyNzgyMmFiNDFjN2UzNWU0N2ZhZDc)

Make sure to search these channels before posting your question.


<!-- [END faq] -->
"
0,integral-human-pose-README.md, Integral Human Pose Regression,"The major contributors of this repository include [Xiao Sun](https://jimmysuen.github.io/), [Chuankang Li](https://github.com/lck1201), [Bin Xiao](https://github.com/leoxiaobin), [Fangyin Wei](https://weify627.github.io/), [Yichen Wei](https://github.com/YichenWei).

"
1,integral-human-pose-README.md, Introduction,"**Integral Regression** is initially described in an [ECCV 2018 paper](https://arxiv.org/abs/1711.08229). ([Slides](https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx)).

We build a [3D pose estimation system](https://arxiv.org/abs/1809.06079) based mainly on the Integral Regression, placing second in the [ECCV2018 3D Human Pose Estimation Challenge](http://vision.imar.ro/human3.6m/ranking.php). Note that, the winner [Sarandi et al.](https://arxiv.org/pdf/1809.04987.pdf) also uses the Integral Regression (or soft-argmax) with a better [augmented 3D dataset](https://github.com/isarandi/synthetic-occlusion) in their method indicating the Integral Regression is the currently state-of-the-art 3D human pose estimation method.

The Integral Regression is also known as soft-argmax. Please refer to two contemporary works ([Luvizon et al.](https://arxiv.org/abs/1710.02322) and [Nibali et al.](https://arxiv.org/abs/1801.07372)) for a better comparision and more comprehensive understanding.


<img src='figure/overview5.png' width='800'>
<img src='figure/examples2.png' width='800'>

"
2,integral-human-pose-README.md, Disclaimer,"This is an official implementation for [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229) based on Pytorch. It is worth noticing that:

  * The original implementation is based on our internal Mxnet version. There are slight differences in the final accuracy and running time due to the plenty details in platform switch.

"
3,integral-human-pose-README.md, License,"© Microsoft, 2017. Licensed under an MIT license.


"
4,integral-human-pose-README.md, Citing Papers,"If you find Integral Regression useful in your research, please consider citing:
```
@article{sun2017integral,
  title={Integral human pose regression},
  author={Sun, Xiao and Xiao, Bin and Liang, Shuang and Wei, Yichen},
  journal={arXiv preprint arXiv:1711.08229},
  year={2017}
}
```
```
@article{sun2018integral,
  title={An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge},
  author={Sun, Xiao and Li, Chuankang and Lin, Stephen},
  journal={arXiv preprint arXiv:1809.06079},
  year={2018}
}
```

"
5,integral-human-pose-README.md, Main Results,"Train on CHALL_H80K train, evaluate on CHALL_H80K val.

|description        |box det.   |dataset        |backbone   |patch size |flip test  |ensemble   |MPJPE(mm)  | model                                     |
|-------------------|-----------|---------------|-----------|-----------|-----------|-----------|-----------|-------------------------------------------|
|original baseline  |No         |HM36           |ResNet-50  |  -        |No         |No         |115.9      |                                           |
|+person box det.   |Yes        |HM36           |ResNet-50  |256*256    |No         |No         |86.5(25.4%)|                                           |
|+MPII data         |Yes        |HM36+MPII      |ResNet-50  |256*256    |No         |No         |62.2(28.1%)|                                           |
|+deeper            |Yes        |HM36+MPII      |ResNet-152 |256*256    |No         |No         |61.2(1.6%) |                                           |
|+larger image      |Yes        |HM36+MPII      |ResNet-152 |288*384    |No         |No         |58.5(4.4%) |                                           |
|+COCO data         |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |No         |No         |57.5(1.7%) |[download](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)|
|+flip test         |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |Yes        |No         |56.9(1.0%) |[download](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)|
|+model ensemble    |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |Yes        |Yes        |55.3(2.8%) |[download](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)|

Train on CHALL_H80K train+val, evaluate on CHALL_H80K test.

|description        |box det.   |dataset        |backbone   |patch size |flip test  |ensemble   |MPJPE(mm)  | model                                     |
|-------------------|-----------|---------------|-----------|-----------|-----------|-----------|-----------|-------------------------------------------|
|challenge result   |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |Yes        |Yes        |47         | [download](https://www.dropbox.com/s/o1xhp3vocnvtgu3/model_chall_trainval_152ft_384x288.pth.tar?dl=0)|

"
6,integral-human-pose-README.md, Environment,"Python Version: 3.6 <br/>
OS: CentOs7 (Other Linux system is also OK) <br/>
CUDA: 9.0 (least 8.0) <br/>
PyTorch:0.4.0(see issue https://github.com/JimmySuen/integral-human-pose/issues/4)


"
7,integral-human-pose-README.md, Installation,"We recommend installing python from [Anaconda](https://www.anaconda.com/), installing pytorch following guide on [PyTorch](https://pytorch.org/) according to your specific CUDA & python version.
In addition, you need to install dependencies below.
```
pip install scipy
pip install matplotlib
pip install opencv-python
pip install easydict
pip install pyyaml
``` 


"
8,integral-human-pose-README.md, Preparation for Training & Testing,"1. Download Human3.6M(ECCV18 Challenge) image from [Human3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) and our processed annotation from [Baidu Disk](https://pan.baidu.com/s/1Qg4dH8PBXm8SzApI-uu0GA) (code: kfsm) or [Google Drive](https://drive.google.com/file/d/1wZynXUq91yECVRTFV8Tetvo271BXzxwI/view?usp=sharing)
2. Download MPII image from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)
3. Download COCO2017 image from [COCO Dataset](http://cocodataset.org/#home)
4. Download cache file from [Dropbox](https://www.dropbox.com/sh/uouev0a1ao84ofd/AADAjJUdr_Fm-eubk7c_s2JTa?dl=0)
5. Organize data like this
```
${PROJECT_ROOT}
 `-- data
     `-- coco
        |-- images
        |-- annotations
        |-- COCO_train2017_cache
     `-- mpii
        |-- images
        |-- annot
        |-- mpii_train_cache
        |-- mpii_valid_cache
     `-- hm36
        |-- images
        |-- annot
        |-- HM36_train_cache
        |-- HM36_validmin_cache
     `-- hm36_eccv_challenge
        `-- Train
            |-- IMG
            |-- POSE
        `-- Val
            |-- IMG
            |-- POSE
        `-- Test
            |-- IMG
        |-- HM36_eccv_challenge_Train_cache
        |-- HM36_eccv_challenge_Test_cache
        |-- HM36_eccv_challenge_Val_cache
```

#:#: Usage
We have placed some example config files in *experiments* folder, and you can use them straight forward. Don't modify them unless you know exactly what it means.
#:#:#: Train 
For [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229), cd to *pytorch_projects/integral_human_pose* <br/>
**Integral Regression**
```bash
python train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs32-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/  
```
**Direct Joint Regression**
```bash
python train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_dj_l1_adam_bs32-4gpus_x140-90-120/lr1e-3.yaml --dataroot=../../data/
```

For [3D pose estimation system](https://arxiv.org/abs/1809.06079) of ECCV18 Challenge, cd to *pytorch_projects/hm36_challenge*
```bash
python train.py --cfg=experiments/hm36/resnet152v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs24-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/
```

By default, logging and model will be saved to *log* and *output* folder respectively.

#:#:#: Test
To run evaluation on CHALL_H80K Val dataset
1. Download [model](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)
2. Place it under $project_root/model/hm36_challenge
3. cd to *$project_root/pytorch_projects/hm36_challenge*
4. execute command below
```bash
python test.py --cfg experiments/hm36/resnet152v1_ft/d-mch_384x288_deconv256x3_min-int-l1_adam_bs12-4gpus/lr1e-4_x300-270-290.yaml --model=../../model/hm36_challenge/model_chall_train_152ft_384x288.pth.tar
```
"
9,integral-human-pose-README.md, Extensions,"- The project is built on old version of pytorch(0.4.0), and currently the latest released one has updated to 1.0.1. So there may be some compatibility problems. Please feel free to submit new issues.
-  [A third-party implementation](https://github.com/mks0601/Integral-Human-Pose-Regression-for-3D-Human-Pose-Estimation) by  [mks0601](https://github.com/mks0601)
-  [A third-party implementation](https://github.com/mkocabas/EpipolarPose) by [mkocabas](https://github.com/mkocabas)
"
0,node-qa-masker-README.md, node-qa-masker,"This is a NodeJS port of [pymasker](https://github.com/haoliangyu/pymasker). It provides a convenient way to produce masks from the Quality Assessment band of Landsat 8 OLI images, as well as MODIS land products.

"
1,node-qa-masker-README.md, Installation,"``` bash
npm install qa-masker
```

"
3,node-qa-masker-README.md, Landsat 8,"The `LandsatMasker` class provides the functionality to load and generate masks from the Quality Assessment band of Landsat 8 OLI image.

``` javascript
var qm = require('qa-masker');
var Masker = qm.LandsatMasker;
var Confidence = qm.LandsatConfidence;

// read the band file to initialize
var masker = new Masker('LC80170302016198LGN00_BQA.TIF');

// generate mask in ndarray format
var mask = masker.getWaterMask(Confidence.high);

// save the mask as GeoTIFF
masker.saveAsTif(mask, 'test.tif');
```

Five methods are provided for masking:

* `getCloudMask(confidence)`

* `getCirrusMask(confidence)`

* `getWaterMask(confidence)`

* `getVegMask(confidence)` (for vegetation)

* `getSnowMask(confidence)`

* `getFillMask()` (for filled pixels)

The `LandsatConfidence` class provide the definition of the confidence that certain condition exists at the pixel:

* `LandsatConfidence.high` (66% - 100% confidence)

* `LandsatConfidence.medium` (33% - 66% confidence)

* `LandsatConfidence.low` (0% - 33% confidence)

* `LandsatConfidence.undefined`

For more detail about the definition, please visit [the USGS Landsat website](http://landsat.usgs.gov/qualityband.php);

These five methods would return a [ndarray](https://github.com/scijs/ndarray) mask.

If a mask that matches multiple conditions is desired, the function `getMultiMask()` could help:

``` javascript
var mask = masker.getMultiMask([
  { type: 'could', confidence: LandsatConfidence.high },
  { type: 'cirrus', confidence: LandsatConfidence.medium }
]);
```

"
4,node-qa-masker-README.md, MODIS Land Products,"By using the lower level `Masker` class, the masking of MODIS land product QA band is supported. Because [node-gdal](https://github.com/scijs/ndarray) doesn't support HDF format, you need to convert the QA band to a GeoTIFF first using like [QGIS](http://www.qgis.org/en/site/),

A handy class `ModisMasker` is provided for particularly masking the quality of land products:

``` javascript
var qm = require('qa-masker');
var Masker = qm.ModisMasker;
var Quality = qm.ModisQuality;

// read the band file to initialize
var masker = new Masker('MODIS_QC_Band.tif');

// generate mask in ndarray format
var mask = masker.getQaMask(Quality.high);

// save the mask as GeoTIFF
masker.saveAsTif(mask, 'mask.tif');
```

The `ModisQuality` provides the definition of pixel quality:

  * `ModisQuality.high`: corrected product produced at ideal quality for all bands

  * `ModisQuality.medium`: corrected product produced at less than ideal quality for some or all bands

  * `ModisQuality.low`: corrected product not produced due to some reasons for some or all bands

  * `ModisQuality.low_cloud`: corrected product not produced due to cloud effects for all bands

Masking other than the product quality is not directly provided because of the variety of bit structure for different products.

A low-level method is available to extract mask with the understand of bit structure:

``` javascript
var masker = new Masker('modis_qa_band.tif');
var mask = masker.getMask(0, 2, 2);
```

`getMask(bitPos, bitLen, value)` function use to bit mask to extract quality mask:

* `bitPos`: the start position of quality assessment bits

* `bitLen`: the length of all used quality assessment bits

* `value`: the desired bit value (in integer)

For the detail explanation, please read [MODIS Land Product QA Tutorial](https://lpdaac.usgs.gov/sites/default/files/public/modis/docs/MODIS_LP_QA_Tutorial-1b.pdf).

"
5,node-qa-masker-README.md, Looking for command line tool?,"If the command line tool is wanted, please use [pymasker](https://github.com/haoliangyu/pymasker).

"
6,node-qa-masker-README.md, You are a GIS guy and want something GIS?,"Take a look at the [arcmasker](https://github.com/haoliangyu/arcmasker), the ArcMap toolbox that uses the same mechanism.
"
1,GANimation-README.md, [[Project]](http://www.albertpumarola.com/research/GANimation/index.html)[ [Paper]](https://arxiv.org/abs/1807.09251) ,"Official implementation of [GANimation](http://www.albertpumarola.com/research/GANimation/index.html). In this work we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describe in a continuous manifold the anatomical facial movements defining a human expression. Our approach permits controlling the magnitude of activation of each AU and combine several of them. For more information please refer to the [paper](https://arxiv.org/abs/1807.09251).

This code was made public to share our research for the benefit of the scientific community. Do NOT use it for immoral purposes.

![GANimation](http://www.albertpumarola.com/images/2018/GANimation/teaser.png)

"
2,GANimation-README.md, Prerequisites,"- Install PyTorch (version 0.3.1), Torch Vision and dependencies from http://pytorch.org
- Install requirements.txt (```pip install -r requirements.txt```)

"
3,GANimation-README.md, Data Preparation,"The code requires a directory containing the following files:
- `imgs/`: folder with all image
- `aus_openface.pkl`: dictionary containing the images action units.
- `train_ids.csv`: file containing the images names to be used to train.
- `test_ids.csv`: file containing the images names to be used to test.

An example of this directory is shown in `sample_dataset/`.

To generate the `aus_openface.pkl` extract each image Action Units with [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units) and store each output in a csv file the same name as the image. Then run:
```
python data/prepare_au_annotations.py
```

"
4,GANimation-README.md, Run,"To train:
```
bash launch/run_train.sh
```
To test:
```
python test --input_path path/to/img
```

"
5,GANimation-README.md, Citation,"If you use this code or ideas from the paper for your research, please cite our paper:
```
@inproceedings{pumarola2018ganimation,
    title={GANimation: Anatomically-aware Facial Animation from a Single Image},
    author={A. Pumarola and A. Agudo and A.M. Martinez and A. Sanfeliu and F. Moreno-Noguer},
    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    year={2018}
}
```
"
0,geojson-vt-README.md, geojson-vt &mdash; GeoJSON Vector Tiles,"[![Build Status](https://travis-ci.org/mapbox/geojson-vt.svg?branch=master)](https://travis-ci.org/mapbox/geojson-vt)

A highly efficient JavaScript library for **slicing GeoJSON data into vector tiles on the fly**,
primarily designed to enable rendering and interacting with large geospatial datasets
on the browser side (without a server).

Created to power GeoJSON in [Mapbox GL JS](https://github.com/mapbox/mapbox-gl-js),
but can be useful in other visualization platforms
like [Leaflet](https://github.com/Leaflet/Leaflet) and [d3](https://github.com/mbostock/d3),
as well as Node.js server applications.

Resulting tiles conform to the JSON equivalent
of the [vector tile specification](https://github.com/mapbox/vector-tile-spec/).
To make data rendering and interaction fast, the tiles are simplified,
retaining the minimum level of detail appropriate for each zoom level
(simplifying shapes, filtering out tiny polygons and polylines).

Read more on how the library works [on the Mapbox blog](https://www.mapbox.com/blog/introducing-geojson-vt/).

There's a C++11 port: [geojson-vt-cpp](https://github.com/mapbox/geojson-vt-cpp)

"
1,geojson-vt-README.md, Demo,"Here's **geojson-vt** action in [Mapbox GL JS](https://github.com/mapbox/mapbox-gl-js),
dynamically loading a 100Mb US zip codes GeoJSON with 5.4 million points:

![](https://cloud.githubusercontent.com/assets/25395/5360312/86028d8e-7f91-11e4-811f-87f24acb09ca.gif)

There's a convenient [debug page](http://mapbox.github.io/geojson-vt/debug/) to test out **geojson-vt** on different data.
Just drag any GeoJSON on the page, watching the console.

![](https://cloud.githubusercontent.com/assets/25395/5363235/41955c6e-7fa8-11e4-9575-a66ef54cb6d9.gif)

"
2,geojson-vt-README.md, Usage,"```js
// build an initial index of tiles
var tileIndex = geojsonvt(geoJSON);

// request a particular tile
var features = tileIndex.getTile(z, x, y).features;

// show an array of tile coordinates created so far
console.log(tileIndex.tileCoords); // [{z: 0, x: 0, y: 0}, ...]
```

"
3,geojson-vt-README.md, Options,"You can fine-tune the results with an options object,
although the defaults are sensible and work well for most use cases.

```js
var tileIndex = geojsonvt(data, {
	maxZoom: 14,  // max zoom to preserve detail on; can't be higher than 24
	tolerance: 3, // simplification tolerance (higher means simpler)
	extent: 4096, // tile extent (both width and height)
	buffer: 64,   // tile buffer on each side
	debug: 0,     // logging level (0 to disable, 1 or 2)
	lineMetrics: false, // whether to enable line metrics tracking for LineString/MultiLineString features
	promoteId: null,    // name of a feature property to promote to feature.id. Cannot be used with `generateId`
	generateId: false,  // whether to generate feature ids. Cannot be used with `promoteId`
	indexMaxZoom: 5,       // max zoom in the initial tile index
	indexMaxPoints: 100000 // max number of points per tile in the index
});
```

By default, tiles at zoom levels above `indexMaxZoom` are generated on the fly, but you can pre-generate all possible tiles for `data` by setting `indexMaxZoom` and `maxZoom` to the same value, setting `indexMaxPoints` to `0`, and then accessing the resulting tile coordinates from the `tileCoords` property of `tileIndex`.

The `promoteId` and `generateId` options ignore existing `id` values on the feature objects.

GeoJSON-VT only operates on zoom levels up to 24.

"
4,geojson-vt-README.md, Install,"Install using NPM (`npm install geojson-vt`) or Yarn (`yarn add geojson-vt`), then:

```js
// import as a ES module
import geojsonvt from 'geojson-vt';

// or require in Node / Browserify
const geojsonvt = require('geojson-vt');
```

Or use a browser build directly:

```html
<script src=""https://unpkg.com/geojson-vt@3.2.0/geojson-vt.js""></script>
```
"
0,tensorflow-README.md, Installation,"To install the current release for CPU-only:

```
pip install tensorflow
```

Use the GPU package for CUDA-enabled GPU cards:

```
pip install tensorflow-gpu
```

*See [Installing TensorFlow](https://www.tensorflow.org/install) for detailed
instructions, and how to build from source.*

People who are a little more adventurous can also try our nightly binaries:

**Nightly pip packages** * We are pleased to announce that TensorFlow now offers
nightly pip packages under the
[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and
[tf-nightly-gpu](https://pypi.python.org/pypi/tf-nightly-gpu) project on PyPi.
Simply run `pip install tf-nightly` or `pip install tf-nightly-gpu` in a clean
environment to install the nightly TensorFlow build. We support CPU and GPU
packages on Linux, Mac, and Windows.

"
1,tensorflow-README.md, *Try your first TensorFlow program*,"```shell
$ python
```

```python
>>> import tensorflow as tf
>>> tf.enable_eager_execution()
>>> tf.add(1, 2).numpy()
3
>>> hello = tf.constant('Hello, TensorFlow!')
>>> hello.numpy()
'Hello, TensorFlow!'
```

Learn more examples about how to do specific tasks in TensorFlow at the
[tutorials page of tensorflow.org](https://www.tensorflow.org/tutorials/).

"
2,tensorflow-README.md, Contribution guidelines,"**If you want to contribute to TensorFlow, be sure to review the [contribution
guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's
[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to
uphold this code.**

**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for
tracking requests and bugs, please see
[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)
for general questions and discussion, and please direct specific questions to
[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**

The TensorFlow project strives to abide by generally accepted best practices in open-source software development:

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)

"
4,tensorflow-README.md, Official Builds,"| Build Type      | Status | Artifacts |
| ---             | ---    | ---       |
| **Linux CPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html) | [pypi](https://pypi.org/project/tf-nightly/) |
| **Linux GPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/) |
| **Linux XLA**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html) | TBA |
| **MacOS**       | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html) | [pypi](https://pypi.org/project/tf-nightly/) |
| **Windows CPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html) | [pypi](https://pypi.org/project/tf-nightly/) |
| **Windows GPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/) |
| **Android**     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html) | [![Download](https://api.bintray.com/packages/google/tensorflow/tensorflow/images/download.svg)](https://bintray.com/google/tensorflow/tensorflow/_latestVersion) |
| **Raspberry Pi 0 and 1** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv6l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl) |
| **Raspberry Pi 2 and 3** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv7l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl) |


"
5,tensorflow-README.md, Community Supported Builds,"Build Type                                                                        | Status                                                                                                                                                                                        | Artifacts
--------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------
**Linux s390x** Nightly                                                           | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon)](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)                                                             | [Nightly](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)
**Linux s390x CPU** Stable Release                                                | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/badge/icon)](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)                                      | [Release](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)
**Linux ppc64le CPU** Nightly                                                     | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/)                                       | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/)
**Linux ppc64le CPU** Stable Release                                              | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)                       | [Release](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)
**Linux ppc64le GPU** Nightly                                                     | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/)                                       | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/)
**Linux ppc64le GPU** Stable Release                                              | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)                       | [Release](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)
**Linux CPU with Intel® MKL-DNN** Nightly                                         | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/)                                     | [Nightly](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)
**Linux CPU with Intel® MKL-DNN** <br> **Supports Python 2.7, 3.4, 3.5, and 3.6** | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/lastStableBuild)      | [1.13.1 pypi](https://pypi.org/project/intel-tensorflow/)
**Red Hat® Enterprise Linux® 7.6 CPU & GPU** <br> Python 2.7, 3.6                 | [![Build Status](https://jenkins-tensorflow.apps.ci.centos.org/buildStatus/icon?job=tensorflow-rhel7-3.6&build=2)](https://jenkins-tensorflow.apps.ci.centos.org/job/tensorflow-rhel7-3.6/2/) | [1.13.1 pypi](https://tensorflow.pypi.thoth-station.ninja/index/)

"
6,tensorflow-README.md, For more information,"*   [TensorFlow Website](https://www.tensorflow.org)
*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)
*   [TensorFlow Model Zoo](https://github.com/tensorflow/models)
*   [TensorFlow Twitter](https://twitter.com/tensorflow)
*   [TensorFlow Blog](https://medium.com/tensorflow)
*   [TensorFlow Course at Stanford](https://web.stanford.edu/class/cs20si)
*   [TensorFlow Roadmap](https://www.tensorflow.org/community/roadmap)
*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)
*   [TensorFlow YouTube Channel](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)
*   [TensorFlow Visualization Toolkit](https://github.com/tensorflow/tensorboard)

Learn more about the TensorFlow community at the [community page of tensorflow.org](https://www.tensorflow.org/community) for a few ways to participate.

"
7,tensorflow-README.md, License,"[Apache License 2.0](LICENSE)
"
1,3D-ResNets-PyTorch-README.md, Update (2018/2/21),"Our paper ""Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"" is accepted to CVPR2018!  
We update the paper information.

"
2,3D-ResNets-PyTorch-README.md, Update (2018/01/16),"We uploaded some of fine-tuned models on UCF-101 and HMDB-51.

* ResNeXt-101 fine-tuned on UCF-101 (split1)
* ResNeXt-101 (64 frame inputs) fine-tuned on UCF-101 (split1)
* ResNeXt-101 fine-tuned on HMDB-51 (split1)
* ResNeXt-101 (64 frame inputs) fine-tuned on HMDB-51 (split1)

"
3,3D-ResNets-PyTorch-README.md, Update (2017/11/27),"We published [a new paper](https://arxiv.org/abs/1711.09577) on arXiv.  
We also added the following new models and their Kinetics pretrained models in this repository.  

* ResNet-50, 101, 152, 200
* Pre-activation ResNet-200
* Wide ResNet-50
* ResNeXt-101
* DenseNet-121, 201

In addition, we supported new datasets (UCF-101 and HDMB-51) and fine-tuning functions.

Some minor changes are included.

* Outputs are normalized by softmax in test.
  * If you do not want to perform the normalization, please use ```--no_softmax_in_test``` option.

"
4,3D-ResNets-PyTorch-README.md, Summary,"This is the PyTorch code for the following papers:

[
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  
""Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"",  
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6546-6555, 2018.
](http://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html)

[
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  
""Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"",  
Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition, 2017.
](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf)

This code includes training, fine-tuning and testing on Kinetics, ActivityNet, UCF-101, and HMDB-51.  
**If you want to classify your videos or extract video features of them using our pretrained models,
use [this code](https://github.com/kenshohara/video-classification-3d-cnn-pytorch).**

**The Torch (Lua) version of this code is available [here](https://github.com/kenshohara/3D-ResNets).**  
Note that the Torch version only includes ResNet-18, 34, 50, 101, and 152.

"
5,3D-ResNets-PyTorch-README.md, Citation,"If you use this code or pre-trained models, please cite the following:

```bibtex
@inproceedings{hara3dcnns,
  author={Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},
  title={Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6546--6555},
  year={2018},
}
```

"
6,3D-ResNets-PyTorch-README.md, Pre-trained models,"Pre-trained models are available [here](https://drive.google.com/drive/folders/1zvl89AgFAApbH0At-gMuZSeQB_LpNP-M?usp=sharing).  
All models are trained on Kinetics.  
ResNeXt-101 achieved the best performance in our experiments. (See [paper](https://arxiv.org/abs/1711.09577) in details.)

```misc
resnet-18-kinetics.pth: --model resnet --model_depth 18 --resnet_shortcut A
resnet-34-kinetics.pth: --model resnet --model_depth 34 --resnet_shortcut A
resnet-34-kinetics-cpu.pth: CPU ver. of resnet-34-kinetics.pth
resnet-50-kinetics.pth: --model resnet --model_depth 50 --resnet_shortcut B
resnet-101-kinetics.pth: --model resnet --model_depth 101 --resnet_shortcut B
resnet-152-kinetics.pth: --model resnet --model_depth 152 --resnet_shortcut B
resnet-200-kinetics.pth: --model resnet --model_depth 200 --resnet_shortcut B
preresnet-200-kinetics.pth: --model preresnet --model_depth 200 --resnet_shortcut B
wideresnet-50-kinetics.pth: --model wideresnet --model_depth 50 --resnet_shortcut B --wide_resnet_k 2
resnext-101-kinetics.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32
densenet-121-kinetics.pth: --model densenet --model_depth 121
densenet-201-kinetics.pth: --model densenet --model_depth 201
```

Some of fine-tuned models on UCF-101 and HMDB-51 (split 1) are also available.

```misc
resnext-101-kinetics-ucf101_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32
resnext-101-64f-kinetics-ucf101_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32 --sample_duration 64
resnext-101-kinetics-hmdb51_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32
resnext-101-64f-kinetics-hmdb51_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32 --sample_duration 64
```

"
7,3D-ResNets-PyTorch-README.md, Performance of the models on Kinetics,"This table shows the averaged accuracies over top-1 and top-5 on Kinetics.

| Method | Accuracies |
|:---|:---:|
| ResNet-18 | 66.1 |
| ResNet-34 | 71.0 |
| ResNet-50 | 72.2 |
| ResNet-101 | 73.3 |
| ResNet-152 | 73.7 |
| ResNet-200 | 73.7 |
| ResNet-200 (pre-act) | 73.4 |
| Wide ResNet-50 | 74.7 |
| ResNeXt-101 | 75.4 |
| DenseNet-121 | 70.8 |
| DenseNet-201 | 72.3 |

"
8,3D-ResNets-PyTorch-README.md, Requirements,"* [PyTorch](http://pytorch.org/)

```bash
conda install pytorch torchvision cuda80 -c soumith
```

* FFmpeg, FFprobe

```bash
wget http://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz
tar xvf ffmpeg-release-64bit-static.tar.xz
cd ./ffmpeg-3.3.3-64bit-static/; sudo cp ffmpeg ffprobe /usr/local/bin;
```

* Python 3

"
10,3D-ResNets-PyTorch-README.md, ActivityNet,"* Download videos using [the official crawler](https://github.com/activitynet/ActivityNet/tree/master/Crawler).
* Convert from avi to jpg files using ```utils/video_jpg.py```

```bash
python utils/video_jpg.py avi_video_directory jpg_video_directory
```

* Generate fps files using ```utils/fps.py```

```bash
python utils/fps.py avi_video_directory jpg_video_directory
```

"
11,3D-ResNets-PyTorch-README.md, Kinetics,"* Download videos using [the official crawler](https://github.com/activitynet/ActivityNet/tree/master/Crawler/Kinetics).
  * Locate test set in ```video_directory/test```.
* Convert from avi to jpg files using ```utils/video_jpg_kinetics.py```

```bash
python utils/video_jpg_kinetics.py avi_video_directory jpg_video_directory
```

* Generate n_frames files using ```utils/n_frames_kinetics.py```

```bash
python utils/n_frames_kinetics.py jpg_video_directory
```

* Generate annotation file in json format similar to ActivityNet using ```utils/kinetics_json.py```
  * The CSV files (kinetics_{train, val, test}.csv) are included in the crawler.

```bash
python utils/kinetics_json.py train_csv_path val_csv_path test_csv_path dst_json_path
```

"
12,3D-ResNets-PyTorch-README.md, UCF-101,"* Download videos and train/test splits [here](http://crcv.ucf.edu/data/UCF101.php).
* Convert from avi to jpg files using ```utils/video_jpg_ucf101_hmdb51.py```

```bash
python utils/video_jpg_ucf101_hmdb51.py avi_video_directory jpg_video_directory
```

* Generate n_frames files using ```utils/n_frames_ucf101_hmdb51.py```

```bash
python utils/n_frames_ucf101_hmdb51.py jpg_video_directory
```

* Generate annotation file in json format similar to ActivityNet using ```utils/ucf101_json.py```
  * ```annotation_dir_path``` includes classInd.txt, trainlist0{1, 2, 3}.txt, testlist0{1, 2, 3}.txt

```bash
python utils/ucf101_json.py annotation_dir_path
```

"
13,3D-ResNets-PyTorch-README.md, HMDB-51,"* Download videos and train/test splits [here](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/).
* Convert from avi to jpg files using ```utils/video_jpg_ucf101_hmdb51.py```

```bash
python utils/video_jpg_ucf101_hmdb51.py avi_video_directory jpg_video_directory
```

* Generate n_frames files using ```utils/n_frames_ucf101_hmdb51.py```

```bash
python utils/n_frames_ucf101_hmdb51.py jpg_video_directory
```

* Generate annotation file in json format similar to ActivityNet using ```utils/hmdb51_json.py```
  * ```annotation_dir_path``` includes brush_hair_test_split1.txt, ...

```bash
python utils/hmdb51_json.py annotation_dir_path
```

"
14,3D-ResNets-PyTorch-README.md, Running the code,"Assume the structure of data directories is the following:

```misc
~/
  data/
    kinetics_videos/
      jpg/
        .../ (directories of class names)
          .../ (directories of video names)
            ... (jpg files)
    results/
      save_100.pth
    kinetics.json
```

Confirm all options.

```bash
python main.lua -h
```

Train ResNets-34 on the Kinetics dataset (400 classes) with 4 CPU threads (for data loading).  
Batch size is 128.  
Save models at every 5 epochs.
All GPUs is used for the training.
If you want a part of GPUs, use ```CUDA_VISIBLE_DEVICES=...```.

```bash
python main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \
--result_path results --dataset kinetics --model resnet \
--model_depth 34 --n_classes 400 --batch_size 128 --n_threads 4 --checkpoint 5
```

Continue Training from epoch 101. (~/data/results/save_100.pth is loaded.)

```bash
python main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \
--result_path results --dataset kinetics --resume_path results/save_100.pth \
--model_depth 34 --n_classes 400 --batch_size 128 --n_threads 4 --checkpoint 5
```

Fine-tuning conv5_x and fc layers of a pretrained model (~/data/models/resnet-34-kinetics.pth) on UCF-101.

```bash
python main.py --root_path ~/data --video_path ucf101_videos/jpg --annotation_path ucf101_01.json \
--result_path results --dataset ucf101 --n_classes 400 --n_finetune_classes 101 \
--pretrain_path models/resnet-34-kinetics.pth --ft_begin_index 4 \
--model resnet --model_depth 34 --resnet_shortcut A --batch_size 128 --n_threads 4 --checkpoint 5
```
"
0,DeepGuidedFilter-README.md, Fast End-to-End Trainable Guided Filter,"[[Project]](http://wuhuikai.me/DeepGuidedFilterProject)    [[Paper]](http://wuhuikai.me/DeepGuidedFilterProject/deep_guided_filter.pdf)    [[arXiv]](https://arxiv.org/abs/1803.05619)    [[Demo]](http://wuhuikai.me/DeepGuidedFilterProject#demo)    [[Home]](http://wuhuikai.me)
  
Official implementation of **Fast End-to-End Trainable Guided Filter**.     
**Faster**, **Better** and **Lighter**  for image processing and dense prediction. 

"
1,DeepGuidedFilter-README.md, Overview,"![](images/results.jpg)

**DeepGuidedFilter** is the author's implementation of the deep learning building block for joint upsampling described in:  

**Fast End-to-End Trainable Guided Filter**     
Huikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang    
CVPR 2018

Given a reference image pair in high-resolution and low-resolution, our algorithm generates high-resolution target from the low-resolution input. Through joint training with CNNs, our algorithm achieves the state-of-the-art performance while runs **10-100** times faster. 

Contact: Hui-Kai Wu (huikaiwu@icloud.com)

"
3,DeepGuidedFilter-README.md, Prepare Environment,"1. Download source code from GitHub.
    ```sh
    git clone https://github.com/wuhuikai/DeepGuidedFilter
    
    cd DeepGuidedFilter && git checkout release
    ```
2. Install dependencies (PyTorch version).
    ```sh
    conda install opencv
    conda install pytorch=0.2.0 cuda80 -c soumith
    
    pip install -r requirements.txt 
    ```
3. (**Optional**) Install dependencies for MonoDepth (Tensorflow version).
    ```sh
    cd ComputerVision/MonoDepth
    
    pip install -r requirements.txt
    ```
"
5,DeepGuidedFilter-README.md, Image Processing,"```sh
cd ImageProcessing/DeepGuidedFilteringNetwork

python predict.py  --task auto_ps \
                   --img_path ../../images/auto_ps.jpg \
                   --save_folder . \
                   --model deep_guided_filter_advanced \
                   --low_size 64 \
                   --gpu 0
```
See [Here](ImageProcessing/DeepGuidedFilteringNetwork/#predict) or `python predict.py -h` for more details.
"
6,DeepGuidedFilter-README.md, Semantic Segmentation with Deeplab-Resnet,"1. Enter the directory.
    ```sh
    cd ComputerVision/Deeplab-Resnet
    ```
2. Download the pretrained model [[Google Drive](https://drive.google.com/open?id=1YXZoZIZNR1ACewiUBp4UDvo_P65cCooK)|[BaiduYunPan](https://pan.baidu.com/s/1dEnpcGfchlZA_fVGdve0ig)].
3. Run it now !
    ```sh
    python predict_dgf.py --img_path ../../images/segmentation.jpg --snapshots [MODEL_PATH]
    ```
Note:
1. Result is in `../../images`.
2. Run `python predict_dgf.py -h` for more details.
"
7,DeepGuidedFilter-README.md, Saliency Detection with DSS,"1. Enter the directory.
    ```sh
    cd ComputerVision/Saliency_DSS
    ```
2. Download the pretrained model [[Google Drive](https://drive.google.com/open?id=1ZxbAAJw9BxCKj2e2QsBmCnjWLFlCGLf1)|[BaiduYunPan](https://pan.baidu.com/s/1pgOMh3V50lRa6slbIW_SKQ)].
3. Try it now !
    ```sh
    python predict.py --im_path ../../images/saliency.jpg \
                      --netG [MODEL_PATH] \
                      --thres 161 \
                      --dgf --nn_dgf \
                      --post_sigmoid --cuda
    ```
Note:
1. Result is in `../../images`.
2. See [Here](ComputerVision/Saliency_DSS/#try_on_an_image) or `python predict.py -h` for more details.
"
8,DeepGuidedFilter-README.md, Monocular Depth Estimation (TensorFlow version),"1. Enter the directory.
    ```sh
    cd ComputerVision/MonoDepth
    ```
2. **Download** and **Unzip** Pretrained Model [[Google Drive](https://drive.google.com/file/d/1dKDYRtZPahoFJZ5ZJNilgHEvT6gG4SC6/view?usp=sharing)|[BaiduYunPan](https://pan.baidu.com/s/1-GkMaRAVym8UEmQ6ia5cHw)]
2. Run on an Image !
    ```sh
    python monodepth_simple.py --image_path ../../images/depth.jpg --checkpoint_path [MODEL_PATH] --guided_filter
    ```
Note:
1. Result is in `../../images`.
2. See [Here](ComputerVision/MonoDepth/#try_it_on_an_image) or `python monodepth_simple.py -h` for more details.

"
10,DeepGuidedFilter-README.md, Install Released Version,"* PyTorch Version
    ```sh
    pip install guided-filter-pytorch
    ```
* Tensorflow Version
    ```sh
    pip install guided-filter-tf
    ```
"
11,DeepGuidedFilter-README.md, Usage,"* PyTorch Version
    ```python
    from guided_filter_pytorch.guided_filter import FastGuidedFilter
    
    hr_y = FastGuidedFilter(r, eps)(lr_x, lr_y, hr_x)
    ```
    ```python
    from guided_filter_pytorch.guided_filter import GuidedFilter
    
    hr_y = GuidedFilter(r, eps)(hr_x, init_hr_y)
    ``` 
* Tensorflow Version
    ```python
    from guided_filter_tf.guided_filter import fast_guided_filter
    
    hr_y = fast_guided_filter(lr_x, lr_y, hr_x, r, eps, nhwc)
    ```
    ```python
    from guided_filter_tf.guided_filter import guided_filter
    
    hr_y = guided_filter(hr_x, init_hr_y, r, eps, nhwc)
    ```
"
13,DeepGuidedFilter-README.md, Prepare Training Environment,"```sh
git checkout master

conda install opencv
conda install pytorch=0.2.0 cuda80 -c soumith
    
pip install -r requirements.txt

#: (Optional) For MonoDepth (TF Version).
pip install -r ComputerVision/MonoDepth/requirements.txt 
```
"
14,DeepGuidedFilter-README.md, Start to Train,"* [Image Processing](ImageProcessing/DeepGuidedFilteringNetwork)
* [Semantic Segmentation with Deeplab-Resnet](ComputerVision/Deeplab-Resnet)
* [Saliency Detection with DSS](ComputerVision/Saliency_DSS)
* [Monocular Depth Estimation (TensorFlow version)](ComputerVision/MonoDepth)

"
15,DeepGuidedFilter-README.md, Citation,"```
@inproceedings{wu2017fast,
  title     = {Fast End-to-End Trainable Guided Filter},
  author    = {Wu, Huikai and Zheng, Shuai and Zhang, Junge and Huang, Kaiqi},
  booktitle = {CVPR},
  year = {2018}
}
```"
0,SRN-Deblur-README.md, Scale-recurrent Network for Deep Image Deblurring,"by [Xin Tao](http://www.xtao.website), Hongyun Gao, [Xiaoyong Shen](http://xiaoyongshen.me/), [Jue Wang](http://juew.org), [Jiaya Jia](http://www.cse.cuhk.edu.hk/leojia/). ([pdf](http://www.xtao.website/projects/srndeblur/srndeblur_cvpr18.pdf))

"
1,SRN-Deblur-README.md, Our results on real data,"<img src=""./imgs/teaser.jpg"" width=""100%"" alt=""Real Photo"">

"
2,SRN-Deblur-README.md, Results on the testing dataset,"<img src=""./imgs/comp_sota.jpg"" width=""100%"" alt=""Testing Dataset"">

"
3,SRN-Deblur-README.md, More cases on real photos from previous papers:,"<img src=""./imgs/comp_real.jpg"" width=""100%"" alt=""More Cases"">

"
4,SRN-Deblur-README.md, Prerequisites,"- Python2.7
- Scipy
- Scikit-image
- numpy
- Tensorflow 1.4 with NVIDIA GPU or CPU (cpu testing is very slow)

"
5,SRN-Deblur-README.md, Installation,"Clone this project to your machine. 

```bash
git clone https://github.com/jiangsutx/SRN-Deblur.git
cd SRN-Deblur
```

"
6,SRN-Deblur-README.md, Testing,"Download pretrained models through: `download_model.sh` inside `checkpoints/`.

To test blur images in a folder, just use arguments 
`--input_path=<TEST_FOLDER>` and save the outputs to `--output_path=<OUTPUT_FOLDER>`.
For example:

```bash
python run_model.py --input_path=./testing_set --output_path=./testing_res
```

If you have a GPU, please include `--gpu` argument, and add your gpu id to your command. 
Otherwise, use `--gpu=-1` for CPU. 

```bash
python run_model.py --gpu=0
```

To test the model, pre-defined height and width of tensorflow 
placeholder should be assigned. 
Our network requires the height and width be multiples of `16`. 
When the gpu memory is enough, the height and width could be assigned to 
the maximum to accommodate all the images. 

Otherwise, the images will be downsampled by the largest scale factor to 
be fed into the placeholder. And results will be upsampled to the original size.

According to our experience, `--height=720` and `--width=1280` work well 
on a Gefore GTX 1050 TI with 4GB memory. For example, 

```bash
python run_model.py --height=720 --width=1280
```

"
7,SRN-Deblur-README.md, Evaluation,"The quantitative results of **PSNR** and **SSIM** in the paper is 
calculated using MATLAB built-in function `psnr()` and `ssim()` based 
on the generated color results.

"
8,SRN-Deblur-README.md, Training,"We trained our model using the dataset from 
[DeepDeblur_release](https://github.com/SeungjunNah/DeepDeblur_release). 
Please put the dataset into `training_set/`. And the provided `datalist_gopro.txt` 
can be used to train the model. 

Hyper parameters such as batch size, learning rate, epoch number can be tuned through command line:

```bash
python run_model.py --phase=train --batch=16 --lr=1e-4 --epoch=4000
```

"
9,SRN-Deblur-README.md, Models,"We provided 3 models (training settings) for testing:
1. `--model=lstm`: This model implements exactly the same structure in our paper.
Current released model weights should produce `PSNR=30.19, SSIM=0.9334` on GOPRO testing dataset.
2. `--model=gray`: According to our further experiments after paper acceptance, we are able
to get a slightly better model by tuning parameters, even without LSTM. 
This model should produce visually sharper and quantitatively better results. 
3. `--model=color`: Previous models are trained on gray images, and may produce color
ringing artifacts. So we train a model directly based on RGB images. 
This model keeps better color consistency, but the results are less sharp.
"
10,SRN-Deblur-README.md, How to choose,"If you would like to compare performance against our method, you can use 
model `gray` and `lstm`. 
If you want to restore blurry images you can try `gray` and `color`. 
And `color` is very useful in low-light noisy images.  

"
11,SRN-Deblur-README.md, Reference,"If you use any part of our code, or SRN-Deblur is useful for your research, please consider citing: 

```bibtex
@inproceedings{tao2018srndeblur,
  title={Scale-recurrent Network for Deep Image Deblurring},
  author={Tao, Xin and Gao, Hongyun and Shen, Xiaoyong and Wang, Jue and Jia, Jiaya},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}
```

"
12,SRN-Deblur-README.md, Contact,"We are glad to hear if you have any suggestions and questions.

Please send email to xtao@cse.cuhk.edu.hk

"
13,SRN-Deblur-README.md, Reference,"[1] `Sun et al.` J. Sun, W. Cao, Z. Xu, and J. Ponce. *Learning a convolutional
neural network for non-uniform motion blur removal.* In CVPR, pages 769–777. IEEE, 2015.

[2] `Nah et al.` S. Nah, T. H. Kim, and K. M. Lee. *Deep multi-scale convolutional
neural network for dynamic scene deblurring.* pages 3883–3891, 2017.

[3] `Whyte et al.` O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. *Nonuniform
deblurring for shaken images.* International Journal on Computer Vision, 98(2):168–186, 2012."
0,hyvr-README.md,"Introduction
","**HyVR: Turning your geofantasy into reality!** 

The Hydrogeological Virtual Reality simulation package (HyVR) is a Python module
that helps researchers and practitioners generate subsurface models with
multiple scales of heterogeneity that are based on geological concepts. The
simulation outputs can then be used to explore groundwater flow and solute
transport behaviour. This is facilitated by HyVR outputs in common flow
simulation packages' input formats. As each site is unique, HyVR has been
designed that users can take the code and extend it to suit their particular
simulation needs.

The original motivation for HyVR was the lack of tools for modelling sedimentary
deposits that include bedding structure model outputs (i.e., dip and azimuth).
Such bedding parameters were required to approximate full hydraulic-conductivity
tensors for groundwater flow modelling. HyVR is able to simulate these bedding
parameters and generate spatially distributed parameter fields, including full
hydraulic-conductivity tensors. More information about HyVR is available in the
online `technical documentation <https://driftingtides.github.io/hyvr/index.html>`_.

I hope you enjoy using HyVR much more than I enjoyed putting it together! I look
forward to seeing what kind of funky fields you created in the course of your
work.

*HyVR can be attributed by citing the following journal article: Bennett, J. P., Haslauer, C. P., Ross, M., & Cirpka, O. A. (2018). An open, object-based framework for generating anisotropy in sedimentary subsurface models. Groundwater. DOI:* `10.1111/gwat.12803 <https://onlinelibrary.wiley.com/doi/abs/10.1111/gwat.12803>`_. *A preprint version of the article is available* `here <https://github.com/driftingtides/hyvr/blob/master/docs/Bennett_GW_2018.pdf>`_.

"
1,hyvr-README.md,"Installing the HyVR package
","Installing Python
^^^^^^^^^^^^^^^^^


Windows
""""""""""""""

If you are using Windows, we recommend installing the `Anaconda distribution
<https://www.anaconda.com/download/>`_ of Python 3. This distribution has the
majority of dependencies that HyVR requires.

It is also a good idea to install the HyVR package into a `virtual environment
<https://conda.io/docs/user-guide/tasks/manage-environments.html>`_. Do this by
opening a command prompt window and typing the following::

    conda create --name hyvr_env

You need to then activate this environment::

    conda activate hyvr_env
	

Linux
""""""""""

Depending on your preferences you can either use the Anaconda/Miniconda
distribution of python, or the version of your package manager. If you choose
the former, follow the same steps as for Windows.

If you choose the latter, you probably already have Python 3 installed. If not,
you can install it using your package manager (e.g. ``apt`` on Ubuntu/Debian).

In any way we recommend using a virtual environment. Non-conda users can use
`virtualenvwrapper <https://virtualenvwrapper.readthedocs.io/en/latest/>`_ or
`pipenv <https://docs.pipenv.org/>`_.


Installing HyVR
^^^^^^^^^^^^^^^

Once you have activated your virtual environment, you can install HyVR from PyPI using ``pip``::

    pip install hyvr

The version on PyPI should always be up to date. If it's not, you can also
install HyVR from github::

    git clone https://github.com/driftingtides/hyvr.git
    pip install hyvr

To install from source you need a C compiler.

Installation from conda-forge will (hopefully) be coming soon.


"
2,hyvr-README.md,"Usage
","To use HyVR you have to create a configuration file with your settings.
You can then run HyVR the following way::

    (hyvr_env) $ python -m hyvr my_configfile.ini

HyVR will then run and store all results in a subdirectory. If no configfile is
given, it will run a test case instead::

    (hyvr_env) $ python -m hyvr

If you want to use HyVR in a script, you can import it and use the ``run`` function::

    import hyvr
    hyvr.run('my_configfile.ini')
    
Examples can be found in the ``testcases`` directory of the `github repository
<https://github.com/driftingtides/hyvr/>`_, the general setup and possible
options of the config-file are described in the documentation.
Currently only ``made.ini`` is ported to version 1.0.0.

"
3,hyvr-README.md,Source,"The most current version of HyVR will be available at this `github repository
<https://github.com/driftingtides/hyvr/>`_; a version will also be available on
the `PyPI index <https://pypi.python.org/pypi/hyvr/>`_ which can be installed
using ``pip``.


"
4,hyvr-README.md,"Requirements
","Python
^^^^^^
HyVR was developed for use with Python 3.4 or greater. It may be possible to use
with earlier versions of Python 3, however this has not been tested.

Dependencies
^^^^^^^^^^^^^^

* `numpy <http://www.numpy.org/>`_ <= 1.13.3
* `matplotlib <https://matplotlib.org/>`_ <= 2.1.0
* `scipy <https://www.scipy.org/scipylib/index.html>`_ = 1.0.0
* `pandas <https://pandas.pydata.org/>`_ = 0.21.0
* `flopy <https://github.com/modflowpy/flopy>`_ == 3.2.9 (optional for modflow output)
* `pyevtk <https://pypi.python.org/pypi/PyEVTK>`_ = 1.1.0 (optional for VTK output)
* `h5py <https://www.h5py.org/>`_ (optional for HDF5 output)


"
5,hyvr-README.md,Development,"HyVR has been developed by Jeremy
Bennett (`website <https://jeremypaulbennett.weebly.com>`_) as part of his
doctoral research at the University of Tübingen and by Samuel Scherrer as a
student assistant.

You can contact the developer(s) of HyVR by `email <mailto:hyvr.sim@gmail.com>`_
or via github.

"
6,hyvr-README.md,"Problems, Bugs, Unclear Documentation
","If you have problems with HyVR have a look at the `troubleshooting
<https://driftingtides.github.io/hyvr/troubleshooting.html>`_ section. If this
doesn't help, don't hesitate to contact us via email or at github.

If you find that the documentation is unclear, lacking, or wrong, please also contact us.
"
0,lasio-README.md, lasio,"![Status](https://img.shields.io/badge/status-beta-yellow.svg)
[![Version](http://img.shields.io/pypi/v/lasio.svg)](https://pypi.python.org/pypi/lasio/)
[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/kinverarity1/lasio/blob/master/LICENSE)
[![Python versions](https://img.shields.io/pypi/pyversions/lasio.svg)](https://www.python.org/downloads/)
[![Build Status](https://travis-ci.org/kinverarity1/lasio.svg?branch=master)](https://travis-ci.org/kinverarity1/lasio)
[![Build status](https://ci.appveyor.com/api/projects/status/csr7bg8urkbtbq4n?svg=true)](https://ci.appveyor.com/project/kinverarity1/lasio)
[![Docker Build](https://img.shields.io/docker/build/kinverarity/lasio.svg)](https://hub.docker.com/r/kinverarity/lasio/)

This is a Python 2.7 and 3.3+ package to read and write Log ASCII Standard (LAS) files, used for borehole data such as geophysical, geological, or petrophysical logs. It's compatible with versions 1.2 and 2.0 of the LAS file specification, published by the [Canadian Well Logging Society](http://www.cwls.org/las). Support for LAS 3 is being worked on. In principle it is designed to read as many types of LAS files as possible, including ones containing common errors or non-compliant formatting.

Depending on your particular application you may also want to check out [striplog](https://github.com/agile-geoscience/striplog) for stratigraphic/lithological data, or [welly](https://github.com/agile-geoscience/welly) for dealing with data at the well level. lasio is primarily for reading & writing LAS files.

Note this is *not* a package for reading LiDAR data (also called ""LAS files"").

"
1,lasio-README.md, Documentation,"See here for the [complete lasio package documentation](https://lasio.readthedocs.io/en/latest/).

"
2,lasio-README.md, Quick start,"Install the usual way:

```bash
$ pip install lasio
```

Very quick example session:

```python
>>> import lasio
>>> las = lasio.read(""sample_big.las"")
```

Data is accessible both directly as numpy arrays

```python
>>> las.keys()
['DEPT', 'DT', 'RHOB', 'NPHI', 'SFLU', 'SFLA', 'ILM', 'ILD']
>>> las['SFLU']
array([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])
>>> las['DEPT']
array([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,
        1669.875])
```

and as ``CurveItem`` objects with associated metadata:

```python
>>> las.curves
[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)), 
CurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)), 
CurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)), 
CurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)), 
CurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)), 
CurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)), 
CurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)), 
CurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]
```

Header information is parsed into simple HeaderItem objects, and stored in a dictionary for each section of the header:

```python
>>> las.version
[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS), 
HeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]
>>> las.well
[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT), 
HeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP), 
HeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP), 
HeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL), 
HeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP), 
HeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #:12, descr=WELL, original_mnemonic=WELL), 
HeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD), 
HeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC), 
HeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV), 
HeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC), 
HeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE), 
HeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]
>>> las.params
[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT), 
HeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS), 
HeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD), 
HeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR), 
HeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN), 
HeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF), 
HeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]
```

The data is stored as a 2D numpy array:

```python
>>> las.data
array([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       ...,
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])
```

You can also retrieve and load data as a ``pandas`` DataFrame, build LAS files from scratch, 
write them back to disc, and export to Excel, amongst other things.

See the [documentation](https://lasio.readthedocs.io/en/latest/) for more details.

"
3,lasio-README.md, License,"MIT
"
0,striplog-README.md, Of course you don't need this one if you didn't install it yet.,"    python setup.py sdist
    pip install dist/striplog-0.6.1.tar.gz    "
1,striplog-README.md, Or whatever was the last version to build.,"SciPy 2015
----------

`Here's a presentation about Striplog. <https://docs.google.com/presentation/d/16HJsJJQylb2_8D2NS1p2cjp1yzslqUl_51BN16J5Y2k/edit?usp=sharing>`_
"
